<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://refstop.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://refstop.github.io/" rel="alternate" type="text/html" /><updated>2021-09-29T02:15:58+09:00</updated><id>https://refstop.github.io/feed.xml</id><title type="html">Refstop의 연구일지</title><subtitle>Deep Learning 및 SLAM을 공부하고 있습니다.</subtitle><author><name>Refstop</name></author><entry><title type="html">2021 여름 프로젝트 - Aruco Marker EKF Localization</title><link href="https://refstop.github.io/cartographer.html" rel="alternate" type="text/html" title="2021 여름 프로젝트 - Aruco Marker EKF Localization" /><published>2021-08-23T11:06:24+09:00</published><updated>2021-08-23T11:06:24+09:00</updated><id>https://refstop.github.io/cartographer</id><content type="html" xml:base="https://refstop.github.io/cartographer.html">&lt;p&gt;작성 예정&lt;/p&gt;</content><author><name>Refstop</name></author><category term="실내 자율 주행" /><category term="2021 여름 프로젝트" /><summary type="html">작성 예정</summary></entry><entry><title type="html">자주쓰는 git 명령어 정리</title><link href="https://refstop.github.io/git-cmd.html" rel="alternate" type="text/html" title="자주쓰는 git 명령어 정리" /><published>2021-08-23T11:06:24+09:00</published><updated>2021-08-23T11:06:24+09:00</updated><id>https://refstop.github.io/git-cmd</id><content type="html" xml:base="https://refstop.github.io/git-cmd.html">&lt;h2 id=&quot;사용해본-git-명령어-정리&quot;&gt;사용해본 git 명령어 정리&lt;/h2&gt;

&lt;p&gt;git init : git 공간 만들기&lt;br /&gt;
git clone : git repository로부터 다운로드(깃 공간까지)&lt;br /&gt;
git add . : 깃 공간의 변경사항 전부 추가(저장후보) (잘몰겟음)&lt;br /&gt;
git commit -m ‘(커밋 메시지)’ : git add로 추가한 내용들 local에 업로드&lt;br /&gt;
git push : local -&amp;gt; remote로 업로드&lt;br /&gt;
git status : 깃 공간의 현재상태(커밋내용, 현재 브랜치 등)&lt;br /&gt;
git branch : 현재 깃 공간(local)의 브랜치 보기&lt;br /&gt;
git log : 현재 브랜치의 커밋 목록들 보기(q 누르면 종료, enter 누르면 계속 보여줌)&lt;br /&gt;
-v : 브랜치 상세정보, 마지막 커밋 메시지 확인&lt;br /&gt;
-r : 리모트 공간의 브랜치 보기&lt;br /&gt;
-a : 로컬, 리모트 브랜치 다 보기&lt;br /&gt;
–delete, -d (브랜치명) : local 브랜치 삭제&lt;br /&gt;
작업된 사항이나 commit 한 이력이 남아있는 경우, branch가 삭제되지 않는 경우&lt;br /&gt;
-D (브랜치명) : 브랜치 강제 삭제 « 이걸로 삭제 (추천x)&lt;br /&gt;
-m (현재 브랜치명) (바꿀 브랜치명) : 브랜치 이름 변경&lt;br /&gt;
git push origin –delete (브랜치명) : remote 브랜치 삭제&lt;br /&gt;
git checkout (브랜치명) : 브랜치로 이동&lt;br /&gt;
git checkout -b (new 브랜치명) : 브랜치를 생성하고 이동&lt;br /&gt;
git push –set-upstream origin (new 브랜치명) : 새로 생성된 브랜치를 리모트에 연결할 때 사용&lt;br /&gt;
이거 해야지 git push로 그냥 업로드 할수 있음, 생성하고 처음 한번만 하면됨&lt;br /&gt;
git push -u origin (new 브랜치명) « 이것도 업스트림과 똑같은 명령어&lt;br /&gt;
git merge (다른 브랜치) : 현재 브랜치에 다른 브랜치를 병합, 이때 병합되는건 최근 commit 기준&lt;/p&gt;

&lt;p&gt;계속 새로 업데이트 예정&lt;/p&gt;</content><author><name>Refstop</name></author><category term="github" /><summary type="html">사용해본 git 명령어 정리</summary></entry><entry><title type="html">2021 여름 프로젝트 - 실내 자율 주행 로봇</title><link href="https://refstop.github.io/indoor-nav.html" rel="alternate" type="text/html" title="2021 여름 프로젝트 - 실내 자율 주행 로봇" /><published>2021-08-22T23:07:24+09:00</published><updated>2021-08-22T23:07:24+09:00</updated><id>https://refstop.github.io/indoor-nav</id><content type="html" xml:base="https://refstop.github.io/indoor-nav.html">&lt;p&gt;지난번에 작성했던 간단한 &lt;a href=&quot;https://refstop.github.io/i2n-signal.html&quot;&gt;실내 자율주행 로봇 패키지&lt;/a&gt;를 조금 발전시킨 형태입니다. 최종적인 목표는 아래 그림과 같은 택배를 가져다주는 실내 배송 로봇입니다.
&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/indoor_2d_nav/Concept.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
제가 맡은 부분은 로봇 SLAM 및 네비게이션이었기 때문에 정리할 내용은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cartographer 사용
    &lt;ul&gt;
      &lt;li&gt;파라미터 사용 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Waypoint Navigation
    &lt;ul&gt;
      &lt;li&gt;move_base Action 사용 방법&lt;/li&gt;
      &lt;li&gt;소스코드&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Aruco Marker를 사용한 EKF Localization
    &lt;ul&gt;
      &lt;li&gt;Fiducial Package 사용 방법&lt;/li&gt;
      &lt;li&gt;소스코드&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Refstop</name></author><category term="실내 자율 주행" /><category term="2021 여름 프로젝트" /><summary type="html">지난번에 작성했던 간단한 실내 자율주행 로봇 패키지를 조금 발전시킨 형태입니다. 최종적인 목표는 아래 그림과 같은 택배를 가져다주는 실내 배송 로봇입니다. 제가 맡은 부분은 로봇 SLAM 및 네비게이션이었기 때문에 정리할 내용은 다음과 같습니다. Cartographer 사용 파라미터 사용 방법 Waypoint Navigation move_base Action 사용 방법 소스코드 Aruco Marker를 사용한 EKF Localization Fiducial Package 사용 방법 소스코드</summary></entry><entry><title type="html">Kalman Filter와 EKF(2) - EKF SLAM</title><link href="https://refstop.github.io/ekf-slam.html" rel="alternate" type="text/html" title="Kalman Filter와 EKF(2) - EKF SLAM" /><published>2021-08-22T21:49:24+09:00</published><updated>2021-08-22T21:49:24+09:00</updated><id>https://refstop.github.io/ekf-slam</id><content type="html" xml:base="https://refstop.github.io/ekf-slam.html">&lt;p&gt;지난 게시글에 이어 EKF를 사용한 SLAM 방법을 정리하겠습니다. SLAM은 로봇의 Localization과 Map을 작성하는 것이 목적으로 실시간 결과로서 로봇의 위치가, 최종적인 결과로서 로봇이 작성한 Map을 얻을 수 있습니다. 여기서 Map은 랜드마크의 위치를 기록한 자료로서, 사람이 생각하는 시각적인 지도가 아닌 랜드마크의 위치만을 기록한 txt 파일이 될 수도 있습니다. 이러한 SLAM의 방법으로는 Kalman Filter, Particle Filter, Graph-based 3가지로 나눌 수 있습니다. 이번 게시글에서는 EKF를 사용한 SLAM을 중심으로 설명하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;ekf-for-online-slam&quot;&gt;EKF for online SLAM&lt;/h2&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_slam_node.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
위 그림은 EKF를 이용한 online SLAM을 표현한 그림입니다. online은 실시간인 만큼 현재 로봇의 위치 state $x_t$만을 저장하는 방식으로 작동합니다. $m$은 랜드마크의 위치를 의미하고, $u_t$와 $z_t$는 각각 observation과 control input입니다. 각각 measurement model과 motion model과 관련이 있는 항입니다. 이 문제를 식으로 표현하면 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$p(x_t, m|z_{1:t}, u_{1:t})$&lt;/center&gt;
&lt;p&gt;즉, 처음부터 현재 시간 $t$까지의 control input과 observation을 알고 있을 때 현재 로봇의 위치 state $x_t$와 맵을 구성하는 랜드마크인 $m$을 구하는 문제입니다.&lt;/p&gt;

&lt;h2 id=&quot;state의-표현방법&quot;&gt;state의 표현방법&lt;/h2&gt;
&lt;p&gt;이 게시글에서 다루는 로봇의 state 모델은 2D를 기준으로 하고 있습니다. 따라서 이 로봇의 state vector는 위치와 방향을 나타내는 $x$, $y$, $\theta$로 이루어져 있는 3$\times$1 크기의 열 벡터입니다. 그리고 랜드마크의 state vector는 위치만을 나타내는 $x$, $y$로 이루어져 있는 2$\times$1 크기의 열 벡터입니다. 기본적으로, EKF SLAM에서의 state vector $x_t$는 위의 로봇 state vector, 랜드마크 state vector 두 요소로 구성되어 있습니다. 이는 위의 $p(x_t, m|z_{1:t}, u_{1:t})$ 공식에서도 볼 수 있습니다.&lt;/p&gt;
&lt;center&gt;$\begin{align*} x_t &amp;amp;= (x_R, m)^T \\
&amp;amp;=(x,y,\theta,m_{1,x},m_{1,y},m_{2,x},m_{2,y},\cdots,m_{n,x},m_{n,y})^T
\end{align*}$&lt;/center&gt;
&lt;p&gt;따라서 랜드마크의 개수를 n이라고 할 때, state 벡터의 크기는 3+2n$\times$1이 됩니다. 이렇게 state vector가 정의되었을 때 공분산 행렬은 다음과 같이 정의됩니다.
&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_cov.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_cov2.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_cov3.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
노란색으로 표시된 부분이 로봇 state에 대한 공분산, 파란색으로 표시된 부분이 랜드마크 state에 대한 공분산입니다. 지난 게시글에서 말했듯이 공분산이란 uncertainty를 나타내는 값으로 이 값이 클수록 불확실한 정보를 나타낸다고 말할 수 있습니다. 랜드마크에도 공분산이 있는 이유는 Mapping을 하는 동안 랜드마크의 위치(state) 역시 불확실한 상태이므로 계속해서 수정되어질 필요가 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;prediction-step&quot;&gt;Prediction Step&lt;/h2&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_prediction.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
위 그림은 로봇이 이동하였을 때 prediction step 단계를 보여주고 있습니다. prediction step에서는 control input을 이용하여 예상되는 &lt;strong&gt;로봇의 위치&lt;/strong&gt;를 추정하는 과정입니다. 따라서 로봇의 위치인 $x_R$과 로봇의 위치에 대한 공분산행렬 $\Sigma_{x_R x_R}$, 그리고 $x_R$이 관련되어 있는 $\Sigma_{x_R m_n}$ 공분산 행렬도 update됩니다. 아직 가장 큰 공분산 행렬인 $\Sigma_{mm}$을 건드리지 않았기 때문에 계산량이 크지 않고, 이 계산량은 랜드마크의 개수에 따라서 선형적으로 증가합니다.&lt;/p&gt;

&lt;h2 id=&quot;correction-step&quot;&gt;Correction Step&lt;/h2&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/ekf_correction.png&quot; align=&quot;center&quot; /&gt;&lt;br /&gt;
위 그림은 로봇이 랜드마크를 관측하여 얻은 데이터와 로봇이 알고 있는 랜드마크의 데이터를 비교하여 state를 보정하는 과정입니다. 이 과정에서 실제 observation의 uncertainty가 state에 반영이 되며, 랜드마크의 공분산 행렬에도 영향을 주게 됩니다. 따라서 correction단계에서는 state vector와 공분산 행렬의 모든 영역이 update됩니다. correction 단계의 계산량은 랜드마크의 숫자에 quadratic하게 증가합니다. 왜냐하면 Kalman gain을 구할 때 역행렬을 구하는 과정에서 행렬 크기에 quadratic하게 증가하기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;ekf-slam-example&quot;&gt;EKF SLAM Example&lt;/h2&gt;
&lt;h3 id=&quot;example을-위한-가정&quot;&gt;Example을 위한 가정&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;로봇은 2D plane 상에서 움직입니다.&lt;/li&gt;
  &lt;li&gt;Velocity-based motion model을 사용합니다.&lt;/li&gt;
  &lt;li&gt;로봇은 랜드마크를 point($x$, $y$)로 인식합니다.&lt;/li&gt;
  &lt;li&gt;Observation model은 LiDAR와 같은 Range-bearing 센서입니다.&lt;/li&gt;
  &lt;li&gt;관측된 랜드마크와 알고있는 map상의 랜드마크와의 대응 관계는 알고 있다고 가정합니다.&lt;/li&gt;
  &lt;li&gt;랜드마크의 개수는 미리 알고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;초기화&quot;&gt;초기화&lt;/h3&gt;
&lt;p&gt;SLAM의 가정에서 랜드마크의 개수는 미리 알고 있다고 하였으므로, state vector와 공분산행렬을 고정된 크기로 취급할 수 있습니다. state vector의 크기는 3+2n$\times$1, 공분산행렬의 크기는 3+2n$\times$3+2n입니다. 또한 로봇의 시작 위치는 원점이 되므로 state vector를 3+2n$\times$1 크기의 영행렬로 초기화합니다.&lt;/p&gt;
&lt;center&gt;$\mu_0=(0,0,0,0,\cdots,0)^T$&lt;/center&gt;
&lt;p&gt;공분산행렬의 경우 로봇의 state를 (0,0,0)으로 확실히 알고 있고(uncertainty 매우 작음), 랜드마크는 아예 어디 있는지 모른다(uncertainty 매우 큼)고 가정하여, $\Sigma_{x_R x_R}$을 3$\times$3 크기의 영행렬로, $\Sigma_{mm}$을 2n$\times$2n 크기의 무한대 대각행렬로 초기화합니다.&lt;/p&gt;
&lt;center&gt;$\Sigma_0=\begin{pmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \infty &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \infty &amp;amp; \cdots &amp;amp; 0\\ 
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \infty
\end{pmatrix}$&lt;/center&gt;

&lt;h3 id=&quot;extended-kalman-filter-ekf-과정&quot;&gt;Extended Kalman Filter (EKF) 과정&lt;/h3&gt;
&lt;center&gt;
$\begin{align*} 1&amp;amp;: Extended Kalman filter(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t) \\
&amp;amp; [Prediction\;step] \\
2&amp;amp;: \overline{\mu}_t=g(u_t, \mu_{t-1}) \\
3&amp;amp;: \overline{\Sigma}_t=G_t\Sigma_{t-1}G_t^T+R_t \\
&amp;amp; [Correction\;step] \\
4&amp;amp;: K_t=\overline{\Sigma}_tH_t^T(H_t\overline{\Sigma}_tH_t^T+Q_t)^{-1}\\
5&amp;amp;: \mu_t = \overline{\mu}_t+K_t(z_t-h(\overline{\mu}_t)) \\
6&amp;amp;: \Sigma_t = (I-K_tH_t)\overline{\Sigma}_t \\ 
7&amp;amp;: return\;\mu_t, \Sigma_t
\end{align*}$
&lt;/center&gt;
&lt;p&gt;&lt;a href=&quot;https://refstop.github.io/kf-ekf.html&quot;&gt;이전 게시글&lt;/a&gt;에서 설명했었던 EKF 알고리즘에 따르면, 입력 $\mu_t{t-1}, \Sigma_{t-1}, u_t, z_t$ 중에서 $\mu_t{t-1}, \Sigma_{t-1}$을 구했고, $u_t, z_t$에 대한 입력은 각각 motion model, observation model에 관한 항이므로, 센서 입력($u_t$: 엔코더, $z_t$: 카메라, LiDAR)에 의해 결정되므로, prediction 단계와 correction 단계를 설명할 때 더욱 자세히 설명하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;statemean-prediction&quot;&gt;State(Mean) Prediction&lt;/h3&gt;
&lt;p&gt;로봇의 motion model을 나타낼 때는 주로 다음 2가지를 사용합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Odometry-based model&lt;/li&gt;
  &lt;li&gt;Velocity-based model&lt;br /&gt;
Odometry-based model은 로봇 또는 자동차의 바퀴에 달린 wheel encoder 센서 데이터를 이용한 모델이며, Velocity-based model은 imu와 같은 관성 센서(선속도, 각속도를 측정해줌)를 이용한 모델입니다. Velocity-based model은 wheel encoder와 같은 Odometry-based model을 사용할 수 없을 때 주로 사용하며, Odometry-based model보다 Velocity-based model이 더 정확한 편입니다.&lt;br /&gt;
이번 예시에서 사용한 모델은 Velocity-based model입니다. Velocity-based model은 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$\begin{bmatrix}
x_t\\ 
y_t\\ 
\theta_t
\end{bmatrix}=\begin{bmatrix}
x_{t-1}\\ 
y_{t-1}\\ 
\theta_{t-1}
\end{bmatrix}+\begin{bmatrix}
-\frac{v_t}{w_t}sin \theta_{t-1}+\frac{v_t}{w_t}sin(\theta_{t-1}+w_t \Delta t)\\ 
-\frac{v_t}{w_t}cos \theta_{t-1}-\frac{v_t}{w_t}cos(\theta_{t-1}+w_t \Delta t)\\ 
w_t \Delta t
\end{bmatrix}$&lt;/center&gt;
&lt;p&gt;위의 motion model은 로봇의 위치에 대한 정보만을 갖고 있는 3$\times$1 크기의 vector입니다. 이 vector를 3+2n$times$1 크기의 state vector $x_t$에 더하기 위해서는 동차 행렬 $F_x$ 행렬을 이용합니다.&lt;/p&gt;
&lt;center&gt;$F_x = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\end{bmatrix}^T$
&lt;/center&gt;
&lt;center&gt;$\begin{bmatrix}
x_t\\ 
y_t\\ 
\theta_t
\end{bmatrix}=\begin{bmatrix}
x_{t-1}\\ 
y_{t-1}\\ 
\theta_{t-1}
\end{bmatrix}+F_x\begin{bmatrix}
-\frac{v_t}{w_t}sin \theta_{t-1}+\frac{v_t}{w_t}sin(\theta_{t-1}+w_t \Delta t)\\ 
-\frac{v_t}{w_t}cos \theta_{t-1}-\frac{v_t}{w_t}cos(\theta_{t-1}+w_t \Delta t)\\ 
w_t \Delta t
\end{bmatrix}$&lt;/center&gt;
&lt;p&gt;위와 같이 $F_x$ 행렬을 이용함으로서 control input에 의한 로봇 위치의 state만 변경되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;공분산-행렬-prediction&quot;&gt;공분산 행렬 prediction&lt;/h3&gt;
&lt;p&gt;3번 단계에서 공분산을 구하기 위해선, motion model의 자코비안 행렬인 $G_t$와 process noise인 $R_t$가 필요합니다. 이때 사용한 motion model은 Velocity-based motion model이므로, 이 motion model의 자코비안 행렬은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$G_t^x=\frac{\partial g(u_t,\mu_{t-1})}{\partial x_{t-1}}=\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; -\frac{v_t}{w_t}cos \theta_{t-1}+\frac{v_t}{w_t}cos(\theta_{t-1}+w_t \Delta t)\\ 
0 &amp;amp; 1 &amp;amp; -\frac{v_t}{w_t}sin \theta_{t-1}+\frac{v_t}{w_t}sin(\theta_{t-1}+w_t \Delta t)\\ 
0 &amp;amp; 0 &amp;amp; 1
\end{pmatrix}$&lt;/center&gt;
&lt;p&gt;하지만 이때 구한 $G_t^x$는 3$\times$3 행렬이므로, 3+2n$\times$3+2n 크기에 맞춰 주기 위해 다음과 같이 행렬을 수정합니다.&lt;/p&gt;
&lt;center&gt;$G_t=\begin{pmatrix}
G_t^x &amp;amp; 0\\ 
0 &amp;amp; I\\ 
\end{pmatrix}$&lt;/center&gt;
&lt;p&gt;또한 process noise $R_t$은 다음과 같이 정의됩니다.&lt;/p&gt;
&lt;center&gt;$R_t=V_tM_tV_t^T$&lt;/center&gt;
&lt;p&gt;여기서 $V_t$는 $g(u_t,\mu_{t-1})$을 $u_t$로 편미분한 값입니다. Velocity-based model에서 $u_t$는 $(v_t, w_t)$로 이루어져 있으므로, 미분한 값은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$V_t=\frac{\partial g(u_t,\mu_{t-1})}{\partial u_t}=\begin{pmatrix}
\frac{-sin\theta_{t-1}+sin(\theta_{t-1}+w_t\Delta t)}{w_t} &amp;amp; \frac{v_t(sin\theta_{t-1}-sin(\theta_{t-1}+w_t\Delta t))}{w_t^2}+\frac{v_t\Delta tcos(\theta_{t-1}+w_t\Delta t)}{w_t}\\ 
\frac{cos\theta_{t-1}-cos(\theta_{t-1}+w_t\Delta t)}{w_t} &amp;amp; \frac{v_t(-cos\theta_{t-1}+cos(\theta_{t-1}+w_t\Delta t))}{w_t^2}+\frac{v_t\Delta tsin(\theta_{t-1}+w_t\Delta t)}{w_t}\\ 
0 &amp;amp; 1
\end{pmatrix}$&lt;/center&gt;
&lt;p&gt;결국 $R_t$의 크기 역시 3$\times$3이므로, 동차 행렬 $F_x$을 곱하여 크기를 수정하여 3번 단계의 연산을 수행합니다.&lt;/p&gt;
&lt;center&gt;$\overline{\Sigma}_t=G_t\Sigma_{t-1}G_t^T+F_xR_tF_x^T$&lt;/center&gt;

&lt;h3 id=&quot;statemean-correction&quot;&gt;State(Mean) Correction&lt;/h3&gt;
&lt;p&gt;EKF SLAM의 correction 단계의 과정은 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$c_t^i=j$는 $t$시점에서 $i$번째로 측정된 랜드마크이며 전체 랜드마크에서 $j$번째 index를 갖습니다.&lt;/li&gt;
  &lt;li&gt;만약 랜드마크가 이전에 관측되지 않았던 랜드마크이면 현재 로봇의 위치를 기반으로 초기화합니다. 즉 랜드마크 initialization입니다.&lt;/li&gt;
  &lt;li&gt;예상되는 observation을 계산합니다.&lt;/li&gt;
  &lt;li&gt;비선형 함수 $h(x_t)$의 자코비안 행렬을 계산합니다.&lt;/li&gt;
  &lt;li&gt;Kalman gain을 계산하고 correction process를 진행합니다.
이 예제에서는 LiDAR 센서와 같은 Range-bearing observation 센서를 기반으로 합니다. Range-bearing observation model은 다음과 같습니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;$z_t^i=\begin{pmatrix}
r_t^i\\
\phi_t^i
\end{pmatrix}$&lt;/center&gt;
&lt;p&gt;$r_t^i$는 로봇으로부터 랜드마크의 거리, $\phi_t^i$는 로봇의 헤딩 각도로부터 랜드마크까지의 방향을 의미합니다. 센서로부터 랜드마크의 데이터가 획득되었으나 이전에 관측되지 않았던 랜드마크라면 아래 식을 통해 랜드마크의 global 위치를 계산하여 state에 등록합니다.&lt;/p&gt;
&lt;center&gt;$\begin{pmatrix}
\overline{\mu}_{j,x}\\
\overline{\mu}_{j,y}
\end{pmatrix}=\begin{pmatrix}
\overline{\mu}_{t,x}\\
\overline{\mu}_{t,y}
\end{pmatrix}+\begin{pmatrix}
r_t^icos(\phi_t^i+\overline{\mu}_{t,\theta})\\
r_t^isin(\phi_t^i+\overline{\mu}_{t,\theta})
\end{pmatrix}$&lt;/center&gt;

&lt;p&gt;$\overline{\mu}&lt;em&gt;j$는 $j$번째 랜드마크의 위치이고 위 state vector에서 $m_j$항으로 나타낼 수 있습니다. $\overline{\mu}&lt;/em&gt;{t,x}$, $\overline{\mu}&lt;em&gt;{t,y}$, $\overline{\mu}&lt;/em&gt;{t,\theta}$는 각각 현재 시점에서 로봇의 $x$, $y$, $\theta$(heading)를 의미합니다. Range-bearing observation model은 다음과 같이 계산합니다.&lt;/p&gt;

&lt;center&gt;$\delta=\begin{pmatrix}
\delta_x \\
\delta_y
\end{pmatrix}=\begin{pmatrix}
\overline{\mu}_{j,x} - \overline{\mu}_{t,x} \\
\overline{\mu}_{j,y} - \overline{\mu}_{t,y}
\end{pmatrix}$&lt;/center&gt;
&lt;center&gt;$q=\delta^T\delta$&lt;/center&gt;
&lt;center&gt;$\hat{z}_t^i=\begin{pmatrix}
\sqrt{q}\\
arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta}
\end{pmatrix}=h(\overline{\mu}_t)$&lt;/center&gt;
&lt;p&gt;$\delta$는 로봇의 위치와 랜드마크의 위치 차를 의미하며, $\sqrt{q}$는 로봇과 랜드마크의 거리를 의미합니다. 따라서 $h(\overline{\mu}&lt;em&gt;t)$는 비선형 observation 모델이며, 현재 로봇의 위치와 랜드마크의 거리를 알고 있을 때 이 observation 모델을 이용하여 예상되는 센서의 observation을 계산할 수 있습니다. 여기서 $\overline{\mu}_t$는 로봇의 현재 위치정보($\overline{\mu}&lt;/em&gt;{t,x}$, $\overline{\mu}&lt;em&gt;{t,y}$, $\overline{\mu}&lt;/em&gt;{t,\theta}$)와 랜드마크의 위치 ($\overline{\mu}&lt;em&gt;{j,x}$, $\overline{\mu}&lt;/em&gt;{j,y}$)를 포함하고 있는 5$\times$1크기의 vector입니다.&lt;/p&gt;

&lt;p&gt;이 observation model의 선형화 과정을 통해 $h(\overline{\mu}_t)$의 자코비안 행렬을 구하면 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$^{low}\textrm{H}_t^i=\frac{\partial h(\overline{\mu}_t)}{\partial \overline{\mu}_t}=\begin{pmatrix}
\frac{\partial \sqrt{q}}{\partial \overline{\mu}_{t,x}} &amp;amp; \frac{\partial \sqrt{q}}{\partial \overline{\mu}_{t,y}} &amp;amp; \frac{\partial \sqrt{q}}{\partial \overline{\mu}_{t,\theta}} &amp;amp; \frac{\partial \sqrt{q}}{\partial \overline{\mu}_{j,x}} &amp;amp; \frac{\partial \sqrt{q}}{\partial \overline{\mu}_{j,y}} \\
\frac{\partial arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta})}{\partial \overline{\mu}_{t,x}} &amp;amp; \frac{\partial arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta})}{\partial \overline{\mu}_{t,y}} &amp;amp; \frac{\partial arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta})}{\partial \overline{\mu}_{t,\theta}} &amp;amp; \frac{\partial arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta})}{\partial \overline{\mu}_{j,x}} &amp;amp; \frac{\partial arctan(\frac{\delta_y}{\delta_x}-\overline{\mu}_{t,\theta})}{\partial \overline{\mu}_{j,y}}
\end{pmatrix}=\frac{1}{q}\begin{pmatrix}
-\sqrt{q}\delta_x &amp;amp; -\sqrt{q}\delta_y &amp;amp; 0 &amp;amp; \sqrt{q}\delta_x &amp;amp; \sqrt{q}\delta_y \\
\delta_y &amp;amp; \delta_x &amp;amp; -q &amp;amp; -\delta_y &amp;amp; \delta_x
\end{pmatrix}$&lt;/center&gt;

&lt;p&gt;$^{low}\textrm{H}&lt;em&gt;t^i$에서 low는 아직 크기를 조정하기 전의 행렬이라는 의미로 사용되었습니다. 자코비안 행렬 $^{low}\textrm{H}_t^i$의 크기는 2$\times$5이므로 EKF correction의 update과정에 적용시키기 위해서 $F&lt;/em&gt;{x,j}$ 행렬을 이용하여 행렬의 크기를 조절합니다.&lt;/p&gt;

&lt;center&gt;$H_t^i=^{low}\textrm{H}_t^iF_{x,j}$&lt;/center&gt;
&lt;center&gt;$F_{x,j}=\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\end{pmatrix}$&lt;/center&gt;

&lt;p&gt;$F_{x,j}$ 행렬에서 1~3행은 로봇의 위치에 대한 자코비안 term을, 4~5 행은 랜드마크에 대한 자코비안 term을 원하는 위치에 입력하기 위함이여, 랜드마크의 index가 $j$인 경우 4~5행의 2$\times$2 단위행렬은 3+2(j-1)열 다음에 위치하게 됩니다. $F_{x,j}$ 행렬이 곱해진 $H_t^i$의 크기는 2$\times$3+2n입니다. 이 크기가 조절된 $H_t^i$행렬을 이용하여 칼만 게인을 구하고(4번 과정) EKF의 5, 6번 과정을 통해 최종 state와 공분산 행렬을 계산할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://jinyongjeong.github.io/2017/02/14/lec02_motion_observation_model/&quot;&gt;Jinyoung - [SLAM] Motion &amp;amp; Observation model&lt;/a&gt;
&lt;a href=&quot;http://jinyongjeong.github.io/2017/02/16/lec05_EKF_SLAM/&quot;&gt;Jinyoung - [SLAM] Extended Kalman Filter(EKF) SLAM&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Kalman Filter" /><category term="EKF" /><summary type="html">지난 게시글에 이어 EKF를 사용한 SLAM 방법을 정리하겠습니다. SLAM은 로봇의 Localization과 Map을 작성하는 것이 목적으로 실시간 결과로서 로봇의 위치가, 최종적인 결과로서 로봇이 작성한 Map을 얻을 수 있습니다. 여기서 Map은 랜드마크의 위치를 기록한 자료로서, 사람이 생각하는 시각적인 지도가 아닌 랜드마크의 위치만을 기록한 txt 파일이 될 수도 있습니다. 이러한 SLAM의 방법으로는 Kalman Filter, Particle Filter, Graph-based 3가지로 나눌 수 있습니다. 이번 게시글에서는 EKF를 사용한 SLAM을 중심으로 설명하도록 하겠습니다.</summary></entry><entry><title type="html">Kalman Filter와 EKF(1) - 선형 KF와 EKF</title><link href="https://refstop.github.io/kf-ekf.html" rel="alternate" type="text/html" title="Kalman Filter와 EKF(1) - 선형 KF와 EKF" /><published>2021-08-05T11:36:24+09:00</published><updated>2021-08-05T11:36:24+09:00</updated><id>https://refstop.github.io/kf-ekf</id><content type="html" xml:base="https://refstop.github.io/kf-ekf.html">&lt;p&gt;이 게시글에서는 SLAM에서의 KF와 EKF를 기준으로 설명할 예정입니다.&lt;/p&gt;
&lt;h2 id=&quot;kalman-filter란&quot;&gt;Kalman Filter란?&lt;/h2&gt;
&lt;p&gt;칼만 필터는 현재 상태(state) $\mu$와 공분산(covariance) $\Sigma$를 기반으로 다음 상태를 추정하는 방법입니다. $\mu$와 $\Sigma$만으로 나타낼 수 있는 Gaussian 분포를 따르며, 지난 게시글의 &lt;a href=&quot;https://refstop.github.io/uda-loc-markov.html&quot;&gt;Bayesian Filter&lt;/a&gt;와도 관계가 있습니다. 선형 칼만 필터를 설명하기 전에 가우시안 분포에 대해서 조금 설명하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-분포&quot;&gt;Gaussian 분포&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;단일 변수 가우시안 분포&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$p(x)=\frac{1}{ \sqrt{2 \sigma^2 \pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;다변수 가우시안 분포&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$p(x)=\frac{1}{ \sqrt{det(2\pi\Sigma)}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$&lt;/center&gt;

&lt;p&gt;가우시안 분포는 단일 변수(single variable)과 다변수(multi variable)의 표현 방식이 있으며, SLAM 에서는 로봇의 현재 state를 벡터 $(x,y,\phi)$로 표시하기 때문에 다변수 가우시안 분포를 사용하는 것이 좋습니다.&lt;/p&gt;

&lt;h2 id=&quot;선형-모델의-가우시안-분포&quot;&gt;선형 모델의 가우시안 분포&lt;/h2&gt;
&lt;p&gt;일반적으로, 선형 모델은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;
&lt;center&gt;$Y=AX+B$&lt;/center&gt;
&lt;p&gt;이때, $X$가 가우시안 분포를 따른다고 할 때, 확률은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;
&lt;center&gt;$X \sim N(\mu_x, \Sigma_x)$&lt;/center&gt;
&lt;p&gt;선형 변환 후의 확률변수 $Y$의 분포는 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$Y \sim N(A\mu_x+B, A\Sigma_xA^T)$&lt;/center&gt;

&lt;h3 id=&quot;유도과정&quot;&gt;유도과정&lt;/h3&gt;
&lt;p&gt;$X$의 평균인 $\mu_x$의 선형변환 $\mu_y$는 직관적으로 이해되지만, 공분산 $\Sigma_y$는 조금 의문이 남습니다. 유도과정을 간단히 정리하겠습니다.&lt;/p&gt;
&lt;center&gt; $\begin{align*} \mu_y &amp;amp;= E((y-\mu_y)(y-\mu_y)^T) \\
&amp;amp;= E((y-(A\mu_{x}+B))(y-(A\mu_{x}+B))^T) \\
&amp;amp;= E(((AX+B)-(A\mu_{x}+B))((AX+B)-(A\mu_{x}+B))^T) \\
&amp;amp;= E(\left [ A(X-\mu_x) \right ] \left [ A(X-\mu_x) \right ]^T) \\
&amp;amp;= E( A(X-\mu_x)(X-\mu_x)^{T} A^T) \\
&amp;amp;= AE((X-\mu_x)(X-\mu_x)^T)A^T \\
&amp;amp;= A \Sigma_x A^T
\end{align*}$ &lt;/center&gt;

&lt;h2 id=&quot;선형-kalman-filter&quot;&gt;선형 Kalman Filter&lt;/h2&gt;
&lt;p&gt;선형 칼만 필터는 motion model과 observation model을 선형으로 가정합니다.&lt;/p&gt;
&lt;center&gt; motion model: $x_t = A_tx_{t-1}+B_tu_t+\epsilon_t$ &lt;/center&gt;
&lt;center&gt; observation model: $z_t = H_tx_t+\delta_t$ &lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;$A_t$: control input $u_t$와 노이즈($\epsilon_t$)가 없을 때의 t-1과 t의 state 사이의 관계, n*n matrix&lt;/li&gt;
  &lt;li&gt;$B_t$: control input $u_t$가 다음 state $x_t$에 어떤 영향을 미치는지를 나타낸 matrix, n*l matrix&lt;/li&gt;
  &lt;li&gt;$H_t$: 현재 로봇의 상태를 나타내는 state $x_t$와 센서의 관측 정보(observation)의 관계, k*n matrix&lt;/li&gt;
  &lt;li&gt;$\epsilon_t,\;\delta_t$: 평균이 0이며 공분산이 각각 $R_t,\;Q_t$인 확률변수, 가우시안 표준정규분포를 따름.&lt;br /&gt;
따라서 이를 가우시안 분포 공식에 적용하면, Bayesian Filter의 motion model과 observation model 확률을 구할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;Motion model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$p(x_t|u_t,x_{t-1})=\frac{1}{ \sqrt{det(2\pi R_t)}}e^{-\frac{1}{2}(x-A_tx_{t-1}-B_tu_t)^TR_t^{-1}(x-A_tx_{t-1}-B_tu_t)}$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;Observation model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$p(z_t|x_t)=\frac{1}{ \sqrt{det(2\pi Q_t)}}e^{-\frac{1}{2}(x-H_tx_t)^TR_t^{-1}(x-H_tx_t)}$&lt;/center&gt;
&lt;p&gt;Motion model은 prediction step에서, Observation model은 correction step에 적용됩니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Prediction step&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$\overline{bel}(x_t)=\int_{x_{t-1}}p(x_t|u_t,x_{t-1})bel(x_{t-1})dx_{t-1}$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;Correction step&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$bel(x_t) = \eta p(z_t|x_t)\overline{bel}(x_t)$&lt;/center&gt;
&lt;p&gt;t에서의 state 확률 $\overline{bel}(x_t)$은 motion model에 의해 결정되고(prediction step), prediction step에서 계산된 t에서의 state 확률 $\overline{bel}(x_t)$은 observation model에 의해서 보정됩니다.&lt;br /&gt;
이때, 모든 확률 분포를 Gaussian 확률 분포를 따른다고 가정하는 모델이 바로 Kalman Filter입니다. Gaussian 분포를 따른다고 가정하면 간단히 평균 $\mu$과 분산(공분산) $\sigma(\Sigma)$로 표시하기 때문에 두개의 파라미터만으로 확률분포를 표현할 수 있는 장점이 있습니다. Kalman Filter 알고리즘은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;
$\begin{align*} 1&amp;amp;: Kalman Filter(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t) \\
&amp;amp; [Prediction\;step] \\
2&amp;amp;: \overline{\mu}_t=A_t\mu_{t-1}+B_tu_t \\
3&amp;amp;: \overline{\Sigma}_t=A_t\Sigma_{t-1}A_t^T+R_t \\
&amp;amp; [Correction\;step] \\
4&amp;amp;: K_t=\overline{\Sigma}_tH_t^T(H_t\overline{\Sigma}_tH_t^T+Q_t)^{-1}\\
5&amp;amp;: \mu_t = \overline{\mu}_t+K_t(z_t-H_t\mu_t) \\
6&amp;amp;: \Sigma_t = (I-K_tH_t)\overline{\Sigma}_t \\ 
7&amp;amp;: return\;\mu_t, \Sigma_t
\end{align*}$
&lt;/center&gt;

&lt;p&gt;Kalman Filter는 Bayesian Filter이기 때문에 predicton step과 correction step을 갖고 있습니다. 각 단계에 대해 Kalman Filter 알고리즘을 관찰해 봅시다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Prediction step&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
$\begin{align*} 1&amp;amp;: Kalman Filter(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t) \\
&amp;amp; [Prediction\;step] \\
2&amp;amp;: \overline{\mu}_t=A_t\mu_{t-1}+B_tu_t \\
3&amp;amp;: \overline{\Sigma}_t=A_t\Sigma_{t-1}A_t^T+R_t
\end{align*}$
&lt;/center&gt;
&lt;p&gt;prediction step은 다음 state를 motion model을 통해 예측하는 단계입니다. 계속 똑같은 말을 반복하는거 같긴 하지만, 우선 알아보도록 합시다. 이전 state와 motion $u_t$의 결합을 통해 다음 단계의 평균을 추측합니다. 하지만 이때는 아직 완전히 정확하지 않은 상태이므로 준 현재 state라는 의미로 평균과 공분산 기호 위에 작대기를 그어 표현해 줍니다. 다음 correction 단계에서 추가적인 과정을 통해 더 정확한 위치를 잡을 것입니다.&lt;br /&gt;
이때 $R_t$는 process noise로, control inpud $u_t$의 공분산 $M_t$에서 선형 변환을 한번 더 거친 형태의 공분산입니다. 그 형태는 다음과 같습니다.&lt;/p&gt;
&lt;center&gt; $R_t=B_tM_tB_t^T$ &lt;/center&gt;
&lt;p&gt;일반적으로 모바일 로봇에서 control input은 wheel encoder로부터 구해진 odometry가 들어가고, encoder에 대한 uncertainty가 공분산 $M_t$가 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correction step&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
$\begin{align*} 
&amp;amp; [Correction\;step] \\
4&amp;amp;: K_t=\overline{\Sigma}_tH_t^T(H_t\overline{\Sigma}_tH_t^T+Q_t)^{-1}\\
5&amp;amp;: \mu_t = \overline{\mu}_t+K_t(z_t-H_t\mu_t) \\
6&amp;amp;: \Sigma_t = (I-K_tH_t)\overline{\Sigma}_t \\ 
7&amp;amp;: return\;\mu_t, \Sigma_t
\end{align*}$
&lt;/center&gt;
&lt;p&gt;Correction 단계는 칼만 게인을 구하여 조금 부정확했던 현재 state $\overline{\mu}_t$를 보정해 주는 과정입니다. 칼만 게인 $K_t$는 현재 예측값 $\overline{\mu}_t$와 관측을 통해 예측한 값 $z_t-H_tx_t$ 중 어떤 것을 더 많이 보정에 반영할지, 즉 센서를 더 믿을 것인가, 기댓값을 더 믿을 것인가를 조절해 주는 가중치입니다.&lt;br /&gt;
예를 들어 observation 공분산 $Q_t$가 무한대의 값을 가지는 대각행렬이라고 하면 observation의 uncertainty가 무한대, 즉 센서 관측값을 전혀 못 믿겠다는 의미가 됩니다. 그렇다면, 칼만 게인의 분모값(다변수 일때는 역행렬값)이 무한대가 되고, 칼만 게인 값은 0에 수렴할 것입니다. 그 결과, 최종 평균은 $\mu_t=\overline{\mu}_t$이 됩니다. 반대로 $Q_t$의 값이 0의 대각행렬이라면 칼만 게인은 $K_t=C_t^{-1}$이 되고, 최종 평균은 $\mu_t=H_t^{-1}z_t$이 됩니다.&lt;br /&gt;
결국 &lt;strong&gt;예측 평균&lt;/strong&gt;과 &lt;strong&gt;관측을 통한 예측 평균&lt;/strong&gt;을 저울질하여 더 실제와 가까운 쪽으로 잘 섞어 주는 과정입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/kalman filter/kalman_fig.png&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;ekf-extended-kalman-filter&quot;&gt;EKF (Extended Kalman Filter)&lt;/h2&gt;
&lt;p&gt;확장 칼만 필터(Extended Kalman Filter)는 칼만 필터를 비선형 모델에 적용할 수 있도록 한 방식입니다. 지금까지 다뤘던 motion model과 observation model은 선형이었지만, 일반적으로 선형보다 비선형 모델이 더 많기 때문에 EKF를 사용합니다. 비선형 모델을 적용한 motion model과 observation model은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$x_t=g(u_t, x_{t-1})+\epsilon_t$&lt;/center&gt;
&lt;center&gt;$z_t=h(x_t)+\delta_t$&lt;/center&gt;
&lt;p&gt;하지만 이 모델을 그대로 칼만 필터에 적용할 경우 아래 이미지와 같은 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;/assets/img/kalman filter/ekf_problem.png&quot; align=&quot;left&quot; /&gt;&lt;br /&gt;
&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;/assets/img/kalman filter/ekf_problem2.png&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 이미지처럼 선형 모델에선 입력이 가우시안 분포를 따른다면 선형성에 의해서 출력도 가우시안 분포를 따릅니다. 하지만 비선형 모델에서는 입력이 가우시안 분포를 따른다고 해도, 출력은 가우시안 분포를 따르지 않는 문제가 발생합니다. 이런 점을 해결하기 위해서 비선형 함수의 선형화(Linearization) 시키는 과정이 필요합니다.&lt;/p&gt;

&lt;h2 id=&quot;선형화linearization&quot;&gt;선형화(Linearization)&lt;/h2&gt;
&lt;p&gt;EKF에서 비선형 함수를 선형화 시키기 위해서 1차 Taylor 근사법을 사용하는데, 선형 근사화된 모델은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Motion model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$g(u_t,x_{t-1})\approx g(u_t, \mu_{t-1})+\frac{\partial g(u_t, \mu_{t-1})}{\partial x_{t-1}}(x_{t-1}-\mu_{t-1})$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;Observation model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$h(x_t)\approx h(\overline{\mu}_t)+\frac{\partial h(\overline{\mu}_t)}{\partial x_t}(x_t-\overline{\mu}_t)$&lt;/center&gt;

&lt;p&gt;이때 비선형 함수를 state $x_t$로 편미분하여 matrix를 생성하는데 이 행렬을 &lt;strong&gt;Jacobian&lt;/strong&gt;이라고 합니다. 두 matrix는 $G_t=\frac{\partial g(u_t, \mu_{t-1})}{\partial x_{t-1}}$과 $H_t=\frac{\partial h(\overline{\mu}_t)}{\partial x_t}$로 표기합니다.&lt;/p&gt;

&lt;p&gt;아래 그림은 Talyer 근사화를 통해 선형화를 하였을 때의 특징을 보여줍니다.
&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;/assets/img/kalman filter/ekf_solve.png&quot; align=&quot;left&quot; /&gt;&lt;br /&gt;
&lt;img width=&quot;50%&quot; height=&quot;50%&quot; src=&quot;/assets/img/kalman filter/ekf_solve2.png&quot; align=&quot;right&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 그림은 입력의 분산(다변수에서는 공분산)이 클 때 평균이 실제값과 크게 차이나는 결과를 볼 수 있습니다. 오른쪽 그림은 입력의 분산이 작을 때 평균이 실제값과 차이가 작은 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;ekf-algorithm&quot;&gt;EKF algorithm&lt;/h2&gt;
&lt;p&gt;선형화된 motion 모델과 observation 모델을 이용한 bayes filter는 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;linearized prediction model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$p(x_t|u_t,x_{t-1}) \approx \frac{1}{ \sqrt{det(2\pi R_t)}}e^{-\frac{1}{2}(x_{t}-g(u_t,x_{t-1}))^T R_{t}^{-1}(x_t-g(u_t,x_{t-1}))}$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;linearized correction model&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$p(z_t|x_t) \approx \frac{1}{ \sqrt{det(2\pi Q_t)}}e^{-\frac{1}{2}(x_{t}-h(\overline{\mu}_t))^T Q_{t}^{-1}(x_t-h(\overline{\mu}_t))}$&lt;/center&gt;

&lt;p&gt;Kalman Filter와 마찬가지로 $R_t$, $Q_t$는 process noise, measurement noise입니다. EKF 알고리즘은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;
$\begin{align*} 1&amp;amp;: Extended\;Kalman\;Filter(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t) \\
&amp;amp; [Prediction\;step] \\
2&amp;amp;: \overline{\mu}_t=g(u_t, \mu_{t-1}) \\
3&amp;amp;: \overline{\Sigma}_t=G_t\Sigma_{t-1}G_t^T+R_t \\
&amp;amp; [Correction\;step] \\
4&amp;amp;: K_t=\overline{\Sigma}_tH_t^T(H_t\overline{\Sigma}_tH_t^T+Q_t)^{-1}\\
5&amp;amp;: \mu_t = \overline{\mu}_t+K_t(z_t-h(\overline{\mu}_t)) \\
6&amp;amp;: \Sigma_t = (I-K_tH_t)\overline{\Sigma}_t \\ 
7&amp;amp;: return\;\mu_t, \Sigma_t
\end{align*}$
&lt;/center&gt;

&lt;p&gt;EKF 알고리즘과 KF 알고리즘의 차이는 무엇보다 비선형 함수의 선형화입니다. KF의 3,4번 식의 $A_t$ 행렬은 EKF에서 $G_t$ 행렬로, $H_t$ 행렬은 $h(\overline{\mu}&lt;em&gt;t)$ 함수의 자코비안 행렬 $H_t$로 대체되었습니다. 여기서 $R_t$는 process noise이며, control input의 공분산 행렬이 $M_t$일 때 $R_t=V_tM_tV&lt;/em&gt;{t}^T$입니다. 여기서 $V_t$는 $g(u_t, \mu_{t-1})$를 control input인 $u_t$로 편미분한 자코비안입니다.&lt;/p&gt;

&lt;h2 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://jinyongjeong.github.io/2017/02/14/lec03_kalman_filter_and_EKF/&quot;&gt;Jinyoung - [SLAM] Kalman filter and EKF(Extended Kalman Filter)&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Kalman Filter" /><category term="EKF" /><summary type="html">이 게시글에서는 SLAM에서의 KF와 EKF를 기준으로 설명할 예정입니다. Kalman Filter란? 칼만 필터는 현재 상태(state) $\mu$와 공분산(covariance) $\Sigma$를 기반으로 다음 상태를 추정하는 방법입니다. $\mu$와 $\Sigma$만으로 나타낼 수 있는 Gaussian 분포를 따르며, 지난 게시글의 Bayesian Filter와도 관계가 있습니다. 선형 칼만 필터를 설명하기 전에 가우시안 분포에 대해서 조금 설명하도록 하겠습니다.</summary></entry><entry><title type="html">[Udacity] Deep Learning (7) - CNN 구성과 LeNet-5</title><link href="https://refstop.github.io/uda-lenet.html" rel="alternate" type="text/html" title="[Udacity] Deep Learning (7) - CNN 구성과 LeNet-5" /><published>2021-07-05T01:11:24+09:00</published><updated>2021-07-05T01:11:24+09:00</updated><id>https://refstop.github.io/uda-lenet</id><content type="html" xml:base="https://refstop.github.io/uda-lenet.html">&lt;p&gt;지난 게시글에 이어 CNN의 구성과 LeNet-5를 예시로 파라미터를 계산해 보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input 1 is the X coordinate of the point,
Input 2 is the y coordinate of the point,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;^^^왜인지 버그가 나서 아무말이나 지껴둔것&lt;/p&gt;

&lt;h2 id=&quot;1-cnn의-구성&quot;&gt;1. CNN의 구성&lt;/h2&gt;
&lt;p&gt;CNN의 구성을 공부하는 예시로서 대표적으로 사용되는 LeNet-5 모델으로 정리하겠습니다.&lt;/p&gt;
&lt;h3 id=&quot;lenet-5-네트워크란&quot;&gt;LeNet-5 네트워크란?&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/deeplearning/lenet.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;LeNet-5 네트워크는 CNN을 처음으로 개발한 Yann Lecun 연구팀이 개발한 CNN 알고리즘의 이름입니다. 이 알고리즘이 소개된 논문 제목은 Gradient-based learning applied to document recognition”입니다.&lt;br /&gt;
LeNet-5 네트워크는 Input-C1-S2-C3-S4-C5-F6-Output으로 이루어져 있고, Convolution layer, Pooling layer 두 쌍(C1-S2-C3-S4)과 Flatten layer(C5), Fully Connected layer(F6) 1개로 구성되어 있습니다. 원래 논문에서는 활성화 함수로서 tanh 함수를 사용했지만, 제 코드에서는 ReLU 함수를 사용하였습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten

model = keras.Sequential()

model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Flatten())

model.add(layers.Dense(units=120, activation='relu'))

model.add(layers.Dense(units=84, activation='relu'))

model.add(layers.Dense(units=10, activation = 'softmax'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실제 사용결과는 나중에 올리도록 하겠습니다….&lt;/p&gt;

&lt;h2 id=&quot;2-cnn-입출력-파라미터-계산&quot;&gt;2. CNN 입출력, 파라미터 계산&lt;/h2&gt;
&lt;p&gt;사용한 예시에서의 입출력 표를 정리하면 다음과 같습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;center&gt;layer&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Input Channel&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Filter&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Output Channel&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Stride&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Max Pooling&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;&lt;center&gt;Activation Function&lt;/center&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Convolution Layer 1&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;3 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(3,3) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;relu &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Max Pooling Layer 1&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(2,2) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Convolution Layer 2&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(4,4) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;relu &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Max Pooling Layer 2&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(2,2) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Flatten&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Fully Connected layer&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;softmax &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Convolution layer의 학습 파라미터 수는 &lt;strong&gt;입력채널수 $\times$ 필터 폭 $\times$ 필터 높이 $\times$ 출력채널수&lt;/strong&gt;로 계산됩니다.&lt;/p&gt;

&lt;h3 id=&quot;21-layer-1의-shape와-파라미터&quot;&gt;2.1 Layer 1의 Shape와 파라미터&lt;/h3&gt;
&lt;p&gt;Layer 1은 1개의 Convolution layer와 1개의 Pooling layer로 구성되어 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;211-convolution-layer-1&quot;&gt;2.1.1 Convolution layer 1&lt;/h4&gt;
&lt;p&gt;Convolution layer 1의 기본 정보는 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터 Shape = (64,64,3)&lt;/li&gt;
  &lt;li&gt;입력 채널 = 3&lt;/li&gt;
  &lt;li&gt;필터 = (3,3)&lt;/li&gt;
  &lt;li&gt;출력 채널 = 6&lt;/li&gt;
  &lt;li&gt;Stride = 1
입력 이미지에 Shape가 (3,3)인 필터를 6개 적용할 경우에, 출력 데이터(Activation Map)의 Shape를 계산하는 과정은 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$Row\;Size=\frac{N-F}{Stride}+1=\frac{64-3}{1}+1=62$&lt;/center&gt;
&lt;center&gt;$Column\;Size=\frac{N-F}{Stride}+1=\frac{64-3}{1}+1=62$&lt;/center&gt;
&lt;p&gt;위 식으로 계산된 Activation Map 사이즈는 (62,62,6)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 채널: 3&lt;/li&gt;
  &lt;li&gt;출력 데이터(Activation Map) Shape: (62,62,6)&lt;/li&gt;
  &lt;li&gt;학습 파라미터: 162개 (3$\times$3$\times$3$\times$6)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;212-max-pooling-layer-1&quot;&gt;2.1.2 Max Pooling layer 1&lt;/h4&gt;
&lt;p&gt;Max Pooling layer 1의 입력 데이터 Shape은 (62,62,6)입니다. 채널 수는 바뀌지 않고, Max Pooling 사이즈가 (2,2)이기 때문에 출력 데이터 크기는 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$Row\;Size=\frac{62}{2}=31$&lt;/center&gt;
&lt;center&gt;$Column\;Size=\frac{62}{2}=31$&lt;/center&gt;
&lt;p&gt;위 식으로 계산된 출력 사이즈는 (31,31,6)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 채널: 6&lt;/li&gt;
  &lt;li&gt;출력 데이터 Shape: (31,31,6)&lt;/li&gt;
  &lt;li&gt;학습 파라미터: 0개&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-layer-2의-shape와-파라미터&quot;&gt;2.2 Layer 2의 Shape와 파라미터&lt;/h3&gt;
&lt;p&gt;Layer 2도 마찬가지로 Convolution layer와 Pooling layer로 구성되어 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;221-convolution-layer-2&quot;&gt;2.2.1 Convolution layer 2&lt;/h4&gt;
&lt;p&gt;Convolution layer 2의 기본 정보는 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터 Shape = (31,31,6)&lt;/li&gt;
  &lt;li&gt;입력 채널 = 6&lt;/li&gt;
  &lt;li&gt;필터 = (4,4)&lt;/li&gt;
  &lt;li&gt;출력 채널 = 16&lt;/li&gt;
  &lt;li&gt;Stride = 1
입력 이미지에 Shape가 (4,4)인 필터를 16개 적용할 경우에, 출력 데이터(Activation Map)의 Shape를 계산하는 과정은 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$Row\;Size=\frac{N-F}{Stride}+1=\frac{31-4}{1}+1=28$&lt;/center&gt;
&lt;center&gt;$Column\;Size=\frac{N-F}{Stride}+1=\frac{31-4}{1}+1=28$&lt;/center&gt;
&lt;p&gt;위 식으로 계산된 Activation Map 사이즈는 (28,28,16)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 채널: 6&lt;/li&gt;
  &lt;li&gt;출력 데이터(Activation Map) Shape: (28,28,16)&lt;/li&gt;
  &lt;li&gt;학습 파라미터: 1536개 (6$\times$4$\times$4$\times$16)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;222-max-pooling-layer-2&quot;&gt;2.2.2 Max Pooling layer 2&lt;/h4&gt;
&lt;p&gt;Max Pooling layer 1의 입력 데이터 Shape은 (28,28,16)입니다. 채널 수는 바뀌지 않고, Max Pooling 사이즈가 (2,2)이기 때문에 출력 데이터 크기는 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$Row\;Size=\frac{28}{2}=14$&lt;/center&gt;
&lt;center&gt;$Column\;Size=\frac{28}{2}=14$&lt;/center&gt;
&lt;p&gt;위 식으로 계산된 출력 사이즈는 (14,14,16)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 채널: 16&lt;/li&gt;
  &lt;li&gt;출력 데이터 Shape: (14,14,16)&lt;/li&gt;
  &lt;li&gt;학습 파라미터: 0개&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-flatten-layer&quot;&gt;2.3 Flatten layer&lt;/h3&gt;
&lt;p&gt;Flatten layer는 CNN의 데이터를 Fully Connected Nerual Network에 사용되는 1차원 벡터로 바꿔주는 layer입니다. 파라미터는 존재하지 않고 이전 layer의 출력을 1차원 벡터로 바꾸는 역할만 합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터 Shape: (14,14,16)&lt;/li&gt;
  &lt;li&gt;출력 데이터 Shape: (3136,1)
원래 LeNet-5 네트워크에서는 Flatten 층의 입력 데이터 Shape와 같은 크기의 필터를 Convolution하고 출력 채널을 조절하여 1차원 벡터의 형태로 변환합니다. 하지만 Flatten 함수로도 충분히 이미지의 특성을 지닌 1차원 벡터로 변환할 수 있으므로 Flatten 함수를 사용합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;24-fully-connected-layersoftmax-layer&quot;&gt;2.4 Fully Connected layer(Softmax layer)&lt;/h3&gt;
&lt;p&gt;마지막 출력층에 해당하는 FNN입니다. 입력 데이터의 class를 분류해야 하는 layer이기 때문에 Softmax 함수를 사용하여 출력층을 구성합니다. 입력 데이터의 Shape는 이전 Flatten layer의 출력인 (3136,1)입니다. 최종 출력 데이터는 원래 입력 데이터의 분류 class만큼이 되므로, 예를 들어 분류할 class가 10개라면 출력 데이터의 Shape는 (10,1)이 됩니다. 파라미터를 계산하면 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터 Shape: (3136,1)&lt;/li&gt;
  &lt;li&gt;출력 데이터 Shape: (10,1)&lt;/li&gt;
  &lt;li&gt;Softmax layer의 파라미터 수: 31360개 (3136$\times$10)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-전체-파라미터-수와-레이어별-inputoutput-요약&quot;&gt;3. 전체 파라미터 수와 레이어별 Input/Output 요약&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;center&gt;layer&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Input Channel&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Filter&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Output Channel&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Stride&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Max Pooling&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;&lt;center&gt;Activation Function&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Input Shape&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;center&gt;Output Shape&lt;/center&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;&lt;center&gt;파라미터 수&lt;/center&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Conv1&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;3 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(3,3) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;relu &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(64,64,3) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(62,62,6) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;162 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;MaxPooling1&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(2,2) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(62,62,6) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(31,31,6) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;0 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Conv2&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;6 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(4,4) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;relu &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(31,31,6) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(28,28,16) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;1536 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;MaxPooling2&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;16 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;1 &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(2,2) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(28,28,16) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(14,14,16) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;0 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;Flatten&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(14,14,16) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(3136,1) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;0 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;FC&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;softmax &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(3136,1) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;(10,1) &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;31360 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;center&gt;합계&lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;X &lt;/center&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;center&gt;33058 &lt;/center&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;4-fully-connected-nerual-network와의-비교&quot;&gt;4. Fully Connected Nerual Network와의 비교&lt;/h2&gt;
&lt;p&gt;이 모델을 FNN의 파라미터 수와 비교한다면, 우선 입력층의 데이터가 (64,64,3)이므로 $64^2\times3=12288$개의 원소를 가진 1차원 벡터가 됩니다. 또한 이 벡터를 Fully Connected layer에 입력하여 class 개수가 10개인 출력을 갖는다면 파라미터 수는 $12288\times10=122880$개가 됩니다. layer를 하나만 가지고 있음에도 불구하고 CNN 구조의 네트워크보다 약 4배 더 많은 연산량을 가지게 됩니다. 따라서 이러한 점에서 CNN은 FNN에 비해 학습이 쉽고 네트워크 처리 속도가 빠르다는 장점을 갖고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://taewan.kim/post/cnn/&quot;&gt;TAEWAN.KIM 블로그 - “CNN, Convolutional Neural Network 요약”&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Udacity" /><category term="Deep Learning" /><summary type="html">지난 게시글에 이어 CNN의 구성과 LeNet-5를 예시로 파라미터를 계산해 보겠습니다.</summary></entry><entry><title type="html">[Udacity] Deep Learning (6) - Convolutional Nerual Network</title><link href="https://refstop.github.io/uda-cnn.html" rel="alternate" type="text/html" title="[Udacity] Deep Learning (6) - Convolutional Nerual Network" /><published>2021-07-04T15:41:24+09:00</published><updated>2021-07-04T15:41:24+09:00</updated><id>https://refstop.github.io/uda-cnn</id><content type="html" xml:base="https://refstop.github.io/uda-cnn.html">&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; height=&quot;100%&quot; src=&quot;/assets/img/deeplearning/cnn.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;지금까지 우리는 심층 신경망(Deep Nerual Network)을 구성하기까지의 과정을 살펴보았습니다. 이제 이 심층 신경망으로 구성할 수 있는 네트워크 중 이미지 학습에 최적화된 &lt;strong&gt;합성곱 신경망(Convolutional Nerual Network, CNN)&lt;/strong&gt;에 대해서 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;cnn이-되기까지&quot;&gt;CNN이 되기까지&lt;/h2&gt;
&lt;p&gt;CNN이 고안되기 전에는 이미지를 Fully Connected Nerual Network라는 1차원 벡터 형태의 데이터를 학습하는 형태의 신경망 구조를 사용했습니다. 하지만 FNN은 몇 가지 단점이 있습니다.&lt;br /&gt;
 첫번째로 이미지 데이터를 평면화 시키는 과정에서 이미지의 인접한 픽셀 간의 정보가 유실된다는 점입니다. FNN은 1차원 벡터 형태의 데이터를 입력받기 때문에 이미지의 인접 픽셀간의 상관관계를 반영할 수 없습니다. 반면 CNN은 이미지의 특징을 보존한 채로 학습을 진행하기 때문에 인접 픽셀간의 정보 유식을 막을 수 있습니다.&lt;br /&gt;
 두번째 FNN의 문제점은 막대한 양의 model parameter입니다. 만약 FNN을 사용하여 (1024,1024)크기의 컬러 이미지를 처리하고자 한다면 FNN에 입력되는 벡터의 차원은 1024$\times$1024$\times$3=315만 개입니다. 약 300만 차원의 데이터를 처리하기 위해서는 막대한 양의 뉴런이 필요하고 이에 따라 model parameter의 개수는 더욱 많은 양이 필요할 것입니다. 하지만 CNN의 경우 필터들을&lt;/p&gt;

&lt;h2 id=&quot;1-cnn의-주요-용어-정리&quot;&gt;1. CNN의 주요 용어 정리&lt;/h2&gt;
&lt;p&gt;CNN에는 다음과 같은 용어들이 사용됩니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convolution(합성곱)&lt;/li&gt;
  &lt;li&gt;채널(Channel)&lt;/li&gt;
  &lt;li&gt;필터(Filter)&lt;/li&gt;
  &lt;li&gt;커널(Kernel)&lt;/li&gt;
  &lt;li&gt;스트라이드(Stride)&lt;/li&gt;
  &lt;li&gt;패딩(Padding)&lt;/li&gt;
  &lt;li&gt;피처 맵(Feature Map)&lt;/li&gt;
  &lt;li&gt;액티베이션 맵(Activation Map)&lt;/li&gt;
  &lt;li&gt;풀링(Pooling) 레이어
각 용어에 대해서 간략하게 정리하겠습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;11-convolution-layer합성곱&quot;&gt;1.1 Convolution Layer(합성곱)&lt;/h3&gt;
&lt;p&gt;Convolution은 합성곱이라는 의미인데, input 함수(신호)와 임펄스 응답 함수(신호)를 반전 이동한 값을 곱하여 적분한 값입니다. 원래의 의미는 이렇지만, 이해하기 너무 어려우므로 다음 gif 파일을 통해 쉽게 알 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/conv.gif&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;위의 gif 파일은 입력된 이미지 데이터를 필터를 통해 Feature Map을 만드는 과정입니다. 필터로 이미지를 훑어가면서 각각의 합성곱 결과를 저장하여 Feature Map을 구성합니다. CNN의 첫번째 과정인 Convolution Layer는 바로 이 Feature Map을 생성하는 과정입니다. Feature Map을 생성한 후 활성화 함수(Activation Function)에 통과시킨 출력을 액티베이션 맵(Activation Map)이라고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;12-채널channel&quot;&gt;1.2 채널(Channel)&lt;/h3&gt;
&lt;p&gt;채널은 이미지를 구성하고 있는 실수값의 차원입니다. 이미지를 색 공간에 따라서 분리할 때, RGB의 3채널로 분리할 수 있습니다. 이미지를 구성하는 각 픽셀은 실수로 표현한 3차원 데이터입니다. 각각의 차원은 R, G, B의 3색을 나타내는 실수로, 이때의 RGB를 채널이라고 합니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/channel.jpg&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Convolution Layer에 입력되는 데이터는 필터가 적용되고, 하나의 필터당 하나의 Feature Map이 생성됩니다. 따라서 Convolution Layer에 n개의 필터가 적용된다면 출력되는 Feature Map은 n개의 채널을 갖게 됩니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/nfeaturemap.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;하지만 이번 게시글에서는 필터의 개수가 1개뿐인 모델만 다룰 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;13-필터filter-커널kernel--스트라이드stride&quot;&gt;1.3 필터(Filter), 커널(Kernel) &amp;amp; 스트라이드(Stride)&lt;/h3&gt;
&lt;p&gt;필터는 이미지의 특징을 찾아내기 위한 공용 파라미터입니다. 일반적으로 (4,4), (3,3)같은 정방행렬로 표현되고 사용 방법은 1.1의 gif 이미지에 잘 표현되어 있습니다. 필터는 이미지를 순회하면서 합성곱을 계산하여 Feature Map을 생성합니다. 이때 필터가 한번에 이동하는 간격을 Stride라고 합니다. Stride는 직역하면 보폭이라는 뜻으로 말 그대로 “필터가 한 걸음에 얼마나 가냐”를 의미하는 값입니다. 위의 gif 이미지의 Stride는 1입니다. 아래 이미지는 Stride가 1이고, 이미지 픽셀이 (16,16), 필터 크기가 (2,2)일때의 Feature Map 생성 과정을 나타낸 사진입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/filter.jpg&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;입력 데이터가 여러 채널을 갖는 경우, 각 채널의 Feature Map을 모두 더한 값이 최종 출력이 됩니다. 따라서 입력 데이터의 채널 수와는 상관없이 필터의 개수에 따라 결정된다는 것을 알 수 있습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/channelfeaturemap.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;14-패딩padding&quot;&gt;1.4 패딩(padding)&lt;/h3&gt;
&lt;p&gt;패딩은 Convolution layer에서 필터와 Stride의 작용으로 출력 Feature Map의 크기가 줄어드는 현상을 방지하고 출력 데이터의 사이즈를 조정하는 방법입니다. 단어 그대로 입력데이터에 패드를 부착하는 것처럼 이미지 외곽에 지정된 픽셀만큼 특정 값으로 채워넣습니다. 보통 CNN에서는 패딩값을 0으로 채웁니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/padding.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;위의 이미지는 원래 입력 데이터 32$\times$32$\times$3 이미지에 2픽셀만큼의 패딩을 추가하여 36$\times$36$\times$3 이미지로 만든 것입니다. 출력 데이터의 사이즈를 조절하는 기능 이외에도 Convolution layer가 이미지의 외곽을 인식하는 학습 효과도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;15-pooling-layer&quot;&gt;1.5 Pooling Layer&lt;/h3&gt;
&lt;p&gt;Pooling layer는 Convolution layer의 출력인 Activation Map을 입력으로 받아서 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용됩니다. Pooling 처리 과정은 지정된 정방행렬 범위 내의 데이터를 Pooling 방식에 따라서 처리합니다. Pooling layer를 처리하는 방법으로는 Max pooling, Min pooling, Average pooling이 있습니다. 이름만 봐도 알 수 있듯이 각각 최댓값, 최솟값, 평균값만을 남기거나 계산하는 방식입니다. 다음 이미지를 참고하면 이해하기 쉽습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/maxpooling.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Pooling layer의 처리과정은 Convolution layer와 거의 비슷하지만 조금의 차이점이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;학습대상 파라미터가 없음&lt;/li&gt;
  &lt;li&gt;행렬의 사이즈 감소&lt;/li&gt;
  &lt;li&gt;채널 수 변경 없음
CNN에서는 주로 Max pooling을 사용합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-레이어별-출력-데이터-계산&quot;&gt;2. 레이어별 출력 데이터 계산&lt;/h2&gt;
&lt;p&gt;Convolution layer와 Pooling layer의 출력 데이터 크기를 계산하는 방법을 정리했습니다.&lt;/p&gt;
&lt;h3 id=&quot;21-convolution-layer-출력-데이터-크기-계산&quot;&gt;2.1 Convolution layer 출력 데이터 크기 계산&lt;/h3&gt;
&lt;p&gt;입력 데이터에 대한 필터의 크기와 Stride 크기에 따라서 Feature Map 크기가 결정됩니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터 높이: H&lt;/li&gt;
  &lt;li&gt;입력 데이터 폭: W&lt;/li&gt;
  &lt;li&gt;필터 높이: FH&lt;/li&gt;
  &lt;li&gt;필터 폭: FW&lt;/li&gt;
  &lt;li&gt;Stride 크기: S&lt;/li&gt;
  &lt;li&gt;Padding 사이즈: P&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$Output\;Height=OH=\frac{(H+2P-FH)}{S}+1$&lt;/center&gt;
&lt;center&gt;$Output\;Width=OW=\frac{(W+2P-FW)}{S}+1$&lt;/center&gt;
&lt;p&gt;위 식의 결과는 자연수가 되어야 합니다. 또한 Convolution layer 다음에 Pooling layer가 온다면, Feature Map의 행과 열 크기는 Pooling 크기의 배수여야 합니다. 만약 Pooling 사이즈가 (3,3)이라면 위 식의 결과는 자연수이고 3의 배수여야 합니다. 이 조건을 만족하도록 Filter의 크기, Stride의 간격, Pooling 크기 및 패딩 크기를 조절해야 합니다.&lt;/p&gt;

&lt;h3 id=&quot;22-pooling-layer-출력-데이터-크기-계산&quot;&gt;2.2 Pooling layer 출력 데이터 크기 계산&lt;/h3&gt;
&lt;p&gt;Pooling layer의 Pooling 사이즈는 일반적으로 정방행렬입니다. 또한 Convolution layer의 출력이 Pooling 사이즈의 정수배가 되도록 하여 Pooling layer의 출력 사이즈를 결정하게 됩니다. 예를 들어 Convolution layer의 출력 Activation Map 사이즈가 (6,6)이고 Pooling 사이즈가 (3,3)이면, Pooling layer의 출력 사이즈는 (2,2)가 됩니다. 따라서 Pooling layer의 출력 사이즈는 다음과 같이 계산할 수 있습니다.&lt;/p&gt;
&lt;center&gt;$Output\;Row\;Size=\frac{Input\;Row\;Size}{Pooling\;Size}$&lt;/center&gt;
&lt;center&gt;$Output\;Column\;Size=\frac{Input\;Column\;Size}{Pooling\;Size}$&lt;/center&gt;

&lt;h2 id=&quot;마무리&quot;&gt;마무리&lt;/h2&gt;
&lt;p&gt;이번 게시글에서는 CNN에서 사용되는 용어 및 입출력 데이터의 크기를 계산하는 방법을 알아보았습니다. 다음 게시글은 이 게시글에 이어 LeNet-5 네트워크 구성과 직접적인 예시를 들어 파라미터를 계산해 보도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://taewan.kim/post/cnn/&quot;&gt;TAEWAN.KIM 블로그 - “CNN, Convolutional Neural Network 요약”&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://untitledtblog.tistory.com/150&quot;&gt;Untitled 블로그 - “[머신 러닝/딥 러닝] 합성곱 신경망 (Convolutional Neural Network, CNN)과 학습 알고리즘”&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://yjjo.tistory.com/8&quot;&gt;YJJo 블로그 - “Convolution Nerual Networks (합성곱 신경망)”&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Udacity" /><category term="Deep Learning" /><summary type="html">지금까지 우리는 심층 신경망(Deep Nerual Network)을 구성하기까지의 과정을 살펴보았습니다. 이제 이 심층 신경망으로 구성할 수 있는 네트워크 중 이미지 학습에 최적화된 합성곱 신경망(Convolutional Nerual Network, CNN)에 대해서 알아보겠습니다.</summary></entry><entry><title type="html">Indoor 2D Navigation - 개요 &amp;amp; 좌표범위내 신호 출력 노드</title><link href="https://refstop.github.io/i2n-signal.html" rel="alternate" type="text/html" title="Indoor 2D Navigation - 개요 &amp;amp; 좌표범위내 신호 출력 노드" /><published>2021-03-02T12:41:24+09:00</published><updated>2021-03-02T12:41:24+09:00</updated><id>https://refstop.github.io/i2n-signal</id><content type="html" xml:base="https://refstop.github.io/i2n-signal.html">&lt;h2 id=&quot;프로젝트의-목적&quot;&gt;프로젝트의 목적&lt;/h2&gt;
&lt;p&gt;본 프로젝트의 목적은 Kobuki 로봇을 사용하여 대학의 로봇관 건물을 자율주행 할 수 있도록 여러 SLAM 기법이나 Navigation 패키지를 사용해 보고, ROS 패키지를 제작하여 ROS와 우분투 환경에 익숙해지는 것입니다. 차후 있을 3D 라이다를 사용한 실내 자율주행 로봇 과제 등을 수행하기 위해 기본적인 사용법을 익히는 것을 목표로 하고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;프로젝트-계획&quot;&gt;프로젝트 계획&lt;/h2&gt;
&lt;p&gt;프로젝트의 진행 계획은 다음과 같습니다. 이 계획은 진행 상황에 따라 수정될 수 있습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Kobuki 패키지 작성&lt;/li&gt;
  &lt;li&gt;map에서 특정 좌표를 기준으로 일정 범위 내에 위치하면 신호 출력 노드&lt;/li&gt;
  &lt;li&gt;좌표를 파라미터화(yaml, dynamic_reconfigure)&lt;/li&gt;
  &lt;li&gt;gmapping, cartographer mapping 실습&lt;/li&gt;
  &lt;li&gt;amcl, cartographer pure localization을 사용한 localization&lt;/li&gt;
  &lt;li&gt;move_base를 사용한 자율주행&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;패키지-작성-전-prerequisite&quot;&gt;패키지 작성 전 Prerequisite&lt;/h2&gt;
&lt;h3 id=&quot;1-kobuki-패키지-설치&quot;&gt;1. Kobuki 패키지 설치&lt;/h3&gt;
&lt;p&gt;Kobuki 운용을 위해서는 Kobuki 패키지 설치를 해야합니다. 기존에 사용하던 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catkin_ws&lt;/code&gt;를 사용해도 되지만 저는 새로운 작업공간 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kobuki_ws&lt;/code&gt;를 생성했습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mkdir -p kobuki_ws/src
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;그 후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src&lt;/code&gt;폴더에 Kobuki 패키지를 설치합니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd kobuki_ws/src
$ git clone https://github.com/yujinrobot/kobuki.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이때 이 Kobuki 패키지에 필요한 ROS 패키지들을 설치하기 위해 다음의 명령어를 통해 패키지들을 설치합니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install ros-melodic-kobuki*
$ sudo apt-get install ros-melodic-ecl*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;마지막으로 의존성을 설치하여 마무리합니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd ~/kobuki_ws
$ rosdep install --from-paths src --ignore-src -r -y
$ catkin_make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;2-turtlebot3-패키지-설치-선택&quot;&gt;2. Turtlebot3 패키지 설치 (선택)&lt;/h3&gt;
&lt;p&gt;패키지를 작성할 때 Turtlebot3 패키지를 참고할 일이 많기 때문에 Turtlebot3 패키지를 설치해 두는 편이 좋습니다. 직접적인 사용은 하지 않을 것이기에, 설치하기 싫다면 필요할 때마다 &lt;a href=&quot;https://github.com/ROBOTIS-GIT/turtlebot3.git&quot;&gt;Turtlebot3 github 링크&lt;/a&gt;를 참고해도 괜찮습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd ~/kobuki_ws/src
$ git clone -b melodic-devel --single-branch https://github.com/ROBOTIS-GIT/turtlebot3.git
$ cd ~/kobuki_ws
$ rosdep install --from-paths src --ignore-src -r -y
$ catkin_make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;map에서-특정-좌표-범위-내-위치시-신호-출력-노드&quot;&gt;map에서 특정 좌표 범위 내 위치시 신호 출력 노드&lt;/h2&gt;
&lt;p&gt;첫번째로 저희가 시도한 것은 map의 특정 좌표를 주고, 로봇이 그 좌표로부터 일정 범위 내에 들어가면 신호를 보내는 것이었습니다. map 좌표계로부터 로봇의 base_link(base_footprint) 좌표계로 transform(좌표계 변환)하여 map 좌표계를 기준으로 로봇이 어느 좌표에 있는지를 알 수 있습니다.&lt;/p&gt;
&lt;center&gt; &lt;iframe width=&quot;560&quot; height=&quot;400&quot; src=&quot;https://youtu.be/pEbvt-Pv_hU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt; &lt;/center&gt;
&lt;p&gt;사실 조금 진행된 프로젝트라서 이미 앞서 정리할 여러 기능들이 추가되어 있습니다. 동영상에 나오는 기능은 신호 출력 노드와 dynamic reconfigure(동적 파라미터 수정), 그 외에 Kobuki를 조종하거나 lidar 동작 기능을 포함하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;좌표를-파라미터화-yaml-dynamic-reconfigure&quot;&gt;좌표를 파라미터화 (yaml, dynamic reconfigure)&lt;/h2&gt;
&lt;h3 id=&quot;yaml-파일-작성&quot;&gt;yaml 파일 작성&lt;/h3&gt;
&lt;p&gt;yaml 파일은 패키지의 설정값을 저장하는 파일입니다. 노드는 이 파일을 참고하여 소스에 값을 전달합니다. 예를 들어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;point.yaml&lt;/code&gt; 파일의 내용물이 다음과 같다고 합시다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x: 13
y: 20
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;저희가 만든 패키지의 소스에 필요한 특정 좌표 부분을 yaml 파일로 작성한 것입니다. 노드를 실행하였을 때, 자동으로 yaml 파일을 참고하여 $x$, $y$ 값을 가져오면, 매번 소스를 건드리지 않고도 설정을 바꿔 줄 수 있습니다. 하지만 yaml 파일을 수정할 때 마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catkin_make&lt;/code&gt;를 다시 해 주어야 하니 번거롭기는 합니다.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-reconfigure&quot;&gt;dynamic reconfigure&lt;/h3&gt;
&lt;p&gt;dynamic reconfigure는 동적 파라미터 수정으로, 노드에 설정되는 파라미터를 수정해 줌으로서 프로그램 실행 중에도 계속 파라미터를 바꿀 수 있도록 하는 도구입니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rosparam&lt;/code&gt; 명령어 또는 yaml 파일로도 계속하여 수정해 줄 수 있지만 매번 명령어를 치거나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catkin_make&lt;/code&gt;를 해야 하는 번거로움이 있습니다. dynamic reconfigure는 GUI를 지원하는 rqt를 통해 수정할 수 있습니다. 이번 프로젝트 같은 경우 처음에는 특정 좌표를 소스 상에서 설정해 주었으나, dynamic reconfigure 기능을 통해 좌표를 계속해서 재설정할 수 있도록 기능을 추가하였습니다. 소스는 다음과 같습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;ros/ros.h&amp;gt;
#include &amp;lt;dynamic_reconfigure/server.h&amp;gt;
#include &amp;lt;indoor_2d_nav/dyn_reconfig_testConfig.h&amp;gt;

void callback(indoor_2d_nav::dyn_reconfig_testConfig &amp;amp;config, uint32_t level) {
    ROS_INFO(&quot;\nstr_posexy: %s&quot;, config.str_posexy);
}

int main(int argc, char **argv) {

    ros::init(argc, argv, &quot;dynamic_reconfigure_test&quot;);

    dynamic_reconfigure::Server&amp;lt;indoor_2d_nav::dyn_reconfig_testConfig&amp;gt; server;
    dynamic_reconfigure::Server&amp;lt;indoor_2d_nav::dyn_reconfig_testConfig&amp;gt;::CallbackType f;

    f = boost::bind(&amp;amp;callback, _1, _2);
    server.setCallback(f);

    ros::spin();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;다음-게시글&quot;&gt;다음 게시글&lt;/h2&gt;
&lt;p&gt;오랜만에 게시글을 수정하여 바뀐 점이 많습니다. 우선 다음 게시글에서는 Cartographer 사용법에 관하여 정리하도록 하겠습니다.&lt;/p&gt;</content><author><name>Refstop</name></author><category term="실내 자율 주행" /><category term="Kobuki" /><summary type="html">프로젝트의 목적 본 프로젝트의 목적은 Kobuki 로봇을 사용하여 대학의 로봇관 건물을 자율주행 할 수 있도록 여러 SLAM 기법이나 Navigation 패키지를 사용해 보고, ROS 패키지를 제작하여 ROS와 우분투 환경에 익숙해지는 것입니다. 차후 있을 3D 라이다를 사용한 실내 자율주행 로봇 과제 등을 수행하기 위해 기본적인 사용법을 익히는 것을 목표로 하고 있습니다.</summary></entry><entry><title type="html">[Udacity] Deep Learning (5) - Deep Nerual Network</title><link href="https://refstop.github.io/uda-dnn.html" rel="alternate" type="text/html" title="[Udacity] Deep Learning (5) - Deep Nerual Network" /><published>2021-03-02T12:34:24+09:00</published><updated>2021-03-02T12:34:24+09:00</updated><id>https://refstop.github.io/uda-dnn</id><content type="html" xml:base="https://refstop.github.io/uda-dnn.html">&lt;h1 id=&quot;deep-nerual-network&quot;&gt;Deep Nerual Network&lt;/h1&gt;
&lt;p&gt;지금까지 정리한 Nerual Network는 모두 layer가 하나뿐인 단층 신경망(Single-Layer Perceptron)입니다. 하지만 단층 신경망으로는 비선형 모델을 구현할 수 없습니다. &lt;a href=&quot;https://refstop.github.io/udacity/deep%20learning/uda-dl-nnnl#1&quot;&gt;지난 게시글&lt;/a&gt;에서 언급한 바와 같이 비선형 모델은 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 이루어진 $^{1)}$심층 신경망(Deep Neural Network)으로 구현할 수 있습니다. 따라서 인공 신경망의 성능 향상을 위해서는 심층 신경망의 사용은 필수적이라고 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;relu-activation-function&quot;&gt;ReLU Activation Function&lt;/h2&gt;
&lt;p&gt;심층 신경망에 대해서 정리하기 전에 &lt;strong&gt;ReLU 활성화 함수&lt;/strong&gt;에 대해서 알아보고 넘어가겠습니다. 활성화 함수는 각 층의 신경망의 출력을 결정하는 함수입니다. 또한 활성화 함수가 없다면 심층 신경망은 $y=W_{1}W_{2}W_{3} \cdots x=Wx$가 되므로 비선형 모델이라고 볼 수 없는 결과가 나옵니다. 이러한 문제를 해결하기 위해서 넣은 활성화 함수이지만, 지금까지 우리가 잘 사용했던 Sigmoid 활성화 함수에는 단점이 있습니다. 다음은 Sigmoid 함수의 미분 그래프를 나타낸 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;70%&quot; height=&quot;70%&quot; src=&quot;/assets/img/deeplearning/sigmoid_deriv.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Sigmoid 함수의 단점은 미분값의 범위가 0~0.25라는 점입니다. &lt;a href=&quot;https://refstop.github.io/udacity/deep%20learning/uda-dl-nnnl#3&quot;&gt;Cross-Entropy 오차 역전파 방법&lt;/a&gt;을 사용할 때, 활성화 함수의 미분값을 곱해주게 됩니다. 이때 활성화 함수의 미분값이 항상 1보다 작은 경우, 바로 Sigmoid 함수같은 경우 심층 신경망의 layer가 많을수록 오차에 대한 가중치의 미분, 즉 가중치가 오차에 영향을 미치는 성분이 점점 작아지고, layer가 너무 많은 경우 오차의 미분값이 0에 수렴하게 됩니다. 이 현상이 바로 &lt;strong&gt;기울기 소실(Gradient Vanishing)&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;이러한 점을 해결하기 위해서 사용하는 활성화 함수가 바로 &lt;strong&gt;ReLU(Rectified Linear Unit)&lt;/strong&gt; 함수입니다. ReLU함수의 그래프는 다음과 같은 형태입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;70%&quot; height=&quot;70%&quot; src=&quot;/assets/img/deeplearning/relu.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;기울기 소실의 근본적인 문제점은 Sigmoid 함수의 미분값 범위 0~0.25 때문에 발생했습니다. 하지만 ReLU 함수는 활성화되었을 때 값이 1, 비활성화 되었을 때 값이 0이므로 기울기 소실 문제를 해결할 수 있습니다. 역전파 연산의 결과값, 즉 가중치에 대한 오차의 미분값이 작아지지 않기 때문에 layer 개수에 관계없이 역전파 연산의 결과를 얻을 수 있습니다. 실제로 가장 많이 사용하는 활성화 함수 역시 ReLU 함수입니다.&lt;/p&gt;

&lt;h2 id=&quot;다시-심층-신경망의-본론으로&quot;&gt;다시 심층 신경망의 본론으로….&lt;/h2&gt;
&lt;p&gt;방금 알아본 바와 같이 ReLU 활성화 함수를 사용하여 심층 신경망을 구성합니다. 구성된 신경망의 형태는 이전 게시글의 &lt;a href=&quot;https://refstop.github.io/udacity/deep%20learning/uda-dl-nnnl#1&quot;&gt;비선형 모델 - 다층 신경망&lt;/a&gt;의 그림과 별 다를 바가 없지만, 아무래도 괴발개발 그린 그림(…)보다는 구글에서 퍼온 그림이 눈이 편할 것 같아 준비했습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/Neuron.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;먼저 각각의 뉴런의 구성을 그린 이미지를 준비했습니다. 위의 이미지가 바로 layer가 하나뿐인 신경망, 단층 신경망입니다. 이전 게시글에 정리했던 내용이지만, 저도 오랜만에 작성하다 보니 복습할 이미지가 필요했습니다. 이러한 뉴런을 이어붙여 만든 신경망은 다음과 같습니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/shallowNN.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;은닉층(Hidden layer)이 하나뿐인 얕은 신경망(Shallow layer)입니다. 왠지 모르게 “얕은 신경망”이라는 이름이 붙어 있지만 그냥 은닉층 1단짜리 심층 신경망입니다. 왜 이런 이름이 있지? 제 생각이지만 은닉층 1층짜리밖에 구현 못하던 시절엔 이걸 심층 신경망이라 불렀는데 더 깊게 구성할 수 있는 기술이 개발된 후 진짜 심층 신경망과 구분하기 위해 지은 이름이 아닐까요? 제 생각일 뿐이니 한귀로 듣고 한귀로 흘리시길 바랍니다….&lt;br /&gt;
다음 그림은 위의 얕은 신경망에서 발전된 형태인 진짜배기 심층 신경망입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/dnn.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;이런 식으로 다수의 뉴런으로 심층 신경망을 구현하여 가중치를 구할 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;과적합-방지법&quot;&gt;과적합 방지법&lt;/h1&gt;
&lt;p&gt;과적합은 훈련 데이터에 모델이 과도하게 회귀되어 오히려 실제 사용에서 성능이 떨어지는 현상입니다. 훈련 데이터에 대한 노이즈까지 학습을 해버려서 일어나는 현상인데, 훈련 데이터에서는 높은 정확도를 보이지만 검증 데이터나 테스트 데이터에서는 제대로 동작하지 않습니다. 이러한 현상을 방지할 수 있는 방법에 대해서 정리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;1-데이터-양을-늘리기&quot;&gt;1. 데이터 양을 늘리기&lt;/h2&gt;
&lt;p&gt;과적합 뿐만 아니라 모델 자체의 성능을 높이는 데도 좋은 방법입니다. 심층 신경망 구조는 학습시킬 데이터가 많을수록 정확도가 올라가는 성질, 즉 데이터의 일반적인 패턴을 학습시키는 방법이라고 볼 수 있습니다. 노이즈가 있는 데이터와 없는 데이터를 모두 학습하며 모델이 좀 더 견고해지는 효과를 볼 수 있습니다.&lt;br /&gt;
 하지만 데이터가 항상 충분할 수는 없으므로 의도적으로 데이터를 변형하여 더 많은 학습 데이터를 생성하기도 하는데, 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지를 돌리거나 자르고, 노이즈를 추가하거나 밝기를 낮추는 식으로 데이터 갯수를 부풀리는 식의 방법입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-데이터-정규화&quot;&gt;2. 데이터 정규화&lt;/h2&gt;
&lt;p&gt;데이터 정규화에는 $L1$ 정규화와 $L2$ 정규화가 있습니다. 이에 따라 필요한 용어부터 살펴보도록 하겠습니다.&lt;/p&gt;
&lt;h3 id=&quot;l1-norm--l2-norm&quot;&gt;$L1$ Norm &amp;amp; $L2$ Norm&lt;/h3&gt;
&lt;p&gt;$L1$ 정규화와 $L2$ 정규화를 설명하기에 앞서 $L1$, $L2$ Norm에 대해서 설명하겠습니다. 우선 Norm이란 것은 벡터의 거리를 측정하는 방법입니다. 이를 표현한 수식은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt; $\left \| x \right \|_p:=\left (\sum_{i=1}^{n}\left | x_i \right |^p\right )^{1/p}$ &lt;/center&gt;
&lt;p&gt;이때 p값은 Norm의 차수를 의미합니다. $L1$, $L2$에 있는 숫자가 바로 p입니다. 이 공식에 따르면 $L1$ Norm과 $L2$ Norm은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt; $p=(p_1, p_2, \cdots, p_n), q=(q_1, q_2, \cdots, q_n)$ 일 때, &lt;/center&gt;
&lt;center&gt; $L1\;Norm: \; \left \| x \right \|_1=\sum_{i=1}^{n}\left | p_i-q_i \right |$ &lt;/center&gt;
&lt;center&gt; $\begin{align*} L2\;Norm: \;\left \| x \right \|_2&amp;amp;=\left (\sum_{i=1}^{n}\left | p_i-q_i \right |^2\right )^{1/2} \\
&amp;amp;= \sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\cdots+(p_n-q_n)^2}
\end{align*}$ &lt;/center&gt;
&lt;p&gt;$L1$ Norm은 각 $p,q$원소들 간의 직선거리입니다. $L2$ Norm은 $p, q$ 벡터 사이의 직선거리입니다.&lt;/p&gt;

&lt;h3 id=&quot;l1-loss--l2-loss&quot;&gt;$L1$ Loss &amp;amp; $L2$ Loss&lt;/h3&gt;
&lt;p&gt;이러한 방식으로 $L1$ Loss와 $L2$ Loss 함수를 구현할 수 있습니다. $p$ 벡터를 실제 값으로, $q$ 벡터를 예측치로 치환하면 두 식은 다음과 같습니다.&lt;/p&gt;
&lt;center&gt; $L1\;Loss=\sum_{i=1}^{n}\left | y_i-f(x_i) \right |$&lt;/center&gt;
&lt;center&gt; $L2\;Loss=\sum_{i=1}^{n}\left ( y_i-f(x_i) \right )^2$&lt;/center&gt;
&lt;p&gt;$L1$ Loss와 $L2$ Loss의 차이는 잘못된 값에 대해서 $L2$ Loss의 경우 오차의 제곱을 더해 주기 때문에 $L2$ Loss가 Outlier에 더 민감하다는 점입니다. 따라서 Outlier가 적당히 무시되길 원하면 $L1$ Loss를 사용하고, Outlier의 등장에 신경써야 한다면 $L2$ Loss를 사용하는 것이 좋습니다.&lt;/p&gt;

&lt;h2 id=&quot;l1-regularization--l2-regularization&quot;&gt;$L1$ Regularization &amp;amp; $L2$ Regularization&lt;/h2&gt;
&lt;p&gt;위의 Loss 함수를 원래 모델에서 사용하던 Cost 함수에 추가하면 다음과 같은 결과를 얻을 수 있습니다.&lt;/p&gt;
&lt;center&gt; $L1\;Regularization:\;\; cost(W,b)=\frac{1}{n} \sum_{i=1}^{n} \left \{ L(y_i,\hat{y_i})+\frac{\lambda}{2}\left | w \right |\right \}$ &lt;/center&gt;
&lt;center&gt; $L2\;Regularization:\;\; cost(W,b)=\frac{1}{n} \sum_{i=1}^{n} \left \{ L(y_i,\hat{y_i})+\frac{\lambda}{2}\left | w \right |^2\right \}$ &lt;/center&gt;
&lt;center&gt; $L(y_i,\hat{y_i}):$ 기존의 Cost function&lt;/center&gt;
&lt;p&gt;기존의 오차함수에 가중치의 크기가 포함되면서 가중치가 너무 큰 방향으로 학습되지 않도록 하는 항을 추가해 주었습니다. 이때 $\lambda$는 학습률로 정규화의 효과를 조절하는 항으로 사용됩니다.&lt;br /&gt;
$L1$ Regularization과 $L2$ Regularization의 차이는 $L2$ Regularization는 미분이 가능하여 Gradient-based learning이 가능하다는 점입니다.&lt;/p&gt;

&lt;h2 id=&quot;3-dropout&quot;&gt;3. Dropout&lt;/h2&gt;
&lt;p&gt;Dropout은 의도적으로 은닉층의 일정 비율을 일부러 학습하지 않아 새로운 Epoch마다 조금씩 특징이 다른 데이터 셋을 학습시키는 효과를 내는 방법입니다. 여러 개의 모델을 만들지 않고도 모델 결합이 여러 형태를 가지게 하는 것입니다. 네트워크를 학습하는 동안 랜덤하게 일부 유닛이 동작하는 것을 생략한다면 뉴런의 조합만큼 지수함수적으로 다양한 모델을 학습시키는 것과 같습니다. n개의 유닛이 있다고 하면 $2^n$개 만큼의 모델이 생성될 것입니다.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/img/deeplearning/dropout.png&quot; /&gt;
&lt;/p&gt;

&lt;h1 id=&quot;마무리&quot;&gt;마무리&lt;/h1&gt;
&lt;p&gt;이번 게시글에서는 DNN과 과적합을 방지하는 법에 대해서 정리했습니다. 다음 게시글은 Convolution Nerual Network과 LeNet에 대해서 정리하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;질문&quot;&gt;질문&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;은닉층을 Wide하게 만드는 것보다 Deeper하게 만드는 것이 좋은 이유가 무엇인가요? 모델이 자연스럽게 계층 구조를 가지게 된다는 의미를 잘 모르겠습니다. 계층별로 비슷한 녀석들끼리 모인다는 뜻인가요?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;보충&quot;&gt;보충&lt;/h1&gt;
&lt;p&gt;1) 다층 신경망(Multi-layer Perceptron)이라고도 함&lt;/p&gt;

&lt;h1 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h1&gt;
&lt;p&gt;Udacity Self-driving car nanodegree - Deep Nerual Network(링크 공유 불가능)&lt;br /&gt;
&lt;a href=&quot;https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;amp;blogId=handuelly&amp;amp;logNo=221824080339&quot;&gt;답을 찾아가는 과정 - “딥러닝 - 활성화 함수(Activation) 종류 및 비교”&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/&quot;&gt;DataLatte’s IT Blog - “심층 신경망의 구조”&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Udacity" /><category term="Deep Learning" /><summary type="html">Deep Nerual Network 지금까지 정리한 Nerual Network는 모두 layer가 하나뿐인 단층 신경망(Single-Layer Perceptron)입니다. 하지만 단층 신경망으로는 비선형 모델을 구현할 수 없습니다. 지난 게시글에서 언급한 바와 같이 비선형 모델은 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 이루어진 $^{1)}$심층 신경망(Deep Neural Network)으로 구현할 수 있습니다. 따라서 인공 신경망의 성능 향상을 위해서는 심층 신경망의 사용은 필수적이라고 할 수 있습니다.</summary></entry><entry><title type="html">[Udacity] Deep Learning (4) - Tensorflow</title><link href="https://refstop.github.io/uda-dl-tensorflow.html" rel="alternate" type="text/html" title="[Udacity] Deep Learning (4) - Tensorflow" /><published>2021-03-01T21:14:35+09:00</published><updated>2021-03-01T21:14:35+09:00</updated><id>https://refstop.github.io/uda-dl-tensorflow</id><content type="html" xml:base="https://refstop.github.io/uda-dl-tensorflow.html">&lt;h1 id=&quot;왜-tensowflow-인가&quot;&gt;왜 Tensowflow 인가&lt;/h1&gt;
&lt;h2 id=&quot;tensorflow란&quot;&gt;Tensorflow란?&lt;/h2&gt;
&lt;p&gt;지난 시간까지 신경망을 쌓으면서 모델을 학습, 즉 적절한 가중치와 바이어스를 구하는 방법을 이론적으로 알아보았습니다. 이번 게시글에선 이 방법들을 물리적인 코드로 구현해 보겠습니다. 하지만 코드로 구현한다고 하여 덧셈 뺄셈 연산자를 이용하여 밑바닥부터 구현한다는 의미는 아닙니다. 이미 시중에는 딥러닝을 초보자도 쉽게(?) 구현할 수 있도록 제작해 놓은 여러 라이브러리들이 있습니다. 이 중 우리는 가장 많은 사람들이 사용하는 &lt;strong&gt;Tensorflow&lt;/strong&gt;를 사용할 것입니다.
&lt;img src=&quot;/assets/img/deeplearning/tensor flow.png&quot; alt=&quot;tensor flow&quot; width=&quot;90%&quot; height=&quot;90%&quot; /&gt;&lt;br /&gt;
Tensorflow는 구글에서 만든 딥러닝을 쉽게 구현할 수 있도록 기능을 제공하는 라이브러리입니다. 위 그림과 같이 딥러닝을 할 사용되는 텐서 형태의 데이터들이 모델을 구성하는 연산들의 그래프를 따라 흐르면서 연산이 일어납니다. 데이터를 의미하는 텐서(tensor)와 데이터 플로우 그래프를 따라 연산이 수행되는 형태인 flow를 합쳐 텐서플로(tensorflow)라고 부릅니다.&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-1-2&quot;&gt;Tensorflow 1? 2?&lt;/h2&gt;
&lt;p&gt;텐서플로는 현재 버전이 1과 2 두 가지가 있습니다. 두 버전의 차이는 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;placeholder, session 사용X&lt;/li&gt;
  &lt;li&gt;필요한 기능은 함수로 구현, @tf.function 사용&lt;/li&gt;
  &lt;li&gt;훨씬 간단하다!
아직 텐서플로를 많이 사용해 보지 못해 모든 차이점을 말할 수는 없지만 요점은 2가 1보다 훨씬 간단하다는 뜻입니다. 텐서플로 1은 코드를 죽 짠 후, 맨 마지막 실행 단계에서는 session이라는 class를 통해 실행을 시키게 됩니다. 하지만 이렇다 보니 실행 단계에서 session이라는 블랙박스에 가까운 공간 안에서 작업이 수행되다 보니 개발자가 개입하기가 힘들었습니다. 하지만 텐서플로 2에서는 session을 없애고 keras라는 강력한 라이브러리를 텐서플로 라이브러에서 편입시키면서 더욱 쓰기 편한 라이브러리가 되었습니다. 따라서 처음 시작하는 분들은 텐서플로 2로 시작하는 것을 추천하지만, 예전 소스를 참고하기 위해선 텐서플로 1을 읽을 수 있는 방법도 알아야 하기에, 어느정도 비율을 조정해서 병행하는 것이 좋다고 생각합니다.&lt;br /&gt;
제가 수강하는 Udacity 강의는 텐서플로 1을 사용하였기 때문에 이 게시글은 텐서플로 1의 문법으로 작성하겠습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;tensorflow-설치&quot;&gt;Tensorflow 설치&lt;/h1&gt;
&lt;p&gt;Tensorflow는 일반적으로 Anaconda라는 가상 환경에서 설치 후 실행합니다. 하지만 Anaconda는 파이썬 3 이상의 버전을 지원하기 때문에 파이썬 2 이하의 버전이 필요하신 분, 또는 다른 파이썬 라이브러리와 같이 사용해야 하는 분은 아나콘다 설치를 권장하지 않습니다. 저는 OpenCV도 함께 설치되어 있기 때문에 아나콘다를 통해 설치하지 않았습니다. 제가 시도해 본 방법은 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;pip install&lt;/li&gt;
  &lt;li&gt;공식 홈페이지 whl 설치&lt;/li&gt;
  &lt;li&gt;Google Colab&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-pip-install-tensorflow&quot;&gt;1. pip install tensorflow&lt;/h2&gt;
&lt;p&gt;파이썬 패키지 라이브러리를 관리해주는 pip 명령어를 통해서 설치하는 방법입니다. 우분투로 치면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; 명령어 정도의 포지션입니다. 일반적으로 파이썬을 설치했다면 설치되어 있겠지만, 혹시나 해서 설치 명령어를 남깁니다.&lt;br /&gt;
Python 2.X의 경우&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install python-pip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Python 3.X의 경우&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install python3-pip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pip가 설치되어 있다면 다음 명령어를 통해 설치합니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip3 install tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 명령어는 기본적으로 텐서플로 최신 버전을 설치합니다. 원하는 버전이 있다면 끝에 ==X.XX를 붙이거나 텐서플로 패키지 이름을 명시하여 설치합니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip3 install tensorflow==1.15
$ pip3 install tensorflow-gpu==1.15
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;참고로 텐서플로 2는 cpu 패키지와 gpu 패키지가 통합되어 있다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;2-공식-홈페이지-msi-설치&quot;&gt;2. 공식 홈페이지 msi 설치&lt;/h2&gt;
&lt;p&gt;위의 방법으로 설치는 할 수 있지만 개발 도구와 연동시키는 법을 몰라 저는 다음 방법으로 시도했습니다. 저는 Visual Studio Code로 코드를 작성하고 싶었기 때문에 whl 파일을 통해 설치했습니다. &lt;a href=&quot;https://www.tensorflow.org/install/pip?hl=ko&quot;&gt;공식 홈페이지 Tensorflow 설치 사이트&lt;/a&gt;에 가면 whl 파일을 받을 수 있는 링크가 있습니다. 저는 노트북에서 실행할 것이기 때문에 CPU만 지원하는 파일을 다운받았습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl
$ pip install tensorflow_cpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이 방법을 통하여 설치하면 VSC에서 텐서플로를 사용할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-google-colab&quot;&gt;3. Google Colab&lt;/h2&gt;
&lt;p&gt;마지막 방법은 Google Colab을 사용하는 것입니다. 이 방법은 텐서플로를 내 컴퓨터에 설치하는 방법이 아니라 오프라인에서는 사용할 수 없지만, 구글에서 이미 준비가 다 된 환경을 마련해 준다는 편리함이 있습니다. 게다가 서버 역시 구글에서 제공하기 때문에 컴퓨터 성능과 관계없이 코드를 실행할 수 있습니다. 성능이 좋지는 않지만 예제 정도를 실행하거나 시간이 많다 하시는 분들은 이 방법도 추천합니다. 하지만 코랩에 기본적으로 설치된 텐서플로는 최신 버전이기 때문에 텐서플로 1을 사용하고 싶다면 별도로 삭제, 설치할 필요가 있습니다. 코랩용 삭제, 설치 코드는 다음과 같습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ !pip3 uninstall tensorflow
$ !pip3 install tensorflow==1.15
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;페이지를 나갔다가 다시 접속하면 매번 설치해 줘야 한다는 번거로움이 있습니다. 하지만 어디서나 실행할 수 있다는 편리함 때문에 용서해 주겠습니다. 코드는 또 텐서플로 2에 맞춰서 짜면 해결되는 부분이기도 하고요.&lt;/p&gt;

&lt;h1 id=&quot;tensorflow-1-기본-함수들&quot;&gt;Tensorflow 1 기본 함수들&lt;/h1&gt;
&lt;p&gt;텐서플로에서 사용하는 함수들에 대해서 알아보겠습니다.&lt;/p&gt;
&lt;h2 id=&quot;tfsession&quot;&gt;tf.Session()&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/deeplearning/session.png&quot; alt=&quot;session&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
위에서 설명했듯이 작성한 코드를 실행하는 환경입니다. 먼저 코드를 짜고, Session에서 실행합니다. 하지만 텐서플로 2에서는 사용하지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;tfconstant&quot;&gt;tf.constant()&lt;/h2&gt;
&lt;p&gt;텐서 상수를 선언하는 함수입니다. 상수인 만큼 변하지 않는 텐서값입니다. 처음 정해준 값으로 끝까지 갑니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

hello_constant = tf.constant('Hello world!')

with tf.Session() as sess:
    hello = sess.run(hello_constant)
print(hello)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;출력:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;b'Hello world!'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;위의 코드와 같은 방식으로 tf.Session()을 사용합니다.&lt;/p&gt;

&lt;h2 id=&quot;tf사칙연산&quot;&gt;tf.사칙연산()&lt;/h2&gt;
&lt;p&gt;tf.add(), tf.multiply(), tf.subtract(), tf.divide() 함수가 있습니다. 덧셈, 곱셈, 뺄셈, 나눗셈 등 연산을 수행합니다. &lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

a = tf.constant(11)
b = tf.constant(5)
ad = tf.add(a, b)
sub = tf.subtract(a, b)
mul = tf.multiply(a, b)
div = tf.divide(a, b)

with tf.Session() as sess:
    r1 = sess.run(ad)
    r2 = sess.run(sub)
    r3 = sess.run(mul)
    r4 = sess.run(div)
print('add: {}\nsubtract: {}\nmultiply: {}\ndivide: {}'.format(r1, r2, r3, r4))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;출력:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add: 16
subtract: 6
multiply: 55
divide: 2.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;tfplaceholder&quot;&gt;tf.placeholder()&lt;/h2&gt;
&lt;p&gt;딥러닝에서 학습할 데이터를 담는 데이터 타입입니다. 학습용 데이터를 담는 그릇이라고 생각하면 됩니다. tf.Session()과 마찬가지로 텐서플로 2에서는 사용되지 않습니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

x = tf.placeholder(tf.int32, (None))
y = tf.placeholder(tf.int32, (None))
sum = tf.add(x, y)
with tf.Session() as sess:
  result = sess.run(sum, feed_dict={x: 11, y: 5})
print('sum: {}'.format(result))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;tfvariable-tfglobal_variables_initializer&quot;&gt;tf.Variable(), tf.global_variables_initializer()&lt;/h2&gt;
&lt;p&gt;상수와 초기화되지 않은 변수를 선언하는 방법을 보았으니 초기화와 동시에 변수를 선언하는 방법도 있습니다. 그것이 tf.Variable입니다. tf.global_variables_initializer()는 함수는 위의 코드에서 선언한 tf.Variable() 변수들을 세션에 적용해 주는 함수입니다. 예시는 다음과 같습니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

x = tf.Variable(5, name='x')

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  result = sess.run(x)
print('result: {}'.format(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;출력:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result: 5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;tftruncated_normal&quot;&gt;tf.truncated_normal()&lt;/h2&gt;
&lt;p&gt;정규분포에서 랜덤 숫자를 뽑아내는 함수입니다. 주로 가중치를 초기화하는데 사용합니다. 가중치를 초기화할 때 정규분포를 사용하는 이유는 &lt;strong&gt;기울기 소실&lt;/strong&gt; 때문입니다. 가중치를 랜덤으로 주면 Sigmoid 함수의 출력값이 0 또는 1에 아주 가까운, 즉 0 또는 1로 근사할 수 있는 출력값이 나오게 됩니다.&lt;br /&gt;
&lt;img src=&quot;/assets/img/deeplearning/logistic.png&quot; alt=&quot;logistic&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
이때, 출력값이 0 또는 1로 가게 된다면 Sigmoid 함수의 미분값이 0으로 치닫게 되므로, &lt;a href=&quot;https://refstop.github.io/posts/uda-dl-nncm/#cross-entropy%EC%9D%98-w-error-%EA%B7%B8%EB%9E%98%ED%94%84&quot;&gt;경사 하강법&lt;/a&gt;에서 활성화 함수인 Sigmoid 함수의 미분을 사용할 때 $\frac{\partial E}{\partial \sigma}\frac{\partial \sigma}{\partial z}\frac{\partial z}{\partial W}$ 중 $\frac{\partial \sigma}{\partial z}$의 값이 0이 되면서 가중치 수정값 역시 0이 됩니다.($z$는 선형 모델 결과값) 이렇게 오차함수의 기울기(Sigmoid 함수의 기울기)가 소실되는 현상을 &lt;strong&gt;기울기 소실(Gradient Vanishing Problem)&lt;/strong&gt;이라고 합니다.&lt;br /&gt;
따라서 정규분포로부터 랜덤 가중치를 뽑게 된다면, $-\infty ~ +\infty$ 사이의 숫자보다는 평균 0 근처의 숫자가 주로 추출될 것입니다. 이러한 특징을 통해 가중치가 한쪽으로 압도되는 것을 막을 수 있습니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

weights = tf.Variable(tf.truncated_normal(shape = (5, 5), mean = 0, stddev = 0.1))
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  result = sess.run(weights)
print('result:\n{}'.format(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;출력:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result:
[[-0.01648159 -0.02329956  0.17793715 -0.06097916 -0.05726282]
 [ 0.14564233  0.14883497  0.01122501  0.08220296  0.06075064]
 [-0.0392657  -0.06555585 -0.00456797  0.00886977 -0.06788757]
 [-0.10041036  0.12152421  0.09188548  0.05627985 -0.11565887]
 [-0.04590392  0.03194086  0.09958582 -0.07237397 -0.06919689]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;대부분 평균 0 근처의 난수가 저장된 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;tfzeros&quot;&gt;tf.zeros()&lt;/h2&gt;
&lt;p&gt;모든 텐서의 요소가 0인 텐서를 만드는 함수입니다. 바이어스를 초기화하는데 사용됩니다. 사용 방법은 weights 초기화 할때랑 비슷합니다.&lt;br /&gt;
예시)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{.python}&quot;&gt;import tensorflow as tf

bias = tf.Variable(tf.zeros(6))
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  result = sess.run(bias)
print('result:\n{}'.format(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;출력:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;result:
[0. 0. 0. 0. 0. 0.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;tensorflow-학습-함수들&quot;&gt;Tensorflow 학습 함수들&lt;/h1&gt;
&lt;h2 id=&quot;softmax&quot;&gt;Softmax&lt;/h2&gt;
&lt;p&gt;소프트맥스 함수는 logits라는 입력을 0~1 사이의 확률값으로 바꾸는 함수입니다. &lt;a href=&quot;https://refstop.github.io/posts/uda-dl-nncm/#softmax%EC%99%80-one-hot-encoding&quot;&gt;지난 게시글&lt;/a&gt;에 설명이 되어 있으니 자세한 설명은 생략하겠습니다. 사용 방법은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.nn.softmax(logits)&lt;/code&gt;입니다.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross-Entropy&lt;/h2&gt;
&lt;p&gt;크로스 엔트로피는 학습에서 사용하는 오차함수입니다. 이 부분 역시 &lt;a href=&quot;https://refstop.github.io/posts/uda-dl-nncm/#3-cross-entropy-%EC%98%A4%EC%B0%A8%ED%95%A8%EC%88%98&quot;&gt;이 게시글&lt;/a&gt;에 설명되어 있으니 생략하겠습니다. 텐서플로에서의 사용법은 다음과 같습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)
loss_operation = tf.reduce_mean(cross_entropy)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;optimization---최적화-단계&quot;&gt;Optimization - 최적화 단계&lt;/h1&gt;
&lt;p&gt;여기서 잠시 최적화 단계에 대해서 설명 드리겠습니다. 최적화는 모델 학습에서 가중치와 바이어스를 조절하는 단계를 의미합니다. 주로 경사 하강법을 사용합니다. 경사 하강법 역시 &lt;a href=&quot;https://refstop.github.io/posts/uda-dl-nncm/#4-gradient-descent&quot;&gt;지난번&lt;/a&gt;에 다루었기에 간단하게 설명하겠습니다. 오차함수의 미분을 통해 오차가 작아질 때까지 가중치 수정을 반복하는 알고리즘입니다. 오차함수의 기울기가 음수면 가중치를 증가, 양수면 가중치를 감소시킵니다. 하지만 경사하강법의 두 가지 문제점인 시간이 오래 걸린다와 지역 최솟값에 빠질 수 있다를 해결하기 위해 &lt;strong&gt;확률적 경사 하강법&lt;/strong&gt;과 &lt;strong&gt;모멘텀&lt;/strong&gt;이라는 방법이 고안되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-gradient-descentsgd-확률적-경사-하강법&quot;&gt;Stochastic Gradient Descent(SGD, 확률적 경사 하강법)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/deeplearning/sgd.png&quot; alt=&quot;sgd&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
확률적 경사 하강법은 경사 하강법과는 다르게 랜덤으로 한개의 데이터만을 보고 계산하는 방법입니다. SGD의 장점은 적은 데이터로 학습할 수 있고 속도가 빠르다는 점입니다. 하지만 학습 중간 과정에서 결과의 진폭이 크고 불안정하며, 데이터를 하나씩 처리하기 때문에 GPU의 성능을 모두 활용하지 못한다는 단점을 가집니다. 따라서 이러한 단점을 보완하기 위해 Mini-Batch라는 방식을 사용합니다.&lt;br /&gt;
&lt;img src=&quot;/assets/img/deeplearning/mb sgd.png&quot; alt=&quot;mb sgd&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
Mini Batch는 전체 학습데이터를 배치 사이즈로 나누어서 순차적으로 진행하는 방식입니다. Mini Batch SGD는 한개만 하던 그냥 SGD와는 다르게 데이터 일부분을 뽑아서 그 평균에 따라 가중치를 수정합니다. 이름에서도 알 수 있듯이 Mini Batch 방식과 SGD가 합쳐진 모습을 볼 수 있습니다. 병렬처리가 가능해지면서 GPU의 성능을 활용할 수도 있고 학습 중간 과정에서의 노이즈를 줄일 수 있습니다. 최근에는 거의 Mini Batch SGD를 사용하기 때문에 그냥 Mini Batch SGD를 SGD라고 부르기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;momentum모멘텀&quot;&gt;Momentum(모멘텀)&lt;/h2&gt;
&lt;p&gt;모멘텀은 지역 최솟값에 빠지지 않도록 고안된 방법입니다. 원래 경사 하강법에서는 오차함수를 미분한 값만큼만 가중치를 조정했지만, 모멘텀을 적용하면 이전 단계에서 오차함수의 미분값의 일부를 이번 단계에서도 적용하여 진짜 최솟값인지 아닌지를 한번 보는 원리로 표현할 수 있습니다. 수식으로 나타내면 다음과 같습니다.&lt;/p&gt;
&lt;center&gt;$\large{
v \leftarrow \alpha v-\eta \frac{\partial E}{\partial W}
}$&lt;/center&gt;
&lt;center&gt;$\large{
W \leftarrow W+v
}$&lt;/center&gt;
&lt;p&gt;여기서 $\alpha$로 이전 단계의 오차함수 미분값의 반영 비율을 조정합니다. 쉽게 설명하면 모멘텀, 즉 관성을 사용하여 원래 검사하려던 것보다 좀 더 멀리 뻗어보는 방법입니다.&lt;/p&gt;

&lt;h2 id=&quot;epoch&quot;&gt;Epoch&lt;/h2&gt;
&lt;p&gt;epoch는 전체 데이터 셋에 대해서 한번 학습을 완료한 상태를 의미합니다. 보통 Hyperparameter로 지정해 주게 되는데, Epoch은 배치를 사용하든 하지 않든 데이터의 전체 값을 모두 한번 본 상태여야 Epoch = 1인 상태라고 볼 수 있습니다. 결과적으로 전체 데이터 셋 학습 횟수로서 사용합니다. 예를 들어 EPOCH = 10이라고 지정해 줬다면 전체 데이터를 10번 학습하였다는 의미입니다.&lt;/p&gt;

&lt;h1 id=&quot;마무리&quot;&gt;마무리&lt;/h1&gt;
&lt;p&gt;Tensorflow에 대해서 알아보았지만 아직 많이 부족한 느낌입니다. 다음 게시글은 Deep Nerual Network를 정리할 예정입니다.&lt;/p&gt;

&lt;h1 id=&quot;질문&quot;&gt;질문&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;그럼 모멘텀으로 못빠져나올만큼 깊은 지역 최솟값일땐 어떻게 하나요?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;참고-사이트&quot;&gt;참고 사이트&lt;/h1&gt;
&lt;p&gt;Udacity Self-driving car nanodegree - Tensorflow(링크 공유 불가능)&lt;br /&gt;
&lt;a href=&quot;https://www.tensorflow.org/install/pip?hl=ko&quot;&gt;Tensorflow 공식 홈페이지 - TensorFlow 설치&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://kolikim.tistory.com/48&quot;&gt;Broccoli’s House - #5-(2) 학습 관련 기술 : 초기 가중치 설정&lt;/a&gt;&lt;/p&gt;</content><author><name>Refstop</name></author><category term="Udacity" /><category term="Deep Learning" /><category term="Tensorflow" /><summary type="html">왜 Tensowflow 인가 Tensorflow란? 지난 시간까지 신경망을 쌓으면서 모델을 학습, 즉 적절한 가중치와 바이어스를 구하는 방법을 이론적으로 알아보았습니다. 이번 게시글에선 이 방법들을 물리적인 코드로 구현해 보겠습니다. 하지만 코드로 구현한다고 하여 덧셈 뺄셈 연산자를 이용하여 밑바닥부터 구현한다는 의미는 아닙니다. 이미 시중에는 딥러닝을 초보자도 쉽게(?) 구현할 수 있도록 제작해 놓은 여러 라이브러리들이 있습니다. 이 중 우리는 가장 많은 사람들이 사용하는 Tensorflow를 사용할 것입니다. Tensorflow는 구글에서 만든 딥러닝을 쉽게 구현할 수 있도록 기능을 제공하는 라이브러리입니다. 위 그림과 같이 딥러닝을 할 사용되는 텐서 형태의 데이터들이 모델을 구성하는 연산들의 그래프를 따라 흐르면서 연산이 일어납니다. 데이터를 의미하는 텐서(tensor)와 데이터 플로우 그래프를 따라 연산이 수행되는 형태인 flow를 합쳐 텐서플로(tensorflow)라고 부릅니다.</summary></entry></feed>