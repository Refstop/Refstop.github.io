[
  
    {
      "title"       : "[Udacity] Deep Learning (7) - CNN 구성과 LeNet-5",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-lenet.html",
      "date"        : "2021-07-05 01:11:24 +0900",
      "description" : "",
      "content"     : "지난 게시글에 이어 CNN의 구성과 LeNet-5를 예시로 파라미터를 계산해 보겠습니다.input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,^^^왜인지 버그가 나서 아무말이나 지껴둔것1. CNN의 구성CNN의 구성을 공부하는 예시로서 대표적으로 사용되는 LeNet-5 모델으로 정리하겠습니다.LeNet-5 네트워크란? LeNet-5 네트워크는 CNN을 처음으로 개발한 Yann Lecun 연구팀이 개발한 CNN 알고리즘의 이름입니다. 이 알고리즘이 소개된 논문 제목은 Gradient-based learning applied to document recognition”입니다.LeNet-5 네트워크는 Input-C1-S2-C3-S4-C5-F6-Output으로 이루어져 있고, Convolution layer, Pooling layer 두 쌍(C1-S2-C3-S4)과 Flatten layer(C5), Fully Connected layer(F6) 1개로 구성되어 있습니다. 원래 논문에서는 활성화 함수로서 tanh 함수를 사용했지만, 제 코드에서는 ReLU 함수를 사용하였습니다.from keras.models import Sequentialfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.layers import Densefrom keras.layers import Flattenmodel = keras.Sequential()model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))model.add(layers.MaxPooling2D(pool_size=(2, 2)))model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))model.add(layers.MaxPooling2D(pool_size=(2, 2)))model.add(layers.Flatten())model.add(layers.Dense(units=120, activation='relu'))model.add(layers.Dense(units=84, activation='relu'))model.add(layers.Dense(units=10, activation = 'softmax'))실제 사용결과는 나중에 올리도록 하겠습니다….2. CNN 입출력, 파라미터 계산사용한 예시에서의 입출력 표를 정리하면 다음과 같습니다. layer Input Channel Filter Output Channel Stride Max Pooling Activation Function Convolution Layer 1 3 (3,3) 6 1 X relu Max Pooling Layer 1 6 X 6 1 (2,2) X Convolution Layer 2 6 (4,4) 16 1 X relu Max Pooling Layer 2 16 X 16 1 (2,2) X Flatten X X X X X X Fully Connected layer X X X X X softmax Convolution layer의 학습 파라미터 수는 입력채널수 $\\times$ 필터 폭 $\\times$ 필터 높이 $\\times$ 출력채널수로 계산됩니다.2.1 Layer 1의 Shape와 파라미터Layer 1은 1개의 Convolution layer와 1개의 Pooling layer로 구성되어 있습니다.2.1.1 Convolution layer 1Convolution layer 1의 기본 정보는 다음과 같습니다. 입력 데이터 Shape = (64,64,3) 입력 채널 = 3 필터 = (3,3) 출력 채널 = 6 Stride = 1입력 이미지에 Shape가 (3,3)인 필터를 6개 적용할 경우에, 출력 데이터(Activation Map)의 Shape를 계산하는 과정은 다음과 같습니다.$Row\\;Size=\\frac{N-F}{Stride}+1=\\frac{64-3}{1}+1=62$$Column\\;Size=\\frac{N-F}{Stride}+1=\\frac{64-3}{1}+1=62$위 식으로 계산된 Activation Map 사이즈는 (62,62,6)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다. 입력 채널: 3 출력 데이터(Activation Map) Shape: (62,62,6) 학습 파라미터: 162개 (3$\\times$3$\\times$3$\\times$6)2.1.2 Max Pooling layer 1Max Pooling layer 1의 입력 데이터 Shape은 (62,62,6)입니다. 채널 수는 바뀌지 않고, Max Pooling 사이즈가 (2,2)이기 때문에 출력 데이터 크기는 다음과 같습니다.$Row\\;Size=\\frac{62}{2}=31$$Column\\;Size=\\frac{62}{2}=31$위 식으로 계산된 출력 사이즈는 (31,31,6)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다. 입력 채널: 6 출력 데이터 Shape: (31,31,6) 학습 파라미터: 0개2.2 Layer 2의 Shape와 파라미터Layer 2도 마찬가지로 Convolution layer와 Pooling layer로 구성되어 있습니다.2.2.1 Convolution layer 2Convolution layer 2의 기본 정보는 다음과 같습니다. 입력 데이터 Shape = (31,31,6) 입력 채널 = 6 필터 = (4,4) 출력 채널 = 16 Stride = 1입력 이미지에 Shape가 (4,4)인 필터를 16개 적용할 경우에, 출력 데이터(Activation Map)의 Shape를 계산하는 과정은 다음과 같습니다.$Row\\;Size=\\frac{N-F}{Stride}+1=\\frac{31-4}{1}+1=28$$Column\\;Size=\\frac{N-F}{Stride}+1=\\frac{31-4}{1}+1=28$위 식으로 계산된 Activation Map 사이즈는 (28,28,16)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다. 입력 채널: 6 출력 데이터(Activation Map) Shape: (28,28,16) 학습 파라미터: 1536개 (6$\\times$4$\\times$4$\\times$16)2.2.2 Max Pooling layer 2Max Pooling layer 1의 입력 데이터 Shape은 (28,28,16)입니다. 채널 수는 바뀌지 않고, Max Pooling 사이즈가 (2,2)이기 때문에 출력 데이터 크기는 다음과 같습니다.$Row\\;Size=\\frac{28}{2}=14$$Column\\;Size=\\frac{28}{2}=14$위 식으로 계산된 출력 사이즈는 (14,14,16)입니다. 따라서 이 레이어의 학습 파라미터는 다음과 같이 계산할 수 있습니다. 입력 채널: 16 출력 데이터 Shape: (14,14,16) 학습 파라미터: 0개2.3 Flatten layerFlatten layer는 CNN의 데이터를 Fully Connected Nerual Network에 사용되는 1차원 벡터로 바꿔주는 layer입니다. 파라미터는 존재하지 않고 이전 layer의 출력을 1차원 벡터로 바꾸는 역할만 합니다. 입력 데이터 Shape: (14,14,16) 출력 데이터 Shape: (3136,1)원래 LeNet-5 네트워크에서는 Flatten 층의 입력 데이터 Shape와 같은 크기의 필터를 Convolution하고 출력 채널을 조절하여 1차원 벡터의 형태로 변환합니다. 하지만 Flatten 함수로도 충분히 이미지의 특성을 지닌 1차원 벡터로 변환할 수 있으므로 Flatten 함수를 사용합니다.2.4 Fully Connected layer(Softmax layer)마지막 출력층에 해당하는 FNN입니다. 입력 데이터의 class를 분류해야 하는 layer이기 때문에 Softmax 함수를 사용하여 출력층을 구성합니다. 입력 데이터의 Shape는 이전 Flatten layer의 출력인 (3136,1)입니다. 최종 출력 데이터는 원래 입력 데이터의 분류 class만큼이 되므로, 예를 들어 분류할 class가 10개라면 출력 데이터의 Shape는 (10,1)이 됩니다. 파라미터를 계산하면 다음과 같습니다. 입력 데이터 Shape: (3136,1) 출력 데이터 Shape: (10,1) Softmax layer의 파라미터 수: 31360개 (3136$\\times$10)3. 전체 파라미터 수와 레이어별 Input/Output 요약 layer Input Channel Filter Output Channel Stride Max Pooling Activation Function Input Shape Output Shape 파라미터 수 Conv1 3 (3,3) 6 1 X relu (64,64,3) (62,62,6) 162 MaxPooling1 6 X 6 1 (2,2) X (62,62,6) (31,31,6) 0 Conv2 6 (4,4) 16 1 X relu (31,31,6) (28,28,16) 1536 MaxPooling2 16 X 16 1 (2,2) X (28,28,16) (14,14,16) 0 Flatten X X X X X X (14,14,16) (3136,1) 0 FC X X X X X softmax (3136,1) (10,1) 31360 합계 X X X X X X X X 33058 4. Fully Connected Nerual Network와의 비교이 모델을 FNN의 파라미터 수와 비교한다면, 우선 입력층의 데이터가 (64,64,3)이므로 $64^2\\times3=12288$개의 원소를 가진 1차원 벡터가 됩니다. 또한 이 벡터를 Fully Connected layer에 입력하여 class 개수가 10개인 출력을 갖는다면 파라미터 수는 $12288\\times10=122880$개가 됩니다. layer를 하나만 가지고 있음에도 불구하고 CNN 구조의 네트워크보다 약 4배 더 많은 연산량을 가지게 됩니다. 따라서 이러한 점에서 CNN은 FNN에 비해 학습이 쉽고 네트워크 처리 속도가 빠르다는 장점을 갖고 있습니다.참고 사이트TAEWAN.KIM 블로그 - “CNN, Convolutional Neural Network 요약”"
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (6) - Convolutional Nerual Network",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-cnn.html",
      "date"        : "2021-07-04 15:41:24 +0900",
      "description" : "",
      "content"     : "지금까지 우리는 심층 신경망(Deep Nerual Network)을 구성하기까지의 과정을 살펴보았습니다. 이제 이 심층 신경망으로 구성할 수 있는 네트워크 중 이미지 학습에 최적화된 합성곱 신경망(Convolutional Nerual Network, CNN)에 대해서 알아보겠습니다.CNN이 되기까지CNN이 고안되기 전에는 이미지를 Fully Connected Nerual Network라는 1차원 벡터 형태의 데이터를 학습하는 형태의 신경망 구조를 사용했습니다. 하지만 FNN은 몇 가지 단점이 있습니다. 첫번째로 이미지 데이터를 평면화 시키는 과정에서 이미지의 인접한 픽셀 간의 정보가 유실된다는 점입니다. FNN은 1차원 벡터 형태의 데이터를 입력받기 때문에 이미지의 인접 픽셀간의 상관관계를 반영할 수 없습니다. 반면 CNN은 이미지의 특징을 보존한 채로 학습을 진행하기 때문에 인접 픽셀간의 정보 유식을 막을 수 있습니다. 두번째 FNN의 문제점은 막대한 양의 model parameter입니다. 만약 FNN을 사용하여 (1024,1024)크기의 컬러 이미지를 처리하고자 한다면 FNN에 입력되는 벡터의 차원은 1024$\\times$1024$\\times$3=315만 개입니다. 약 300만 차원의 데이터를 처리하기 위해서는 막대한 양의 뉴런이 필요하고 이에 따라 model parameter의 개수는 더욱 많은 양이 필요할 것입니다. 하지만 CNN의 경우 필터들을1. CNN의 주요 용어 정리CNN에는 다음과 같은 용어들이 사용됩니다. Convolution(합성곱) 채널(Channel) 필터(Filter) 커널(Kernel) 스트라이드(Stride) 패딩(Padding) 피처 맵(Feature Map) 액티베이션 맵(Activation Map) 풀링(Pooling) 레이어각 용어에 대해서 간략하게 정리하겠습니다.1.1 Convolution Layer(합성곱)Convolution은 합성곱이라는 의미인데, input 함수(신호)와 임펄스 응답 함수(신호)를 반전 이동한 값을 곱하여 적분한 값입니다. 원래의 의미는 이렇지만, 이해하기 너무 어려우므로 다음 gif 파일을 통해 쉽게 알 수 있습니다. 위의 gif 파일은 입력된 이미지 데이터를 필터를 통해 Feature Map을 만드는 과정입니다. 필터로 이미지를 훑어가면서 각각의 합성곱 결과를 저장하여 Feature Map을 구성합니다. CNN의 첫번째 과정인 Convolution Layer는 바로 이 Feature Map을 생성하는 과정입니다. Feature Map을 생성한 후 활성화 함수(Activation Function)에 통과시킨 출력을 액티베이션 맵(Activation Map)이라고 합니다.1.2 채널(Channel)채널은 이미지를 구성하고 있는 실수값의 차원입니다. 이미지를 색 공간에 따라서 분리할 때, RGB의 3채널로 분리할 수 있습니다. 이미지를 구성하는 각 픽셀은 실수로 표현한 3차원 데이터입니다. 각각의 차원은 R, G, B의 3색을 나타내는 실수로, 이때의 RGB를 채널이라고 합니다. Convolution Layer에 입력되는 데이터는 필터가 적용되고, 하나의 필터당 하나의 Feature Map이 생성됩니다. 따라서 Convolution Layer에 n개의 필터가 적용된다면 출력되는 Feature Map은 n개의 채널을 갖게 됩니다. 하지만 이번 게시글에서는 필터의 개수가 1개뿐인 모델만 다룰 것입니다.1.3 필터(Filter), 커널(Kernel) &amp; 스트라이드(Stride)필터는 이미지의 특징을 찾아내기 위한 공용 파라미터입니다. 일반적으로 (4,4), (3,3)같은 정방행렬로 표현되고 사용 방법은 1.1의 gif 이미지에 잘 표현되어 있습니다. 필터는 이미지를 순회하면서 합성곱을 계산하여 Feature Map을 생성합니다. 이때 필터가 한번에 이동하는 간격을 Stride라고 합니다. Stride는 직역하면 보폭이라는 뜻으로 말 그대로 “필터가 한 걸음에 얼마나 가냐”를 의미하는 값입니다. 위의 gif 이미지의 Stride는 1입니다. 아래 이미지는 Stride가 1이고, 이미지 픽셀이 (16,16), 필터 크기가 (2,2)일때의 Feature Map 생성 과정을 나타낸 사진입니다. 입력 데이터가 여러 채널을 갖는 경우, 각 채널의 Feature Map을 모두 더한 값이 최종 출력이 됩니다. 따라서 입력 데이터의 채널 수와는 상관없이 필터의 개수에 따라 결정된다는 것을 알 수 있습니다. 1.4 패딩(padding)패딩은 Convolution layer에서 필터와 Stride의 작용으로 출력 Feature Map의 크기가 줄어드는 현상을 방지하고 출력 데이터의 사이즈를 조정하는 방법입니다. 단어 그대로 입력데이터에 패드를 부착하는 것처럼 이미지 외곽에 지정된 픽셀만큼 특정 값으로 채워넣습니다. 보통 CNN에서는 패딩값을 0으로 채웁니다. 위의 이미지는 원래 입력 데이터 32$\\times$32$\\times$3 이미지에 2픽셀만큼의 패딩을 추가하여 36$\\times$36$\\times$3 이미지로 만든 것입니다. 출력 데이터의 사이즈를 조절하는 기능 이외에도 Convolution layer가 이미지의 외곽을 인식하는 학습 효과도 있습니다.1.5 Pooling LayerPooling layer는 Convolution layer의 출력인 Activation Map을 입력으로 받아서 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용됩니다. Pooling 처리 과정은 지정된 정방행렬 범위 내의 데이터를 Pooling 방식에 따라서 처리합니다. Pooling layer를 처리하는 방법으로는 Max pooling, Min pooling, Average pooling이 있습니다. 이름만 봐도 알 수 있듯이 각각 최댓값, 최솟값, 평균값만을 남기거나 계산하는 방식입니다. 다음 이미지를 참고하면 이해하기 쉽습니다. Pooling layer의 처리과정은 Convolution layer와 거의 비슷하지만 조금의 차이점이 있습니다. 학습대상 파라미터가 없음 행렬의 사이즈 감소 채널 수 변경 없음CNN에서는 주로 Max pooling을 사용합니다.2. 레이어별 출력 데이터 계산Convolution layer와 Pooling layer의 출력 데이터 크기를 계산하는 방법을 정리했습니다.2.1 Convolution layer 출력 데이터 크기 계산입력 데이터에 대한 필터의 크기와 Stride 크기에 따라서 Feature Map 크기가 결정됩니다. 입력 데이터 높이: H 입력 데이터 폭: W 필터 높이: FH 필터 폭: FW Stride 크기: S Padding 사이즈: P$Output\\;Height=OH=\\frac{(H+2P-FH)}{S}+1$$Output\\;Width=OW=\\frac{(W+2P-FW)}{S}+1$위 식의 결과는 자연수가 되어야 합니다. 또한 Convolution layer 다음에 Pooling layer가 온다면, Feature Map의 행과 열 크기는 Pooling 크기의 배수여야 합니다. 만약 Pooling 사이즈가 (3,3)이라면 위 식의 결과는 자연수이고 3의 배수여야 합니다. 이 조건을 만족하도록 Filter의 크기, Stride의 간격, Pooling 크기 및 패딩 크기를 조절해야 합니다.2.2 Pooling layer 출력 데이터 크기 계산Pooling layer의 Pooling 사이즈는 일반적으로 정방행렬입니다. 또한 Convolution layer의 출력이 Pooling 사이즈의 정수배가 되도록 하여 Pooling layer의 출력 사이즈를 결정하게 됩니다. 예를 들어 Convolution layer의 출력 Activation Map 사이즈가 (6,6)이고 Pooling 사이즈가 (3,3)이면, Pooling layer의 출력 사이즈는 (2,2)가 됩니다. 따라서 Pooling layer의 출력 사이즈는 다음과 같이 계산할 수 있습니다.$Output\\;Row\\;Size=\\frac{Input\\;Row\\;Size}{Pooling\\;Size}$$Output\\;Column\\;Size=\\frac{Input\\;Column\\;Size}{Pooling\\;Size}$마무리이번 게시글에서는 CNN에서 사용되는 용어 및 입출력 데이터의 크기를 계산하는 방법을 알아보았습니다. 다음 게시글은 이 게시글에 이어 LeNet-5 네트워크 구성과 직접적인 예시를 들어 파라미터를 계산해 보도록 하겠습니다.참고 사이트TAEWAN.KIM 블로그 - “CNN, Convolutional Neural Network 요약”Untitled 블로그 - “[머신 러닝/딥 러닝] 합성곱 신경망 (Convolutional Neural Network, CNN)과 학습 알고리즘”YJJo 블로그 - “Convolution Nerual Networks (합성곱 신경망)”"
    } ,
  
    {
      "title"       : "Indoor 2D Navigation - 개요 &amp; 좌표범위내 신호 출력 노드",
      "category"    : "",
      "tags"        : "실내 자율 주행, Kobuki",
      "url"         : "./i2n-signal.html",
      "date"        : "2021-03-02 12:41:24 +0900",
      "description" : "",
      "content"     : "프로젝트의 목적본 프로젝트의 목적은 Kobuki 로봇을 사용하여 대학의 로봇관 건물을 자율주행 할 수 있도록 여러 SLAM 기법이나 Navigation 패키지를 사용해 보고, ROS 패키지를 제작하여 ROS와 우분투 환경에 익숙해지는 것입니다. 차후 있을 3D 라이다를 사용한 실내 자율주행 로봇 과제 등을 수행하기 위해 기본적인 사용법을 익히는 것을 목표로 하고 있습니다.프로젝트 계획프로젝트의 진행 계획은 다음과 같습니다. 이 계획은 진행 상황에 따라 수정될 수 있습니다. Kobuki 패키지 작성 map에서 특정 좌표를 기준으로 일정 범위 내에 위치하면 신호 출력 노드 좌표를 파라미터화(yaml, dynamic_reconfigure) gmapping, cartographer mapping 실습 amcl, cartographer pure localization을 사용한 localization move_base를 사용한 자율주행패키지 작성 전 Prerequisite1. Kobuki 패키지 설치Kobuki 운용을 위해서는 Kobuki 패키지 설치를 해야합니다. 기존에 사용하던 catkin_ws를 사용해도 되지만 저는 새로운 작업공간 kobuki_ws를 생성했습니다.$ mkdir -p kobuki_ws/src그 후 src폴더에 Kobuki 패키지를 설치합니다.$ cd kobuki_ws/src$ git clone https://github.com/yujinrobot/kobuki.git이때 이 Kobuki 패키지에 필요한 ROS 패키지들을 설치하기 위해 다음의 명령어를 통해 패키지들을 설치합니다.$ sudo apt-get install ros-melodic-kobuki*$ sudo apt-get install ros-melodic-ecl*마지막으로 의존성을 설치하여 마무리합니다.$ cd ~/kobuki_ws$ rosdep install --from-paths src --ignore-src -r -y$ catkin_make2. Turtlebot3 패키지 설치 (선택)패키지를 작성할 때 Turtlebot3 패키지를 참고할 일이 많기 때문에 Turtlebot3 패키지를 설치해 두는 편이 좋습니다. 직접적인 사용은 하지 않을 것이기에, 설치하기 싫다면 필요할 때마다 Turtlebot3 github 링크를 참고해도 괜찮습니다.$ cd ~/kobuki_ws/src$ git clone -b melodic-devel --single-branch https://github.com/ROBOTIS-GIT/turtlebot3.git$ cd ~/kobuki_ws$ rosdep install --from-paths src --ignore-src -r -y$ catkin_makemap에서 특정 좌표 범위 내 위치시 신호 출력 노드첫번째로 저희가 시도한 것은 map의 특정 좌표를 주고, 로봇이 그 좌표로부터 일정 범위 내에 들어가면 신호를 보내는 것이었습니다. map 좌표계로부터 로봇의 base_link(base_footprint) 좌표계로 transform(좌표계 변환)하여 map 좌표계를 기준으로 로봇이 어느 좌표에 있는지를 알 수 있습니다. 사실 조금 진행된 프로젝트라서 이미 앞서 정리할 여러 기능들이 추가되어 있습니다. 동영상에 나오는 기능은 신호 출력 노드와 dynamic reconfigure(동적 파라미터 수정), 그 외에 Kobuki를 조종하거나 lidar 동작 기능을 포함하였습니다.좌표를 파라미터화 (yaml, dynamic reconfigure)yaml 파일 작성yaml 파일은 패키지의 설정값을 저장하는 파일입니다. 노드는 이 파일을 참고하여 소스에 값을 전달합니다. 예를 들어 point.yaml 파일의 내용물이 다음과 같다고 합시다.x: 13y: 20저희가 만든 패키지의 소스에 필요한 특정 좌표 부분을 yaml 파일로 작성한 것입니다. 노드를 실행하였을 때, 자동으로 yaml 파일을 참고하여 $x$, $y$ 값을 가져오면, 매번 소스를 건드리지 않고도 설정을 바꿔 줄 수 있습니다. 하지만 yaml 파일을 수정할 때 마다 catkin_make를 다시 해 주어야 하니 번거롭기는 합니다.dynamic reconfiguredynamic reconfigure는 동적 파라미터 수정으로, 노드에 설정되는 파라미터를 수정해 줌으로서 프로그램 실행 중에도 계속 파라미터를 바꿀 수 있도록 하는 도구입니다. rosparam 명령어 또는 yaml 파일로도 계속하여 수정해 줄 수 있지만 매번 명령어를 치거나 catkin_make를 해야 하는 번거로움이 있습니다. dynamic reconfigure는 GUI를 지원하는 rqt를 통해 수정할 수 있습니다. 이번 프로젝트 같은 경우 처음에는 특정 좌표를 소스 상에서 설정해 주었으나, dynamic reconfigure 기능을 통해 좌표를 계속해서 재설정할 수 있도록 기능을 추가하였습니다. 소스는 다음과 같습니다.#include &lt;ros/ros.h&gt;#include &lt;dynamic_reconfigure/server.h&gt;#include &lt;indoor_2d_nav/dyn_reconfig_testConfig.h&gt;void callback(indoor_2d_nav::dyn_reconfig_testConfig &amp;config, uint32_t level) { ROS_INFO(\"\\nstr_posexy: %s\", config.str_posexy);}int main(int argc, char **argv) { ros::init(argc, argv, \"dynamic_reconfigure_test\"); dynamic_reconfigure::Server&lt;indoor_2d_nav::dyn_reconfig_testConfig&gt; server; dynamic_reconfigure::Server&lt;indoor_2d_nav::dyn_reconfig_testConfig&gt;::CallbackType f; f = boost::bind(&amp;callback, _1, _2); server.setCallback(f); ros::spin(); return 0;}다음 게시글오랜만에 게시글을 수정하여 바뀐 점이 많습니다. 우선 다음 게시글에서는 Cartographer 사용법에 관하여 정리하도록 하겠습니다."
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (5) - Deep Nerual Network",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-dnn.html",
      "date"        : "2021-03-02 12:34:24 +0900",
      "description" : "",
      "content"     : "Deep Nerual Network지금까지 정리한 Nerual Network는 모두 layer가 하나뿐인 단층 신경망(Single-Layer Perceptron)입니다. 하지만 단층 신경망으로는 비선형 모델을 구현할 수 없습니다. 지난 게시글에서 언급한 바와 같이 비선형 모델은 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 이루어진 $^{1)}$심층 신경망(Deep Neural Network)으로 구현할 수 있습니다. 따라서 인공 신경망의 성능 향상을 위해서는 심층 신경망의 사용은 필수적이라고 할 수 있습니다.ReLU Activation Function심층 신경망에 대해서 정리하기 전에 ReLU 활성화 함수에 대해서 알아보고 넘어가겠습니다. 활성화 함수는 각 층의 신경망의 출력을 결정하는 함수입니다. 또한 활성화 함수가 없다면 심층 신경망은 $y=W_{1}W_{2}W_{3} \\cdots x=Wx$가 되므로 비선형 모델이라고 볼 수 없는 결과가 나옵니다. 이러한 문제를 해결하기 위해서 넣은 활성화 함수이지만, 지금까지 우리가 잘 사용했던 Sigmoid 활성화 함수에는 단점이 있습니다. 다음은 Sigmoid 함수의 미분 그래프를 나타낸 것입니다. Sigmoid 함수의 단점은 미분값의 범위가 0~0.25라는 점입니다. Cross-Entropy 오차 역전파 방법을 사용할 때, 활성화 함수의 미분값을 곱해주게 됩니다. 이때 활성화 함수의 미분값이 항상 1보다 작은 경우, 바로 Sigmoid 함수같은 경우 심층 신경망의 layer가 많을수록 오차에 대한 가중치의 미분, 즉 가중치가 오차에 영향을 미치는 성분이 점점 작아지고, layer가 너무 많은 경우 오차의 미분값이 0에 수렴하게 됩니다. 이 현상이 바로 기울기 소실(Gradient Vanishing)입니다.이러한 점을 해결하기 위해서 사용하는 활성화 함수가 바로 ReLU(Rectified Linear Unit) 함수입니다. ReLU함수의 그래프는 다음과 같은 형태입니다. 기울기 소실의 근본적인 문제점은 Sigmoid 함수의 미분값 범위 0~0.25 때문에 발생했습니다. 하지만 ReLU 함수는 활성화되었을 때 값이 1, 비활성화 되었을 때 값이 0이므로 기울기 소실 문제를 해결할 수 있습니다. 역전파 연산의 결과값, 즉 가중치에 대한 오차의 미분값이 작아지지 않기 때문에 layer 개수에 관계없이 역전파 연산의 결과를 얻을 수 있습니다. 실제로 가장 많이 사용하는 활성화 함수 역시 ReLU 함수입니다.다시 심층 신경망의 본론으로….방금 알아본 바와 같이 ReLU 활성화 함수를 사용하여 심층 신경망을 구성합니다. 구성된 신경망의 형태는 이전 게시글의 비선형 모델 - 다층 신경망의 그림과 별 다를 바가 없지만, 아무래도 괴발개발 그린 그림(…)보다는 구글에서 퍼온 그림이 눈이 편할 것 같아 준비했습니다. 먼저 각각의 뉴런의 구성을 그린 이미지를 준비했습니다. 위의 이미지가 바로 layer가 하나뿐인 신경망, 단층 신경망입니다. 이전 게시글에 정리했던 내용이지만, 저도 오랜만에 작성하다 보니 복습할 이미지가 필요했습니다. 이러한 뉴런을 이어붙여 만든 신경망은 다음과 같습니다. 은닉층(Hidden layer)이 하나뿐인 얕은 신경망(Shallow layer)입니다. 왠지 모르게 “얕은 신경망”이라는 이름이 붙어 있지만 그냥 은닉층 1단짜리 심층 신경망입니다. 왜 이런 이름이 있지? 제 생각이지만 은닉층 1층짜리밖에 구현 못하던 시절엔 이걸 심층 신경망이라 불렀는데 더 깊게 구성할 수 있는 기술이 개발된 후 진짜 심층 신경망과 구분하기 위해 지은 이름이 아닐까요? 제 생각일 뿐이니 한귀로 듣고 한귀로 흘리시길 바랍니다….다음 그림은 위의 얕은 신경망에서 발전된 형태인 진짜배기 심층 신경망입니다. 이런 식으로 다수의 뉴런으로 심층 신경망을 구현하여 가중치를 구할 수 있습니다.과적합 방지법과적합은 훈련 데이터에 모델이 과도하게 회귀되어 오히려 실제 사용에서 성능이 떨어지는 현상입니다. 훈련 데이터에 대한 노이즈까지 학습을 해버려서 일어나는 현상인데, 훈련 데이터에서는 높은 정확도를 보이지만 검증 데이터나 테스트 데이터에서는 제대로 동작하지 않습니다. 이러한 현상을 방지할 수 있는 방법에 대해서 정리하였습니다.1. 데이터 양을 늘리기과적합 뿐만 아니라 모델 자체의 성능을 높이는 데도 좋은 방법입니다. 심층 신경망 구조는 학습시킬 데이터가 많을수록 정확도가 올라가는 성질, 즉 데이터의 일반적인 패턴을 학습시키는 방법이라고 볼 수 있습니다. 노이즈가 있는 데이터와 없는 데이터를 모두 학습하며 모델이 좀 더 견고해지는 효과를 볼 수 있습니다. 하지만 데이터가 항상 충분할 수는 없으므로 의도적으로 데이터를 변형하여 더 많은 학습 데이터를 생성하기도 하는데, 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지를 돌리거나 자르고, 노이즈를 추가하거나 밝기를 낮추는 식으로 데이터 갯수를 부풀리는 식의 방법입니다.2. 데이터 정규화데이터 정규화에는 $L1$ 정규화와 $L2$ 정규화가 있습니다. 이에 따라 필요한 용어부터 살펴보도록 하겠습니다.$L1$ Norm &amp; $L2$ Norm$L1$ 정규화와 $L2$ 정규화를 설명하기에 앞서 $L1$, $L2$ Norm에 대해서 설명하겠습니다. 우선 Norm이란 것은 벡터의 거리를 측정하는 방법입니다. 이를 표현한 수식은 다음과 같습니다. $\\left \\| x \\right \\|_p:=\\left (\\sum_{i=1}^{n}\\left | x_i \\right |^p\\right )^{1/p}$ 이때 p값은 Norm의 차수를 의미합니다. $L1$, $L2$에 있는 숫자가 바로 p입니다. 이 공식에 따르면 $L1$ Norm과 $L2$ Norm은 다음과 같습니다. $p=(p_1, p_2, \\cdots, p_n), q=(q_1, q_2, \\cdots, q_n)$ 일 때, $L1\\;Norm: \\; \\left \\| x \\right \\|_1=\\sum_{i=1}^{n}\\left | p_i-q_i \\right |$ $\\begin{align*} L2\\;Norm: \\;\\left \\| x \\right \\|_2&amp;=\\left (\\sum_{i=1}^{n}\\left | p_i-q_i \\right |^2\\right )^{1/2} \\\\&amp;= \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\\cdots+(p_n-q_n)^2}\\end{align*}$ $L1$ Norm은 각 $p,q$원소들 간의 직선거리입니다. $L2$ Norm은 $p, q$ 벡터 사이의 직선거리입니다.$L1$ Loss &amp; $L2$ Loss이러한 방식으로 $L1$ Loss와 $L2$ Loss 함수를 구현할 수 있습니다. $p$ 벡터를 실제 값으로, $q$ 벡터를 예측치로 치환하면 두 식은 다음과 같습니다. $L1\\;Loss=\\sum_{i=1}^{n}\\left | y_i-f(x_i) \\right |$ $L2\\;Loss=\\sum_{i=1}^{n}\\left ( y_i-f(x_i) \\right )^2$$L1$ Loss와 $L2$ Loss의 차이는 잘못된 값에 대해서 $L2$ Loss의 경우 오차의 제곱을 더해 주기 때문에 $L2$ Loss가 Outlier에 더 민감하다는 점입니다. 따라서 Outlier가 적당히 무시되길 원하면 $L1$ Loss를 사용하고, Outlier의 등장에 신경써야 한다면 $L2$ Loss를 사용하는 것이 좋습니다.$L1$ Regularization &amp; $L2$ Regularization위의 Loss 함수를 원래 모델에서 사용하던 Cost 함수에 추가하면 다음과 같은 결과를 얻을 수 있습니다. $L1\\;Regularization:\\;\\; cost(W,b)=\\frac{1}{n} \\sum_{i=1}^{n} \\left \\{ L(y_i,\\hat{y_i})+\\frac{\\lambda}{2}\\left | w \\right |\\right \\}$ $L2\\;Regularization:\\;\\; cost(W,b)=\\frac{1}{n} \\sum_{i=1}^{n} \\left \\{ L(y_i,\\hat{y_i})+\\frac{\\lambda}{2}\\left | w \\right |^2\\right \\}$ $L(y_i,\\hat{y_i}):$ 기존의 Cost function기존의 오차함수에 가중치의 크기가 포함되면서 가중치가 너무 큰 방향으로 학습되지 않도록 하는 항을 추가해 주었습니다. 이때 $\\lambda$는 학습률로 정규화의 효과를 조절하는 항으로 사용됩니다.$L1$ Regularization과 $L2$ Regularization의 차이는 $L2$ Regularization는 미분이 가능하여 Gradient-based learning이 가능하다는 점입니다.3. DropoutDropout은 의도적으로 은닉층의 일정 비율을 일부러 학습하지 않아 새로운 Epoch마다 조금씩 특징이 다른 데이터 셋을 학습시키는 효과를 내는 방법입니다. 여러 개의 모델을 만들지 않고도 모델 결합이 여러 형태를 가지게 하는 것입니다. 네트워크를 학습하는 동안 랜덤하게 일부 유닛이 동작하는 것을 생략한다면 뉴런의 조합만큼 지수함수적으로 다양한 모델을 학습시키는 것과 같습니다. n개의 유닛이 있다고 하면 $2^n$개 만큼의 모델이 생성될 것입니다. 마무리이번 게시글에서는 DNN과 과적합을 방지하는 법에 대해서 정리했습니다. 다음 게시글은 Convolution Nerual Network과 LeNet에 대해서 정리하겠습니다.질문 은닉층을 Wide하게 만드는 것보다 Deeper하게 만드는 것이 좋은 이유가 무엇인가요? 모델이 자연스럽게 계층 구조를 가지게 된다는 의미를 잘 모르겠습니다. 계층별로 비슷한 녀석들끼리 모인다는 뜻인가요?보충1) 다층 신경망(Multi-layer Perceptron)이라고도 함참고 사이트Udacity Self-driving car nanodegree - Deep Nerual Network(링크 공유 불가능)답을 찾아가는 과정 - “딥러닝 - 활성화 함수(Activation) 종류 및 비교”DataLatte’s IT Blog - “심층 신경망의 구조”"
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (4) - Tensorflow",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning, Tensorflow",
      "url"         : "./uda-dl-tensorflow.html",
      "date"        : "2021-03-01 21:14:35 +0900",
      "description" : "",
      "content"     : "왜 Tensowflow 인가Tensorflow란?지난 시간까지 신경망을 쌓으면서 모델을 학습, 즉 적절한 가중치와 바이어스를 구하는 방법을 이론적으로 알아보았습니다. 이번 게시글에선 이 방법들을 물리적인 코드로 구현해 보겠습니다. 하지만 코드로 구현한다고 하여 덧셈 뺄셈 연산자를 이용하여 밑바닥부터 구현한다는 의미는 아닙니다. 이미 시중에는 딥러닝을 초보자도 쉽게(?) 구현할 수 있도록 제작해 놓은 여러 라이브러리들이 있습니다. 이 중 우리는 가장 많은 사람들이 사용하는 Tensorflow를 사용할 것입니다.Tensorflow는 구글에서 만든 딥러닝을 쉽게 구현할 수 있도록 기능을 제공하는 라이브러리입니다. 위 그림과 같이 딥러닝을 할 사용되는 텐서 형태의 데이터들이 모델을 구성하는 연산들의 그래프를 따라 흐르면서 연산이 일어납니다. 데이터를 의미하는 텐서(tensor)와 데이터 플로우 그래프를 따라 연산이 수행되는 형태인 flow를 합쳐 텐서플로(tensorflow)라고 부릅니다.Tensorflow 1? 2?텐서플로는 현재 버전이 1과 2 두 가지가 있습니다. 두 버전의 차이는 다음과 같습니다. placeholder, session 사용X 필요한 기능은 함수로 구현, @tf.function 사용 훨씬 간단하다!아직 텐서플로를 많이 사용해 보지 못해 모든 차이점을 말할 수는 없지만 요점은 2가 1보다 훨씬 간단하다는 뜻입니다. 텐서플로 1은 코드를 죽 짠 후, 맨 마지막 실행 단계에서는 session이라는 class를 통해 실행을 시키게 됩니다. 하지만 이렇다 보니 실행 단계에서 session이라는 블랙박스에 가까운 공간 안에서 작업이 수행되다 보니 개발자가 개입하기가 힘들었습니다. 하지만 텐서플로 2에서는 session을 없애고 keras라는 강력한 라이브러리를 텐서플로 라이브러에서 편입시키면서 더욱 쓰기 편한 라이브러리가 되었습니다. 따라서 처음 시작하는 분들은 텐서플로 2로 시작하는 것을 추천하지만, 예전 소스를 참고하기 위해선 텐서플로 1을 읽을 수 있는 방법도 알아야 하기에, 어느정도 비율을 조정해서 병행하는 것이 좋다고 생각합니다.제가 수강하는 Udacity 강의는 텐서플로 1을 사용하였기 때문에 이 게시글은 텐서플로 1의 문법으로 작성하겠습니다.Tensorflow 설치Tensorflow는 일반적으로 Anaconda라는 가상 환경에서 설치 후 실행합니다. 하지만 Anaconda는 파이썬 3 이상의 버전을 지원하기 때문에 파이썬 2 이하의 버전이 필요하신 분, 또는 다른 파이썬 라이브러리와 같이 사용해야 하는 분은 아나콘다 설치를 권장하지 않습니다. 저는 OpenCV도 함께 설치되어 있기 때문에 아나콘다를 통해 설치하지 않았습니다. 제가 시도해 본 방법은 다음과 같습니다. pip install 공식 홈페이지 whl 설치 Google Colab1. pip install tensorflow파이썬 패키지 라이브러리를 관리해주는 pip 명령어를 통해서 설치하는 방법입니다. 우분투로 치면 apt-get 명령어 정도의 포지션입니다. 일반적으로 파이썬을 설치했다면 설치되어 있겠지만, 혹시나 해서 설치 명령어를 남깁니다.Python 2.X의 경우$ sudo apt-get install python-pipPython 3.X의 경우$ sudo apt-get install python3-pippip가 설치되어 있다면 다음 명령어를 통해 설치합니다.$ pip3 install tensorflow이 명령어는 기본적으로 텐서플로 최신 버전을 설치합니다. 원하는 버전이 있다면 끝에 ==X.XX를 붙이거나 텐서플로 패키지 이름을 명시하여 설치합니다.예시)$ pip3 install tensorflow==1.15$ pip3 install tensorflow-gpu==1.15참고로 텐서플로 2는 cpu 패키지와 gpu 패키지가 통합되어 있다고 합니다.2. 공식 홈페이지 msi 설치위의 방법으로 설치는 할 수 있지만 개발 도구와 연동시키는 법을 몰라 저는 다음 방법으로 시도했습니다. 저는 Visual Studio Code로 코드를 작성하고 싶었기 때문에 whl 파일을 통해 설치했습니다. 공식 홈페이지 Tensorflow 설치 사이트에 가면 whl 파일을 받을 수 있는 링크가 있습니다. 저는 노트북에서 실행할 것이기 때문에 CPU만 지원하는 파일을 다운받았습니다.$ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl$ pip install tensorflow_cpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl이 방법을 통하여 설치하면 VSC에서 텐서플로를 사용할 수 있습니다.3. Google Colab마지막 방법은 Google Colab을 사용하는 것입니다. 이 방법은 텐서플로를 내 컴퓨터에 설치하는 방법이 아니라 오프라인에서는 사용할 수 없지만, 구글에서 이미 준비가 다 된 환경을 마련해 준다는 편리함이 있습니다. 게다가 서버 역시 구글에서 제공하기 때문에 컴퓨터 성능과 관계없이 코드를 실행할 수 있습니다. 성능이 좋지는 않지만 예제 정도를 실행하거나 시간이 많다 하시는 분들은 이 방법도 추천합니다. 하지만 코랩에 기본적으로 설치된 텐서플로는 최신 버전이기 때문에 텐서플로 1을 사용하고 싶다면 별도로 삭제, 설치할 필요가 있습니다. 코랩용 삭제, 설치 코드는 다음과 같습니다.$ !pip3 uninstall tensorflow$ !pip3 install tensorflow==1.15페이지를 나갔다가 다시 접속하면 매번 설치해 줘야 한다는 번거로움이 있습니다. 하지만 어디서나 실행할 수 있다는 편리함 때문에 용서해 주겠습니다. 코드는 또 텐서플로 2에 맞춰서 짜면 해결되는 부분이기도 하고요.Tensorflow 1 기본 함수들텐서플로에서 사용하는 함수들에 대해서 알아보겠습니다.tf.Session()위에서 설명했듯이 작성한 코드를 실행하는 환경입니다. 먼저 코드를 짜고, Session에서 실행합니다. 하지만 텐서플로 2에서는 사용하지 않습니다.tf.constant()텐서 상수를 선언하는 함수입니다. 상수인 만큼 변하지 않는 텐서값입니다. 처음 정해준 값으로 끝까지 갑니다.예시)import tensorflow as tfhello_constant = tf.constant('Hello world!')with tf.Session() as sess: hello = sess.run(hello_constant)print(hello)출력:b'Hello world!'위의 코드와 같은 방식으로 tf.Session()을 사용합니다.tf.사칙연산()tf.add(), tf.multiply(), tf.subtract(), tf.divide() 함수가 있습니다. 덧셈, 곱셈, 뺄셈, 나눗셈 등 연산을 수행합니다. 예시)import tensorflow as tfa = tf.constant(11)b = tf.constant(5)ad = tf.add(a, b)sub = tf.subtract(a, b)mul = tf.multiply(a, b)div = tf.divide(a, b)with tf.Session() as sess: r1 = sess.run(ad) r2 = sess.run(sub) r3 = sess.run(mul) r4 = sess.run(div)print('add: {}\\nsubtract: {}\\nmultiply: {}\\ndivide: {}'.format(r1, r2, r3, r4))출력:add: 16subtract: 6multiply: 55divide: 2.2tf.placeholder()딥러닝에서 학습할 데이터를 담는 데이터 타입입니다. 학습용 데이터를 담는 그릇이라고 생각하면 됩니다. tf.Session()과 마찬가지로 텐서플로 2에서는 사용되지 않습니다.예시)import tensorflow as tfx = tf.placeholder(tf.int32, (None))y = tf.placeholder(tf.int32, (None))sum = tf.add(x, y)with tf.Session() as sess: result = sess.run(sum, feed_dict={x: 11, y: 5})print('sum: {}'.format(result))tf.Variable(), tf.global_variables_initializer()상수와 초기화되지 않은 변수를 선언하는 방법을 보았으니 초기화와 동시에 변수를 선언하는 방법도 있습니다. 그것이 tf.Variable입니다. tf.global_variables_initializer()는 함수는 위의 코드에서 선언한 tf.Variable() 변수들을 세션에 적용해 주는 함수입니다. 예시는 다음과 같습니다.예시)import tensorflow as tfx = tf.Variable(5, name='x')with tf.Session() as sess: sess.run(tf.global_variables_initializer()) result = sess.run(x)print('result: {}'.format(result))출력:result: 5tf.truncated_normal()정규분포에서 랜덤 숫자를 뽑아내는 함수입니다. 주로 가중치를 초기화하는데 사용합니다. 가중치를 초기화할 때 정규분포를 사용하는 이유는 기울기 소실 때문입니다. 가중치를 랜덤으로 주면 Sigmoid 함수의 출력값이 0 또는 1에 아주 가까운, 즉 0 또는 1로 근사할 수 있는 출력값이 나오게 됩니다.이때, 출력값이 0 또는 1로 가게 된다면 Sigmoid 함수의 미분값이 0으로 치닫게 되므로, 경사 하강법에서 활성화 함수인 Sigmoid 함수의 미분을 사용할 때 $\\frac{\\partial E}{\\partial \\sigma}\\frac{\\partial \\sigma}{\\partial z}\\frac{\\partial z}{\\partial W}$ 중 $\\frac{\\partial \\sigma}{\\partial z}$의 값이 0이 되면서 가중치 수정값 역시 0이 됩니다.($z$는 선형 모델 결과값) 이렇게 오차함수의 기울기(Sigmoid 함수의 기울기)가 소실되는 현상을 기울기 소실(Gradient Vanishing Problem)이라고 합니다.따라서 정규분포로부터 랜덤 가중치를 뽑게 된다면, $-\\infty ~ +\\infty$ 사이의 숫자보다는 평균 0 근처의 숫자가 주로 추출될 것입니다. 이러한 특징을 통해 가중치가 한쪽으로 압도되는 것을 막을 수 있습니다.예시)import tensorflow as tfweights = tf.Variable(tf.truncated_normal(shape = (5, 5), mean = 0, stddev = 0.1))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) result = sess.run(weights)print('result:\\n{}'.format(result))출력:result:[[-0.01648159 -0.02329956 0.17793715 -0.06097916 -0.05726282] [ 0.14564233 0.14883497 0.01122501 0.08220296 0.06075064] [-0.0392657 -0.06555585 -0.00456797 0.00886977 -0.06788757] [-0.10041036 0.12152421 0.09188548 0.05627985 -0.11565887] [-0.04590392 0.03194086 0.09958582 -0.07237397 -0.06919689]]대부분 평균 0 근처의 난수가 저장된 것을 볼 수 있습니다.tf.zeros()모든 텐서의 요소가 0인 텐서를 만드는 함수입니다. 바이어스를 초기화하는데 사용됩니다. 사용 방법은 weights 초기화 할때랑 비슷합니다.예시)import tensorflow as tfbias = tf.Variable(tf.zeros(6))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) result = sess.run(bias)print('result:\\n{}'.format(result))출력:result:[0. 0. 0. 0. 0. 0.]Tensorflow 학습 함수들Softmax소프트맥스 함수는 logits라는 입력을 0~1 사이의 확률값으로 바꾸는 함수입니다. 지난 게시글에 설명이 되어 있으니 자세한 설명은 생략하겠습니다. 사용 방법은 tf.nn.softmax(logits)입니다.Cross-Entropy크로스 엔트로피는 학습에서 사용하는 오차함수입니다. 이 부분 역시 이 게시글에 설명되어 있으니 생략하겠습니다. 텐서플로에서의 사용법은 다음과 같습니다.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)loss_operation = tf.reduce_mean(cross_entropy)Optimization - 최적화 단계여기서 잠시 최적화 단계에 대해서 설명 드리겠습니다. 최적화는 모델 학습에서 가중치와 바이어스를 조절하는 단계를 의미합니다. 주로 경사 하강법을 사용합니다. 경사 하강법 역시 지난번에 다루었기에 간단하게 설명하겠습니다. 오차함수의 미분을 통해 오차가 작아질 때까지 가중치 수정을 반복하는 알고리즘입니다. 오차함수의 기울기가 음수면 가중치를 증가, 양수면 가중치를 감소시킵니다. 하지만 경사하강법의 두 가지 문제점인 시간이 오래 걸린다와 지역 최솟값에 빠질 수 있다를 해결하기 위해 확률적 경사 하강법과 모멘텀이라는 방법이 고안되었습니다.Stochastic Gradient Descent(SGD, 확률적 경사 하강법)확률적 경사 하강법은 경사 하강법과는 다르게 랜덤으로 한개의 데이터만을 보고 계산하는 방법입니다. SGD의 장점은 적은 데이터로 학습할 수 있고 속도가 빠르다는 점입니다. 하지만 학습 중간 과정에서 결과의 진폭이 크고 불안정하며, 데이터를 하나씩 처리하기 때문에 GPU의 성능을 모두 활용하지 못한다는 단점을 가집니다. 따라서 이러한 단점을 보완하기 위해 Mini-Batch라는 방식을 사용합니다.Mini Batch는 전체 학습데이터를 배치 사이즈로 나누어서 순차적으로 진행하는 방식입니다. Mini Batch SGD는 한개만 하던 그냥 SGD와는 다르게 데이터 일부분을 뽑아서 그 평균에 따라 가중치를 수정합니다. 이름에서도 알 수 있듯이 Mini Batch 방식과 SGD가 합쳐진 모습을 볼 수 있습니다. 병렬처리가 가능해지면서 GPU의 성능을 활용할 수도 있고 학습 중간 과정에서의 노이즈를 줄일 수 있습니다. 최근에는 거의 Mini Batch SGD를 사용하기 때문에 그냥 Mini Batch SGD를 SGD라고 부르기도 합니다.Momentum(모멘텀)모멘텀은 지역 최솟값에 빠지지 않도록 고안된 방법입니다. 원래 경사 하강법에서는 오차함수를 미분한 값만큼만 가중치를 조정했지만, 모멘텀을 적용하면 이전 단계에서 오차함수의 미분값의 일부를 이번 단계에서도 적용하여 진짜 최솟값인지 아닌지를 한번 보는 원리로 표현할 수 있습니다. 수식으로 나타내면 다음과 같습니다.$\\large{v \\leftarrow \\alpha v-\\eta \\frac{\\partial E}{\\partial W}}$$\\large{W \\leftarrow W+v}$여기서 $\\alpha$로 이전 단계의 오차함수 미분값의 반영 비율을 조정합니다. 쉽게 설명하면 모멘텀, 즉 관성을 사용하여 원래 검사하려던 것보다 좀 더 멀리 뻗어보는 방법입니다.Epochepoch는 전체 데이터 셋에 대해서 한번 학습을 완료한 상태를 의미합니다. 보통 Hyperparameter로 지정해 주게 되는데, Epoch은 배치를 사용하든 하지 않든 데이터의 전체 값을 모두 한번 본 상태여야 Epoch = 1인 상태라고 볼 수 있습니다. 결과적으로 전체 데이터 셋 학습 횟수로서 사용합니다. 예를 들어 EPOCH = 10이라고 지정해 줬다면 전체 데이터를 10번 학습하였다는 의미입니다.마무리Tensorflow에 대해서 알아보았지만 아직 많이 부족한 느낌입니다. 다음 게시글은 Deep Nerual Network를 정리할 예정입니다.질문 그럼 모멘텀으로 못빠져나올만큼 깊은 지역 최솟값일땐 어떻게 하나요?참고 사이트Udacity Self-driving car nanodegree - Tensorflow(링크 공유 불가능)Tensorflow 공식 홈페이지 - TensorFlow 설치Broccoli’s House - #5-(2) 학습 관련 기술 : 초기 가중치 설정"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (14) - 파라미터 사용(Python)",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-custom_param.html",
      "date"        : "2021-02-13 20:18:23 +0900",
      "description" : "",
      "content"     : "시작하며이 게시글은 Python 노드에서 파라미터를 사용하는 방법을 정리합니다. 파라미터는 지난번에 turtlesim으로 한번 찍먹해본 적이 있습니다. 간단히 말하자면 노드 내에서 사용되는 변수이고, 이 값을 노드 내 또는 ros2 param 명령어를 통해 확인하거나 수정할 수 있습니다. 이번 게시글에서는 직접 생성, 수정을 해보도록 하겠습니다.1. parameter 전용 패키지 생성우선 파라미터 예제를 실습할 패키지를 생성합니다. 패키지의 위치는 ~/dev_ws/src 폴더 안입니다.$ ros2 pkg create --build-type ament_python python_parameters --dependencies rclpy--dependencies 명령어를 통해 rclpy 의존성 패키지를 추가합니다. 이렇게 추가한 의존성은 자동으로 package.xml 파일과 CMakeLists.txt 파일에 의존성 라인이 추가됩니다….만 파이썬 패키지는 CMakeLists.txt 파일이 없으므로 추가되지 않습니다. setup.py 파일에는 자동으로 추가되지 않으니 수정해 줘야 합니다. 우선 노드부터 작성 후 수정합시다.2. Python 노드 작성dev_ws/src/python_parameters/python_parameters 폴더에 python_parameters_node.py 노드 소스 파일을 작성합니다. 내용은 다음과 같습니다.import rclpyimport rclpy.nodefrom rclpy.exceptions import ParameterNotDeclaredExceptionfrom rcl_interfaces.msg import ParameterTypeclass MinimalParam(rclpy.node.Node): def __init__(self): super().__init__('minimal_param_node') timer_period = 2 # seconds self.timer = self.create_timer(timer_period, self.timer_callback) self.declare_parameter('my_parameter', 'world') def timer_callback(self): my_param = self.get_parameter('my_parameter').get_parameter_value().string_value self.get_logger().info('Hello %s!' % my_param) my_new_param = rclpy.parameter.Parameter( 'my_parameter', rclpy.Parameter.Type.STRING, 'world' ) all_new_parameters = [my_new_param] self.set_parameters(all_new_parameters)def main(): rclpy.init() node = MinimalParam() rclpy.spin(node)if __name__ == '__main__': main()코드를 분석해 봅시다. import 명령어로 의존성 패키지를 추가합니다. rclpy.exceptions는 파라미터를 사용하거나 수정하기 전에 선언이 안되있으면 ParameterNotDeclaredException 예외가 발생하게 하는 의존성입니다.import rclpyimport rclpy.nodefrom rclpy.exceptions import ParameterNotDeclaredExceptionfrom rcl_interfaces.msg import ParameterType다음 부분은 MinimalParam의 생성자 함수입니다. 이 함수에서는 노드 이름, 타이머, 파라미터 선언을 합니다. 타이머는 시간 간격 timer_period와 타이머에 맞춰 실행될 함수 timer_callback을 타이머 함수에 대입합니다. 그리고 declare_parameter 함수를 사용하여 my_parameter라는 이름의 파라미터에 world 문자열을 저장하여 초기화합니다.class MinimalParam(rclpy.node.Node): def __init__(self): super().__init__('minimal_param_node') timer_period = 2 # seconds self.timer = self.create_timer(timer_period, self.timer_callback) self.declare_parameter('my_parameter', 'world')타이머에 따라 실행될 timer_callback 함수입니다. 이 함수는 현재 파라미터의 내용을 출력해주는 기능과 ros2 param set 명령어로 파라미터를 수정했을 때 다시 원래 내용으로 되돌리는 기능을 갖고 있습니다. 파라미터 노드 생성 클래스의 내용은 이 함수까지입니다.def timer_callback(self): my_param = self.get_parameter('my_parameter').get_parameter_value().string_value self.get_logger().info('Hello %s!' % my_param) # world로 되돌리는 구문 my_new_param = rclpy.parameter.Parameter( 'my_parameter', rclpy.Parameter.Type.STRING, 'world' ) all_new_parameters = [my_new_param] self.set_parameters(all_new_parameters)마지막으로 남은 부분은 main함수와 main함수의 실행부입니다.def main(): rclpy.init() node = MinimalParam() rclpy.spin(node)if __name__ == '__main__': main()3. setup.py 파일 수정setup.py 파일을 수정합니다.entry_points={ 'console_scripts': [ 'param_talker = python_parameters.python_parameters_node:main', ],},4. 빌드, 소스, 실행이 과정은 많이 했으니 설명은 생략하겠습니다.$ colcon build --packages-select python_parameters$ . install/setup.bash실행 결과위에서 작성한 param_talker 노드의 실행 결과입니다.$ ros2 run python_parameters param_talker [INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world!이제 ros2 param set 명령어로 파라미터를 수정해 보겠습니다.$ ros2 param set /minimal_param_node my_parameter earthSet parameter successfulparam_talker 노드가 실행되고 있는 터미널의 결과는 다음과 같습니다.$ ros2 run python_parameters param_talker [INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello earth![INFO] [minimal_param_node]: Hello world![INFO] [minimal_param_node]: Hello world!my_parameter 파라미터가 잠깐 earth로 바뀌었다가 다시 world로 돌아왔습니다. 물론 위 소스의 원래대로 되돌리는 부분을 지우면 earth 상태가 고정됩니다.launch 파일을 사용한 파라미터 수정그렇다면 소스는 그대로 두고 파라미터만 수정할 수는 없을까요? launch 파일을 작성하면 할 수 있습니다. ~/dev_ws/src/python_parameters 폴더에 launch 폴더를 생성하여 python_parameters_launch.py 파일을 작성합니다. 내용은 다음과 같습니다.from launch import LaunchDescriptionfrom launch_ros.actions import Nodedef generate_launch_description(): return LaunchDescription([ Node( package='python_parameters', node_executable='param_talker', node_name='custom_parameter_node', output='screen', parameters=[ {'my_parameter': 'earth'} ] ) ])그 후 setup.py 파일에 import 구문과 data_files 안의 구문을 추가합니다. launch 파일을 실행하기 위해 필요한 구문입니다.import osfrom glob import glob# ...setup( # ... data_files=[ # ... (os.path.join('share', package_name), glob('launch/*_launch.py')), ] )일반적인 노드 선언에서 다음 부분이 추가되었습니다.parameters=[ {'my_parameter': 'earth'}]my_parameter 파라미터를 earth로 set하는 기능입니다. 물론 원래대로 되돌리는 구문에 의해 되돌아오긴 하지만 이런 방식으로 launch 파일에서 파라미터를 사용할 수 있습니다. 다만, launch 파일을 실행하면 터미널 창에 출력값이 보이지 않으므로 ros2 param get 명령어로 확인할 수 있습니다.$ ros2 launch python_parameters python_parameters_launch.py [INFO] [launch]: All log files can be found below /home/bhbhchoi/.ros/log/2021-02-15-15-14-38-127277-bhbhchoi-900X3L-11707[INFO] [launch]: Default logging verbosity is set to INFO[INFO] [param_talker-1]: process started with pid [11720]$ ros2 param get /custom_parameter_node my_parameterString value is: earth저는 결과가 잘 나오는지 확인하기 위해 원래대로 되돌리는 구문은 주석처리하고 빌드, 소스 한 후 실행했습니다. 계속해서 earth로 잘 나오고 있음을 볼 수 있습니다.마무리ROS2 첫걸음 정리 게시글은 이것으로 마무리 하겠습니다. 아래 페이지를 번역만 해 놓은 것 같은 느낌이 들지만 ROS2를 맛볼 수 있었던 좋은 기회가 되었습니다.참고 사이트ROS Index-ROS2 튜토리얼 파라미터 사용(Python)"
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (3) - Nerual Network 비선형 모델",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-dl-nnnl.html",
      "date"        : "2021-02-08 16:24:39 +0900",
      "description" : "",
      "content"     : "지난 게시글의 복습지난 게시글들에서는 뉴럴 네트워크를 활성화 함수에 따라 이산 모델, 연속 모델로 나누어서 살펴보았습니다. 뉴럴 네트워크의 최종 목적은 정확한 예측 모델, 선형 함수를 만드는 것입니다. 이 선형 함수를 이산 모델에서는 Step Function, 연속 모델에서는 Sigmoid 또는 Softmax 함수에 대입합니다. 그 후 이산 모델에서는 퍼셉트론 알고리즘으로 가중치를 조절, 연속 모델에서는 Cross-Entropy 오차함수와 경사하강법을 사용하여 가중치를 조절합니다. 그 결과, 뉴럴 네트워크의 예측 모델인 선형 함수를 도출할 수 있습니다.비선형 모델 - 다층 신경망이번 게시글에서는 비선형 모델에 대해서 알아보겠습니다. 비선형 모델은 어떤 식으로 찾을 수 있을까요? 바로 선형 함수를 여러개 합치는 것입니다. 간단한 예시를 보겠습니다.위의 그림은 두 선형 함수를 합쳐서 비선형 함수를 만들어내는 과정입니다. 이 과정을 노드 그래프로 나타내면 다음과 같습니다.맨 좌측부터 입력층, 은닉층, 출력층으로 이루어져 있습니다. 이렇게 입력층과 출력층 외의 은닉층이 존재하는 뉴럴 네트워크를 다층 신경망이라고 합니다. 사실 은닉층이라고 해도 하는 일은 똑같습니다. 입력층의 입력을 받아 선형모델의 결과를 활성화 함수에 대입하여 은닉층의 결과를 도출합니다. 다시 은닉층의 결과값을 출력층에 대입하여 나온 출력이 바로 비선형 모델의 결과물입니다. 물론 여기서 추측할 수 있는 점은 입력이 $x_1, x_2, \\cdots , x_n$과 같이 많아지면 많아질수록 출력이 나오기까지, 즉 결과가 분류되기까지 은닉층이 많아질 것입니다. 그렇다면 점점 더 복잡한 비선형 함수를 학습할 수 있게 됩니다.순전파 (Feedforward)이때 비선형 모델의 결과를 내는 과정을 순전파라고 합니다. 순리를 거스르지 않고 입력층부터 은닉층을 거쳐 출력층에서 출력을 내보냅니다. 위의 비선형 모델 다층 신경망 그림에 나온 과정을 그대로 따라갑니다.역전파 (Backpropagation)역전파는 순전파와 반대로 출력값으로부터 입력값 방향으로 계산하여 가중치 조정값을 찾는 과정을 의미합니다. 저번 게시글에서 봤던 경사 하강법과는 다르게 다층 신경망에서 오차함수의 미분을 구할 때 사용하는 방법입니다. 오차함수를 가중치로 미분한 값의 의미는 이 가중치가 오차에 얼마나 영향을 미치는가입니다. 저번과 마찬가지로 수정할 가중치에 대해 연쇄법칙을 사용하여 그 가중치가 영향을 미치는 노드들을 출력 쪽에서부터 되짚어 가면서 오차의 미분을 구합니다. 위의 그림에 표시된 가중치는 $W^{(1)}{11}$입니다. 이 가중치에 대한 오차의 미분값을 계산해 보겠습니다. 오차함수를 $E(W)$로 표현하면, 오차를 가중치 $W^{(1)}{11}$에 대하여 미분한 값은 $\\large{\\frac{\\partial E}{\\partial W^{(1)}_{11}}}$입니다. 이 미분값을 연쇄법칙을 사용하여 나타낸 값은 다음과 같습니다.$\\large{\\frac{\\partial E}{\\partial W^{(1)}_{11}}=\\frac{\\partial E}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial h}\\frac{\\partial h}{\\partial h_1}\\frac{\\partial h_1}{\\partial W^{(1)}_{11}}}$연쇄법칙한 결과의 각 항은 경사 하강법 때 계산했던 방식과 같은 방법으로 구할 수 있습니다.마무리이번 강의에서 배운 뉴럴 네트워크는 여기까지입니다. 다음에 정리할 내용은 Tensorflow의 사용에 대해 정리해 보겠습니다.참고 사이트Udacity Self-driving car nanodegree - Neural Network(링크 공유 불가능)"
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (2) - Nerual Network 연속 모델",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-dl-nncm.html",
      "date"        : "2021-02-06 13:35:23 +0900",
      "description" : "",
      "content"     : "이산 모델에서 연속 모델로저번 게시글에선 로지스틱 회귀의 이산 모델에 대해서 정리했습니다. 하지만 경사 하강법을 사용하기 위해서는 연속 모델을 사용해야 합니다. 이산 모델과 비교하여 연속 모델에서 추가된 과정은 다음과 같습니다. Sigmoid 함수 (확률함수) Maximum Likelihood Estimation (최대우도법) Cross-Entropy Gradient Descent (경사 하강법)경사 하강법을 사용하기 위해서는 Cross-Entropy가 연속이어야 하고, Cross-Entropy에서 사용되는 최대우도법을 위해서는 확률 개념이 도입되어야 합니다. 이에 따라 선형 모델에 대한 확률을 나타낸 것이 바로 Sigmoid 함수입니다.1. Sigmoid 함수저번 게시글에서도 언급했던 시그모이드 함수입니다. 활성화 함수로 사용된다는 점에서 Step Function과 같은 포지션의 함수입니다. 하지만 Step Function과는 다르게 $x=0$을 기준으로 0과 1로 딱딱 맞게 나눠지지는 않습니다. 하지만 $x$의 절댓값이 약 5인 지점까지는 시그모이드 함수의 출력이 0 또는 1이 되지 않습니다. 이러한 점에 주목해 시그모이드 함수를 확률함수이자 활성화 함수로 사용합니다. 이 함수는 로지스틱 회귀에 활성화 함수로서 사용하기 때문에 로지스틱 함수라고도 불립니다.2. Maximum Likelihood Estimation (최대우도법)$\\large{P(x|\\theta)=\\prod_{k=1}^{n}P(x_k|\\theta)\\;\\;\\;\\;\\;\\;\\;P(x|\\theta):\\;예측이\\;맞을\\;확률}$그 다음 과정은 시그모이드 함수에서 구한 확률들에 최대우도법을 적용하여 가장 좋은 모델, 즉 가장 확률이 높은 모델을 선정합니다. 최대우도법은 각 데이터 샘플에서 예측이 실제와 같을 확률을 계산하여 모두 곱한 것입니다. 계산된 확률을 더해주지 않고 곱해주는 것은 모든 데이터들의 추출이 독립적으로 동시에 일어나는 사건이기 때문입니다. 따라서 최대우도법 계산 결과값이 가장 높은 것을 가장 정확한 예측으로 봅니다.$\\large{ln(P(x|\\theta))=\\sum_{k=1}^{n}ln(P(x_k|\\theta))}$하지만 이 경우 하나의 확률만 바뀌어도 결과값이 심하게 바뀌므로, 곱셈을 덧셈의 형태로 표현해 줄 수 있는 로그함수를 취합니다. 덧셈으로 바꾸면 값 하나가 바뀌어도 결과값에 큰 영향이 가지 않습니다. 이 결과는 Cross-Entropy 오차함수를 만드는데 사용됩니다.3. Cross-Entropy 오차함수Cross-Entropy는 오차함수로 출력값이 작을수록 모델이 정확하다는 의미를 나타냅니다. 지난 과정에서 log likelihood 함수에 1보다 작은 값인 확률을 대입하기 때문에 결과가 항상 음수입니다. 따라서 비교의 용이를 위해 (-)부호를 취해 양수로 만들어 줍니다.$\\large{MLE=\\sum_{k=1}^{n}ln(p_i)\\rightarrow-\\sum_{k=1}^{n}ln(p_i)}$그 다음 이진적으로, 예를 들어 선물이 있다/없다, 샘플이 제대로 분류 되었다/되지 않았다를 판단할 때는 한쪽의 확률을 $p_i$, 다른 쪽의 확률을 $1-p_i$로 둡니다. 그리고 확률이 $p_i$일 때를 $y_i$=1, $1-p_i$일때를 $y_i$=0으로 두면 다음과 같은 식을 세울 수 있습니다.$\\large{Cross-Entropy = -\\sum_{k=1}^{n}\\left\\{\\begin{matrix}ln(p_i)\\;\\;\\;\\;\\;\\;if\\;y_i=1\\\\ ln(1-p_i)\\;\\;\\;\\;if\\;y_i=0\\end{matrix}\\right.}$이때 $y_i$는 실제값으로 볼 수 있는데, $p(x=1)$, $p(x=0)$의 확률로 볼 수 있습니다. 이처럼 확률이 0 또는 1만으로 결과가 나오는 확률변수를 베르누이 확률변수라고 합니다. 위의 공식을 한줄로 표현하면 다음과 같은 식으로 나타낼 수 있습니다.$\\large{\\begin{align*}Cross-Entropy(y_i, p_i) &amp;= -\\sum_{k=1}^{n}\\left\\{\\begin{matrix}ln(p_i)\\;\\;\\;\\;\\;\\;if\\;y_i=1\\\\ ln(1-p_i)\\;\\;\\;\\;if\\;y_i=0\\end{matrix}\\right. \\\\&amp;=-\\sum_{k=1}^{n}y_iln(p_i)+(1-y_i)ln(1-p_i)\\end{align*}}$이 공식에서 Cross-Entropy의 이름이 왜 교차 엔트로피인지를 볼 수 있습니다. $y_i$와 $p_i$ 두 확률이 교차(Cross)하는 계산에 의해 Entropy, 즉 정보량이 정해진다고 하여 Cross-Entropy인 것입니다. 즉 두 확률을 기반으로 구한 정보량이 Cross-Entropy인 것입니다. 결과값을 관찰하면 값이 작을수록, 즉 관계성이 옅을수록 낮은 값이 나옵니다. 다음의 예시가 이해를 도와줄 것입니다.위의 예시는 3개의 문 뒤에 선물이 있을 확률을 나타낸 것입니다. n번째 문 뒤에 선물이 있을 확률은 각각 $p_n$입니다. 그리고 실제값 $y_i$는 선물이 있을 때 1, 없을 때 0을 의미합니다. 이때 일어날 확률이 가장 높은 값, 즉 예측값은 (0.8 0.7 0.9)이고 이때의 실제값은 (1 1 0)입니다. 따라서 이 확률과 실제값을 Cross-Entropy 오차함수에 대입하면 위의 그림에서 볼 수 있듯 0.69가 나옵니다. 반대로 가장 일어날 확률이 낮은 값, 즉 예측값에서 가장 먼 값을 Cross-Entropy 오차함수에 대입하면 5.12가 나옵니다. 예측값에서 작은 값을 출력되고, 예측값에서 먼 값일수록 큰 값을 출력하는 특징을 이용하여 Cross-Entropy를 오차함수로 사용합니다.4. Gradient Descent오차가 큰지 작은지를 구했다면 구한 오차를 기반으로 가중치와 바이어스를 보정합니다. 여기서 경사 하강법을 사용하는데, 이는 적절한 가중치와 바이어스를 찾는 방법입니다. 높은 산에서 경사를 따라 내려오듯이 오차의 미분값을 따라 가중치를 조정합니다. 이산 모델에서의 퍼셉트론 알고리즘과 비슷한 포지션에 있습니다. 다음 이미지는 평균 제곱 오차(이하 MSE)의 경사하강법을 나타낸 그림입니다. 가로축은 가중치 $W$, 세로축은 오차 $Error$입니다.$\\large{E(W)=\\frac{1}{2m}\\sum_{k=1}^{m}(y_i-\\sigma(Wx+b))^2}$MSE 함수는 오차함수로서 가중치와 오차의 관계를 2차 방정식으로 표현할 수 있습니다. 따라서 그림과 같은 형태의 그래프가 나오게 됩니다. 적절한 가중치를 찾기 위해서는 이 오차가 가중치에 끼치는 영향을 찾아 가중치에 더하거나 빼서 가중치를 보정해 줍니다. 그 과정은 다음과 같습니다.$\\large{\\hat{y}=\\sigma(Wx+b)}$$\\large{\\hat{y}=\\sigma(w_1x_1+w_2x_2+\\cdots+w_nx_n+b)}$선형 모델을 활성화 함수에 넣은 모습입니다. 출력값은 확률입니다.$\\large{\\triangledown E=(\\frac{\\partial E}{\\partial w_1},\\frac{\\partial E}{\\partial w_2},\\cdots,\\frac{\\partial E}{\\partial w_n})}$이때 오차함수는 MSE, Cross-Entropy 등을 사용합니다.$\\large{\\alpha=0.1\\;\\;(학습률)}$오차함수의 미분값을 얼마나 반영할 것인지 정합니다. 학습률이 너무 작으면 오차 최솟값까지 가는데 시간이 너무 오래 걸릴 수 있고, 학습률이 너무 크면 가중치가 오차 최솟값이 되는 지점을 넘어가버려 오차 최솟값에 수렴하지 못할 수 있습니다.$\\large{w_i' \\leftarrow w_i - \\alpha\\frac{\\partial E}{\\partial w_i}}$$\\large{b_i' \\leftarrow b_i-\\alpha\\frac{\\partial E}{\\partial b}}$가중치와 바이어스에 오차함수의 미분에 비례한 값을 조정해 줍니다. 미분값이 작아질수록 가중치 변화가 작아지고, 0이 되면 최적 가중치가 됩니다. 모든 샘플에 대해 이 과정을 수행하기 때문에 이 새로운 가중치로 다시 경사 하강법을 수행합니다.$\\large{\\hat{y}=\\sigma(W'x+b')}$Cross-Entropy의 W-Error 그래프사실 경사하강법에서 가장 문제가 되는 부분은 오차함수의 미분을 구하는 부분입니다. 그 이외에는 간단하기에 Cross-Entropy 오차함수의 미분을 구하는 방법을 알아보겠습니다. 먼저 시그모이드 함수의 미분을 구합니다.$\\large{z=Wx+b}$$\\large{\\hat{y}=\\sigma(z)\\;(Sigmoid)}$$\\large{\\frac{\\partial \\sigma}{\\partial z}=\\sigma(z)(1-\\sigma(z))}$시그모이드 함수의 미분은 간단하게 $\\sigma(z)(1-\\sigma(z))$로 표현할 수 있습니다. 다시 Cross-Entropy 오차함수를 참고하면 다음과 같습니다.$\\large{E(W)=-\\sum_{k=1}^{n}y_iln(\\sigma(z))+(1-y_i)ln(1-\\sigma(z))}$미분하기 두려워지게 생겼지만 괜찮습니다. 연쇄법칙을 적용하여 $\\frac{\\partial E}{\\partial W}$를 풀어줍시다.$\\large{\\begin{align*}\\frac{\\partial E}{\\partial W}&amp;=\\frac{\\partial E}{\\partial \\sigma}\\frac{\\partial \\sigma}{\\partial z}\\frac{\\partial z}{\\partial W}\\;\\;\\;(연쇄법칙) \\\\&amp;=(\\frac{1-y_i}{1-\\sigma}-\\frac{y_i}{\\sigma})(\\sigma(1-\\sigma))X \\\\&amp;=(\\sigma(z)-y_i)X \\\\&amp;=(\\hat{y}-y_i)X\\end{align*}}$미분하기 어렵게 생겼던 것 치고는 간단한 형태의 미분값이 나왔습니다. 이 값을 다음 식에 대입하여 새로운 가중치와 바이어스를 찾습니다.$\\large{w_i'\\leftarrow w_i-\\alpha \\frac{\\partial E}{\\partial W}}$$\\large{w_i'\\leftarrow w_i+\\alpha (y_i-\\hat{y})x_i}$$\\large{b_i'\\leftarrow b_i-\\alpha \\frac{\\partial E}{\\partial b}}$$\\large{b_i'\\leftarrow b_i+\\alpha (y_i-\\hat{y})}$생각보다 간결한 결과가 나왔습니다. 경사하강법은 이 과정을 모든 샘플에 대해 반복하여 가중치와 바이어스를 조정합니다. MSE의 그래프는 Cross-Entropy의 W-Error 그래프에 비해 직관적이기에 이해하기 쉽지만, 실제 Cross-Entropy 오차함수는 무시무시하게 생긴 경우가 많습니다. 실제 Cross-Entropy의 함수는 아니지만, 다음 그림처럼 아주 복잡한 함수를 예시로 들어 봅시다.경사 하강법을 사용하는 데는 두 가지 문제가 있습니다.첫번째는 그림에서 볼 수 있는 전역 최소값과 지역 최소값이 존재한다는 점입니다. 우리의 목표는 당연히 오차가 가장 낮아지는 전역 최솟값을 찾는 것입니다. 하지만 경사 하강법에는 지역 최소값에 대한 면역요소가 없기에, 지역 최소값에 속아넘어갈 수 있습니다. 이 문제를 해결하기 위해 고안된 것이 모멘텀(Momentum)이란 방법입니다. 기존에 업데이트했던 미분값의 일정 비율을 남겨서 현재의 미분값에 더하여 가중치를 업데이트 하는 방식입니다.두번째 문제는 모든 데이터를 계산하기에 수고가 많이 든다는 점입니다. 퍼셉트론 알고리즘과 다르게 맞는 샘플도, 틀린 샘플도 모두 검사하기에 계산량이 매우 많습니다. 이 문제를 해결하는 방법은 무작위로 샘플을 뽑아서 가중치 업데이트를 수행하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)입니다. 완전히 정확한 결과를 얻는 것은 아니지만, 무작위 추출된 샘플이란 점에서 평균에 가까운 결과를 얻을 수 있고, 무엇보다 시간을 많이 단축시킬 수 있어 사용하는 방법입니다.Perceptron Algorithm VS Gradient Descent퍼셉트론 알고리즘과 경사 하강법의 차이는 바로 샘플의 검사 범위입니다. 퍼셉트론 알고리즘은 잘못 분류된 샘플만 검사하고, 경사 하강법은 모든 샘플을 검사합니다. 따라서 모델은 경사 하강법이 더 정확하게 만들지만 수행하는데 걸리는 시간은 퍼셉트론 알고리즘이 더 짧습니다. 공학은 역시 Trade-off입니다.Softmax와 One-Hot Encoding$\\large{Softmax(z)=\\frac{z_i}{\\sum_{k=1}^{n}z_i}\\;\\;\\;\\rightarrow\\;\\;\\;\\frac{e^{z_i}}{\\sum_{k=1}^{n}e^{z_i}}}$소프트맥스는 시그모이드, step function과 같은 활성화 함수입니다. 3개 이상의 범주에 대한 확률을 나타낼때 사용합니다. 선형 모델의 각 결과값$(z_i)$을 모든 결과값의 합으로 나누어 표현합니다. 이는 결과값인 확률들의 총합을 1로 만들기 위함입니다. 하지만 이때 선형 모델의 결과값이 음수인 원소가 있을 때, 분모가 0이거나 0 이하로 내려가는 문제가 발생합니다. 소프트맥스는 활성화 함수로서 출력이 확률, 즉 양수로 나와야 하기 때문에 이 문제를 해결하기 위해 exp 함수를 사용합니다. exp 함수를 사용하면 선형 모델의 결과값의 합이 음수로 나오거나 분모가 0이 되는 경우를 막을 수 있습니다. 다음 그림으로 예시를 들겠습니다.선형 모델의 결과가 (2.0 1.0 0.1)일 때, 결과값은 (0.7 0.2 0.1)이 나옵니다. 실제 결과값은 (0.6590011388859679, 0.2424329707047139, 0.09856589040931818)이지만, 소수점 둘째 자리에서 반올림한 값으로 생각합시다. 선형 모델의 결과가 높을수록 높은 확률이 출력되고, Softmax 함수의 출력값을 합하면 1이 되는 특징을 갖고 있습니다.원 핫 인코딩은 정답만을 1로 만드는 데이터 전처리 방식입니다. 컴퓨터가 데이터를 학습하기 전에 데이터를 가공해주는 것입니다. 정답에 1을 부여하고 정답이 아닌 항에는 0을 부여합니다. 예를 들어 위의 표에서 [1, 0, 0] 벡터는 사과를 의미합니다. [0, 1, 0] 벡터는 치킨을 의미합니다. 이 방식으로 크로스 엔트로피에 들어갈 실제값 데이터를 만듭니다. 다음 그림을 통해 예시를 살펴보겠습니다.Softmax 결과값과 실제값 데이터를 전처리한 One-Hot Encoding 값을 비교합니다. 이 차이를 가지고 오차함수로 사용합니다. 이 경우, 그냥 |One-Hot Encoding 값 - Softmax 결과값|을 하여 오차를 구할 수도 있습니다. 이 방법을 사용하여 평균을 구한 것이 위에 잠깐 나왔던 MSE(평균 제곱 오차) 함수입니다. 하지만 여기서는 Cross-Entropy를 사용합니다. Softmax 함수의 결과값인 $S(y)$와 One-Hot Encoding의 결과인 라벨 $L$을 Cross-Entropy 오차함수에 대입하여 오차를 구합니다. 그러고 나면 위에서 설명했듯 Cross-Entropy는 두 확률간의 관계를 나타내는 방식으로 오차를 산출합니다.마무리다음 게시글은 NN의 비선형 모델에 대해 정리하겠습니다. 너무 길어 3편으로 나누어서 정리했네요. 한 강의에 이걸 다 넣을줄이야… 아무튼 다음 게시글에서 NN을 마무리하겠습니다.질문 이산 모델에서 연속 모델로 변경하는 이유를 잘 모르겠습니다. 정리한 게 맞나요? 최대우도법에서 초기 확률을 부여하는 법? MSE보다 Cross-Entropy를 사용하는 이유?참고 사이트Udacity Self-driving car nanodegree - Neural Network(링크 공유 불가능)ratsgo’s blog - 로지스틱 회귀공돌이의 수학정리노트 - 최대우도법(MLE)John 블로그 - 데이터 전처리 : 레이블 인코딩과 원핫 인코딩"
    } ,
  
    {
      "title"       : "[Udacity] Deep Learning (1) - Nerual Network 이산 모델",
      "category"    : "",
      "tags"        : "Udacity, Deep Learning",
      "url"         : "./uda-dl-nndm.html",
      "date"        : "2021-02-04 15:14:23 +0900",
      "description" : "",
      "content"     : "Neural Network란?뉴럴 네트워크는 우리말로 신경망이라는 의미입니다. 옛날 옛적 공학자들은 컴퓨터에 지능, 즉 학습능력을 부여할 방법을 고민했고, 결국 사람의 뇌 구조를 모방하는 방법을 고안했습니다. 우리 뇌 속의 뉴런 세포가 연결 및 신호를 주고받는 방식을 알고리즘으로 구현하여 신경망이라는 것을 만들었습니다.위 그림을 보면 뉴런 세포의 가지돌기(Dendrite)로 전기 신호가 전달되어 핵(Nucleus)을 통해 축삭(Axon)으로 전기 신호가 나갑니다. 지금부터 이러한 뉴런의 구조를 어떻게 구현했는지 알아볼 것입니다.선형 회귀 (Linear Regression)먼저 가장 기초적인 선형 회귀를 살펴보겠습니다. 선형 회귀란 자료가 분포한 형태를 보고 $x$축과 $y$축 사이의 샘플 관계를 선형으로 나타내는 방법입니다. 일반적으로 선형 회귀 모델의 형태는 다음과 같습니다.$\\large{w_1x_1+w_2x_2+b=0}$위의 공식에서는 가중치가 $w_1$, $w_2$ 둘 뿐이지만 실제로는 고려해야 할 가중치 수만큼 필요합니다. 데이터의 수가 $n$개라면, 그 수인 $n$개만큼 가중치가 존재합니다. 데이터와 가중치를 표현하면 다음과 같습니다.$\\large{X=\\begin{bmatrix}x_1 \\\\x_2 \\\\\\vdots \\\\x_n\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\;\\;W=\\begin{bmatrix}w_1 \\\\w_2 \\\\\\vdots \\\\w_n\\end{bmatrix}}$바이어스는 그냥 $b$로 선형 모델당 하나입니다. 이들을 행렬로 된 식으로 나타내면 다음과 같습니다.$\\large{WX+b=0}$하지만 이 강의에서는 이해를 돕기 위해서 2개의 가중치만 사용할 것입니다. 위의 그림에서 볼 수 있듯이 가중치와 바이어스 수정을 통해 선형 회귀 모델을 찾게 됩니다.로지스틱 회귀 (Logistic Regression)로지스틱 회귀는 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고, 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘입니다. 핵심은 분류입니다. 회귀라고 부르지만 정작 기능은 분류인 이상한 녀석입니다. 이 이름의 유래는 연속형 변수 대신 범주형 변수에 회귀시키려다 발생한 문제입니다.로지스틱 회귀를 사용하는 이유첫번째 예시는 나이에 따른 혈압의 증가 데이터입니다. 그래프를 보면 선형 회귀가 잘 이루어진 것을 볼 수 있습니다.두번째 예시는 나이에 따른 암 발생 여부 데이터입니다. 데이터 결과값이 yes or no, 즉 0과 1로 출력됩니다. 데이터의 경향을 잘 따르는 첫번째 그래프와는 달리 이번 그래프는 뭔가 이상합니다.결과값이 0~1 사이의 값이 나와야 하는데 양의 무한대, 음의 무한대까지 올라가 버립니다. 이것이 범주형 변수에 선형회귀를 적용했을 때의 문제입니다.이제 어떤 방식으로 이 문제를 해결할 수 있는지 봅시다.로지스틱 회귀의 개요로지스틱 회귀가 선택한 해결방법은 바로 활성화 함수(Activation Function)를 사용하는 것입니다. 활성화 함수는 선형 모델을 비선형 모델로 만들어 주는 역할을 하고 있습니다. 선형 모델만 다층으로 쌓을 경우, 결국 결과가 선형 모델이 되는 문제점이 있기에, 비선형 함수를 한번 거쳐주는 과정이 필요합니다. 로지스틱 회귀에서 주로 사용되는 활성화 함수는 Step, Sigmoid, Softmax 함수입니다. 이 함수들은 어떤 *모델에서 적용하는지에 따라, 그리고 분류할 범주의 갯수에 따라 사용 함수가 다릅니다. 모델에 따라서는 Step Function은 이산 모델에서 사용하고, Sigmoid와 Softmax는 연속 모델에서 사용합니다. 또한 범주에 따라서는 범주가 2개(일반적으로 0 또는 1, yes or no 같은것)일 때 Step Function과 Sigmoid, 3개 이상일때 Softmax를 사용합니다. 이 함수들에 대해서는 앞으로 더 자세히 알아보도록 하겠습니다.선형 경계면 (Linear Boundary)계속해서 로지스틱 회귀를 알아보겠습니다. 다음의 자료는 학생들의 시험 시행 횟수에 따른 평균 등급을 샘플로 나타낸 것입니다.시험 횟수가 많을수록, 그리고 성적이 높을수록 Pass 범주로, 시험 횟수가 적을수록, 그리고 성적이 낮을수록 Fail 범주로에 분류됩니다. 음… 그런데 시험 횟수가 적고 등급이 높은 사람은 불합격은 횟수가 걸릴 수 있으니 불합이 될 수 있지만, 시험 횟수가 많은데 등급이 낮은 사람은 합격이라… 이 결과는 뭔가 이상하네요. 아무튼 분류의 예시를 드는데는 문제가 없으니 이 데이터를 예시로 들겠습니다. 이 그림을 관찰해 보면 $w_1x_1+w_2x_2+b=0$의 선형 모델로 Pass와 Fail을 구분할 수 있습니다. 따라서 데이터를 선형 모델에 대입했을 때 $w_1x_1+w_2x_2+b$의 값이 0 이상이면, 즉 0이거나 양수이면 Pass, 0 이하, 즉 음수이면 Fail로 분류할 수 있습니다. 이때 Pass를 1, Fail을 0이라는 값을 부여한다고 하면 출력값, 결과 $y$의 함수는 다음과 같습니다.$\\large{y=\\left\\{\\begin{matrix}0\\;\\;\\;\\;if\\;w_1x_1+w_2x_2+b&lt;0\\\\ 1\\;\\;\\;\\;if\\;w_1x_1+w_2x_2+b\\geq 0\\end{matrix}\\right.}$이 $y$의 함수를 그래프로 나타낸 것이 바로 Step Function입니다. 간단히 말해서 양수일때 1, 음수일때 0의 결과를 도출하는 함수입니다. 이산 모델에선 이 함수를 활성화 함수로 사용하게 됩니다.Perceptron Algorithm퍼셉트론 알고리즘은 가중치와 바이어스를 조정해 주는 알고리즘입니다. 잘못 분류된 sample의 성분을 현재 선형 모델에 적용하여 가중치 $w_1, w_2$와 바이어스 $b$를 보정해 줍니다. 하지만 한번에 그 sample의 성분만큼 조정하면 가중치와 바이어스의 변화가 너무 급격하기 때문에 학습률 $\\alpha$로 반영 비율을 조정합니다. 이 과정을 잘못 분류된 모든 샘플에 대하여 시행합니다.이 알고리즘이 반복되는 동안 선형 모델은 잘못된 sample에 점점 더 가까이 접근합니다. 결국 과정이 끝나고 나면 위 그림의 마지막과 같이 잘못된 sample을 분류하게 됩니다.마무리이산 모델의 선형 모델 찾는 방법은 여기까지입니다. 선형 모델을 찾은 후, 활성화 함수에 대입하여 0 또는 1의 결과를 구합니다. 다음 게시글에서는 이산 모델에서 연속 모델로 변화했을 때 추가되는 과정들을 알아보겠습니다.질문 딥러닝 방식, 즉 다층 신경망을 사용하는 방법은 모두 이런 방식인가요? *이산 모델, 연속 모델이라고 사용하는게 맞나요? Positive면 (–)해주고 negative면 (+)해주는 이유가 뭔가요? 가중치에 sample값을 반영하여 가중치를 수정할 수 있는 이유? 가중치와 sample(x1 x2)의 관계? 가중치를 초기화해줄 수 있는 방법은 없나요?참고 사이트Udacity Self-driving car nanodegree - Neural Network(링크 공유 불가능)"
    } ,
  
    {
      "title"       : "[Udacity] Computer Vision (4) - Advanced CV",
      "category"    : "",
      "tags"        : "Udacity, Computer Vision",
      "url"         : "./uda-cv-adcv.html",
      "date"        : "2021-02-04 14:44:23 +0900",
      "description" : "",
      "content"     : "곡선 차선 검출지금까지 우리는 단지 차선을 검출한 방법에 대해서 공부했습니다. 하지만 직선이 아닌 곡선 차선일 경우 어떻게 자동차의 방향을 조절할 수 있을까요? 이번 게시글에서는 그 방법에 대해서 정리합니다. 대략적인 과정은 다음과 같습니다. 히스토그램 구하기 슬라이드 윈도우 생성 (시각화) 곡률 반경 구하기방향 조절을 위해서 곡률 반경을 구해야 합니다. 모터 제어값같은 하드웨어적 출력을 주는 코드는 컴퓨터 비전과는 관계가 없으므로, 결론적으로 곡률 반경을 구하는 것이 목적입니다.이번 강의에서는 위의 이미지를 예시로 사용합니다. 이 이미지는 저번 강의에서 사용했던 조감도(bird’s eye view)와 Gradient 검출 알고리즘으로 차선만을 흰색으로 검출한 결과입니다. 이번 강의는 거의 대부분이 코드 분석을 중심으로 진행됩니다. 단계별로 알아보도록 합시다.1. 히스토그램 구하기히스토그램은 통계학에서 나오는 막대그래프를 말합니다. $x$축의 각 좌표별, 즉 표본별 자료값을 나타낸 그래프입니다. 우리는 차선만을 검출하기 위해 히스토그램을 사용합니다. 코드를 통해 살펴봅시다.# 이미지의 하단부만 취함bottom_half = img[height//2:,:]먼저 이미지의 하단부만을 취하는 구문입니다. 다음 과정은 이미지의 세로 픽셀값을 모두 합하여 1차원 배열로 나타내는 것입니다. 따라서 그 과정의 계산을 줄이기 위해, 그리고 차선 시작점을 찾아야 하기 때문에 하단부 반쪽만 남겨서 계산합니다.# 이미지 하단부의 세로값을 모두 합침.histogram = np.sum(bottom_half, axis=0)이미지 세로값을 모두 합쳐 1차원 배열로 나타내는 구문입니다. 이때 결과로 나오는 배열이 히스토그램의 $y$축 값입니다. 이 배열을 plt.plot 함수를 사용하여 이미지 위에 출력하여 결과를 봅시다.히스토그램 구하기 예제import numpy as npimport matplotlib.image as mpimgimport matplotlib.pyplot as pltimg = mpimg.imread('/home/이미지 경로/warped_example.png')height = img.shape[0]width = img.shape[1]def hist(img): # 이미지의 하단부만 취함 bottom_half = img[height//2:,:] # 이미지 하단부의 세로값을 모두 합침. histogram = np.sum(bottom_half, axis=0) return bottom_half, histogrambottom_half, histogram = hist(img)# 히스토그램 시각화 - 값이 비약적으로 높은 부분이 차선(흰색값)plt.imshow(bottom_half, extent=[0, width, 0, height/2])plt.plot(histogram)plt.show()Input:Output:결과를 보면 이미지를 반토막내서 히스토그램을 구한 것을 볼 수 있습니다.2. 슬라이드 윈도우 생성다음 과정은 슬라이드 윈도우를 생성하는 것입니다. 코드는 크게 2가지 함수로 나누어집니다. 윈도우 내의 픽셀 좌표 찾기 히스토그램 추출 &amp; 최댓값(차선 중심 좌표) 구하기 윈도우 파라미터 설정 윈도우 그리기 &amp; 차선 픽셀 검출 차선을 의미하는 2차함수 곡선 구하기 &amp; 시각화 ployfit 함수 사용: 계수 찾기 차선, 곡선 시각화 윈도우 내의 픽셀 좌표를 찾는 함수와 그 픽셀 및 곡선을 시각화하는 함수로 나눌 수 있습니다. 각각의 함수 내의 단계를 코드를 보면서 알아보겠습니다.윈도우 내의 픽셀 좌표 찾기함수를 선언한 후 내용을 보겠습니다. 시각화 함수에서 사용할 것이므로 선언부를 적어두겠습니다.def find_lane_pixels(binary_warped):1. 히스토그램 추출 &amp; 최댓값(차선 중심 좌표) 구하기먼저 앞선 과정에서 알아봤던 히스토그램을 구합니다. 이제는 한줄로 축약해서 구할 수 있습니다.histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)그 후, 이미지의 좌측과 우측에서 각각 히스토그램 최댓값을 구합니다. 그 지점(leftx_base, rightx_base)을 첫번째 윈도우의 중심점으로 사용할 것입니다.# 좌측 차선/우측 차선을 나누기 위해 이미지의 중심점 설정.midpoint = np.int(histogram.shape[0]//2)# 히스토그램에서 최댓값 취하기=차선이 있는 곳leftx_base = np.argmax(histogram[:midpoint])rightx_base = np.argmax(histogram[midpoint:]) + midpoint2. 윈도우 파라미터 설정다음 과정으로 윈도우 파라미터들을 설정합니다. 파라미터는 윈도우 갯수, 윈도우 너비, 최소 픽셀 갯수 등의 처음 설정값입니다.# HYPERPARAMETERS# 윈도우 갯수nwindows = 9# 중심을 기준으로 좌우 윈도우 너비(100*2(양옆))margin = 100# 다음 중심을 정하기 위해 필요한 최소 픽셀 갯수minpix = 50# 윈도우의 높이 설정 - 이미지 세로/윈도우 갯수window_height = np.int(binary_warped.shape[0]//nwindows)# 이미지에서 0(검정색)이 아닌 픽셀 좌표nonzero = binary_warped.nonzero()nonzeroy = np.array(nonzero[0])nonzerox = np.array(nonzero[1])# 윈도우가 생겨날 중심, 계속 업데이트 됨leftx_current = leftx_baserightx_current = rightx_base# 차선 픽셀들이 저장될 리스트left_lane_inds = []right_lane_inds = []3. 윈도우 그리기 &amp; 차선 검출윈도우 갯수는 위의 파라미터에서 설정한 것과 같이 nwindows개 입니다. 따라서 반복문을 사용하여 nwindows번 반복합니다.# 윈도우 하나씩 만들어 보자.for window in range(nwindows):먼저 윈도우 사각형의 범위를 지정하고 그립니다. OpenCV에 내장된 rectangle 함수를 사용합니다.# window번째 윈도우의 범위 지정# 세로 범위win_y_low = binary_warped.shape[0] - (window+1)*window_heightwin_y_high = binary_warped.shape[0] - window*window_height# 왼쪽 가로 범위win_xleft_low = leftx_current - marginwin_xleft_high = leftx_current + margin# 오른쪽 가로 범위win_xright_low = rightx_current - marginwin_xright_high = rightx_current + margin # 윈도우 사각형 그리기cv2.rectangle(out_img, (win_xleft_low, win_y_low), (win_xleft_high, win_y_high), (0,255,0), 2)cv2.rectangle(out_img, (win_xright_low, win_y_low), (win_xright_high, win_y_high), (0,255,0), 2)그 다음, 윈도우 범위 내의 차선으로 예상되는 픽셀들을 구하여 leftx_lane_inds, rightx_lane_inds 리스트에 저장합니다. 취한 픽셀 갯수가 설정해준 최솟값 이상일 때, 다음 윈도우의 중심을 픽셀들의 $x$좌표의 평균으로 저장합니다.# 윈도우 범위 안에 있는 (차선으로 예상되는) nonzero 픽셀 취하기(1차원 배열)# good_left_inds: (차선으로 예상되는) nonzero 픽셀 위치 리스트good_left_inds = ((nonzeroy &gt;= win_y_low) &amp; (nonzeroy &lt; win_y_high) &amp; (nonzerox &gt;= win_xleft_low) &amp; (nonzerox &lt; win_xleft_high)).nonzero()[0]good_right_inds = ((nonzeroy &gt;= win_y_low) &amp; (nonzeroy &lt; win_y_high) &amp; (nonzerox &gt;= win_xright_low) &amp; (nonzerox &lt; win_xright_high)).nonzero()[0] # 취한 nonzero array(픽셀 리스트) 저장 (array, array, ...) 형태로 저장(술에 취한 아님)# left_lane_inds: 각 층의 윈도우 안의 차선으로 예상되는 픽셀 리스트left_lane_inds.append(good_left_inds)right_lane_inds.append(good_right_inds) # 취한 nonzero 픽셀 갯수가 minpix 이상일 때, 다음 차선의 중심(윈도우가 생겨날 곳)을 픽셀들의 x좌표의 평균으로 취함.if len(good_left_inds) &gt; minpix: leftx_current = np.int(np.mean(nonzerox[good_left_inds]))if len(good_right_inds) &gt; minpix: rightx_current = np.int(np.mean(nonzerox[good_right_inds]))# 배열1[배열2]: 배열2의 내용의 위치의 배열1 원소를 반환(numpy array만 가능)# ex) a=[1,2,3,4,5] b=[2,3] -&gt; a[b]=[3,4]그리고 어차피 이 좌표들은 차원이 상관 없기에 그냥 다 합쳐서 1차원 배열로 만듭니다. 그리고 좌우 차선을 의미하는 윈도우 안의 픽셀들과 이미지를 반환합니다.# 그냥 1차원 배열로 퉁침try: left_lane_inds = np.concatenate(left_lane_inds) right_lane_inds = np.concatenate(right_lane_inds)except ValueError: # Avoids an error if the above is not implemented fully pass# 좌우 윈도우 안의 픽셀 좌표들leftx = nonzerox[left_lane_inds]lefty = nonzeroy[left_lane_inds] rightx = nonzerox[right_lane_inds]righty = nonzeroy[right_lane_inds]return leftx, lefty, rightx, righty, out_img이로써 사실상 윈도우 안의 차선 검출은 끝났습니다. 하지만 진짜로 되었는지 확인하기 위해서 시각화 함수를 만들어 보겠습니다.차선을 의미하는 2차함수 곡선 구하기 &amp; 시각화함수 선언부는 다음과 같습니다.def fit_polynomial(binary_warped):1. ployfit 함수 사용: 계수 찾기이번 과정에서는 차선 픽셀을 보고 ployfit 함수로 2차함수의 계수를 찾습니다. 그리고 그 계수로 그래프를 그릴 수 있도록 방정식을 선언합니다.# 차선 픽셀 먼저 찾고leftx, lefty, rightx, righty, out_img = find_lane_pixels(binary_warped)# 차선을 2차함수로 보고 계수 구하기left_fit = np.polyfit(lefty, leftx, 2)right_fit = np.polyfit(righty, rightx, 2)# 그래프를 그리기 위한 방정식ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )try: left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2] right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]except TypeError: # Avoids an error if `left` and `right_fit` are still none or incorrect print('The function failed to fit a line!') left_fitx = 1*ploty**2 + 1*ploty right_fitx = 1*ploty**2 + 1*ploty2. 차선, 곡선 시각화차선과 곡선을 시각화 하기 위해 색상을 입힙니다. 이때 위에서 dstack 함수로 3채널로 만들어 주었던 보람이 생깁니다.## 시각화 ### 좌/우 차선 색상 입히기out_img[lefty, leftx] = [255, 0, 0]out_img[righty, rightx] = [0, 0, 255]# 좌/우 차선의 그래프 그리기(노란색)plt.plot(left_fitx, ploty, color='yellow')plt.plot(right_fitx, ploty, color='yellow')return out_img좌측 차선에 빨간색을, 우측 차선에 파란색을 입혔습니다. 2차 함수에는 노란색을 주어 잘 보이도록 했습니다.이 함수들을 실행하여 적절한 출력을 내는 코드를 작성하면, 다음과 같습니다.슬라이드 윈도우 예제import numpy as npimport matplotlib.image as mpimgimport matplotlib.pyplot as pltimport cv2img = mpimg.imread('/home/이미지 경로/warped_example.png')# 채널 1로 줄이기 - 원본 코드에서는 이 과정이 없었으나 채널 문제로 인한 오류로 구문을 추가하였음.# 원본 이미지는 채널이 4여서 채널 3을 요구하는 함수에서 오류가 발생했음.binary_warped = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)def find_lane_pixels(binary_warped): # 히스토그램 추출 histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0) # 시각화를 위한 채널 나누기, 각각 RGB 채널로 활용될 예정. out_img = np.dstack((binary_warped, binary_warped, binary_warped)) # 좌측 차선/우측 차선을 나누기 위해 이미지의 중심점 설정. midpoint = np.int(histogram.shape[0]//2) # 히스토그램에서 최댓값 취하기=차선이 있는 곳 leftx_base = np.argmax(histogram[:midpoint]) rightx_base = np.argmax(histogram[midpoint:]) + midpoint # HYPERPARAMETERS # 윈도우 갯수 nwindows = 9 # 중심을 기준으로 좌우 윈도우 너비(100*2(양옆)) margin = 100 # 다음 중심을 정하기 위해 필요한 최소 픽셀 갯수 minpix = 50 # 윈도우의 높이 설정 - 이미지 세로/윈도우 갯수 window_height = np.int(binary_warped.shape[0]//nwindows) # 이미지에서 0(검정색)이 아닌 픽셀 좌표 nonzero = binary_warped.nonzero() nonzeroy = np.array(nonzero[0]) nonzerox = np.array(nonzero[1]) # 윈도우가 생겨날 곳, 계속 업데이트 됨 leftx_current = leftx_base rightx_current = rightx_base # 차선 픽셀들이 저장될 리스트 left_lane_inds = [] right_lane_inds = [] # 윈도우 하나씩 만들어 보자. for window in range(nwindows): # window번째 윈도우의 범위 지정 # 세로 범위 win_y_low = binary_warped.shape[0] - (window+1)*window_height win_y_high = binary_warped.shape[0] - window*window_height # 왼쪽 가로 범위 win_xleft_low = leftx_current - margin win_xleft_high = leftx_current + margin # 오른쪽 가로 범위 win_xright_low = rightx_current - margin win_xright_high = rightx_current + margin # 윈도우 사각형 그리기 cv2.rectangle(out_img, (win_xleft_low, win_y_low), (win_xleft_high, win_y_high), (0,255,0), 2) cv2.rectangle(out_img, (win_xright_low, win_y_low), (win_xright_high, win_y_high), (0,255,0), 2) # 윈도우 범위 안에 있는 (차선으로 예상되는) nonzero 픽셀 취하기(1차원 배열) # good_left_inds: (차선으로 예상되는) nonzero 픽셀 위치 리스트 good_left_inds = ((nonzeroy &gt;= win_y_low) &amp; (nonzeroy &lt; win_y_high) &amp; (nonzerox &gt;= win_xleft_low) &amp; (nonzerox &lt; win_xleft_high)).nonzero()[0] good_right_inds = ((nonzeroy &gt;= win_y_low) &amp; (nonzeroy &lt; win_y_high) &amp; (nonzerox &gt;= win_xright_low) &amp; (nonzerox &lt; win_xright_high)).nonzero()[0] # 취한 nonzero array(픽셀 리스트) 저장 (array, array, ...) 형태로 저장(술에 취한 아님) # left_lane_inds: 각 층의 윈도우 안의 차선으로 예상되는 픽셀 리스트 left_lane_inds.append(good_left_inds) right_lane_inds.append(good_right_inds) # 취한 nonzero 픽셀 갯수가 minpix 이상일 때, 다음 차선의 중심(윈도우가 생겨날 곳)을 픽셀들의 x좌표의 평균으로 취함. if len(good_left_inds) &gt; minpix: leftx_current = np.int(np.mean(nonzerox[good_left_inds])) if len(good_right_inds) &gt; minpix: rightx_current = np.int(np.mean(nonzerox[good_right_inds])) # 배열1[배열2]: 배열2의 내용의 위치의 배열1 원소를 반환(numpy array만 가능) # ex) a=[1,2,3,4,5] b=[2,3] -&gt; a[b]=[3,4] # 그냥 1차원 배열로 퉁침. try: left_lane_inds = np.concatenate(left_lane_inds) right_lane_inds = np.concatenate(right_lane_inds) except ValueError: # Avoids an error if the above is not implemented fully pass # 좌우 윈도우 안의 픽셀 좌표들 leftx = nonzerox[left_lane_inds] lefty = nonzeroy[left_lane_inds] rightx = nonzerox[right_lane_inds] righty = nonzeroy[right_lane_inds] return leftx, lefty, rightx, righty, out_imgdef fit_polynomial(binary_warped): # 차선 픽셀 먼저 찾고 leftx, lefty, rightx, righty, out_img = find_lane_pixels(binary_warped) # 차선을 2차함수로 보고 계수 구하기 left_fit = np.polyfit(lefty, leftx, 2) right_fit = np.polyfit(righty, rightx, 2) # 그래프를 그리기 위한 방정식 ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] ) try: left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2] right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] except TypeError: # Avoids an error if `left` and `right_fit` are still none or incorrect print('The function failed to fit a line!') left_fitx = 1*ploty**2 + 1*ploty right_fitx = 1*ploty**2 + 1*ploty ## 시각화 ## # 좌/우 차선 색상 입히기 out_img[lefty, leftx] = [255, 0, 0] out_img[righty, rightx] = [0, 0, 255] # 좌/우 차선의 그래프 그리기(노란색) plt.plot(left_fitx, ploty, color='yellow') plt.plot(right_fitx, ploty, color='yellow') return out_imgout_img = fit_polynomial(binary_warped)plt.imshow(out_img)plt.show()Input:히스토그램 구하기 예제와 같습니다.Output:이전 프레임으로부터 윈도우 중심 찾기하지만 매 프레임마다 2차방정식 계수를 계산하는 것은 계산적으로 낭비가 많습니다. 따라서 이번에는 이전 프레임에서의 2차 방정식 계수를 기반으로 차선을 따라가는 윈도우를 작성합니다. 윈도우라기보단 한줄짜리 선을 이어붙인 도형입니다. 위의 슬라이드 윈도우 만들기와 과정은 비슷합니다. 코드를 통해 살펴도록 하겠습니다.먼저 다항식 계수를 구하는 함수입니다. 이후 작성할 함수에서 사용될 예정입니다.def fit_poly(img_shape, leftx, lefty, rightx, righty): # 다항식 계수 구하기 # 지역 변수로서 left_fit, right_fit. 전역 변수의 수정은 없다. left_fit = np.polyfit(lefty, leftx, 2) right_fit = np.polyfit(righty, rightx, 2) # 그래프 그리기 위해 방정식 &amp; 범위 규정 ploty = np.linspace(0, img_shape[0]-1, img_shape[0]) left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2] right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] return left_fitx, right_fitx, ploty다음 함수는 윈도우를 만드는 함수입니다. 선언부는 다음과 같습니다.def search_around_poly(binary_warped):저번 윈도우와 같이 탐색할 차선의 중심 기준 좌우 범위를 margin으로 놓습니다. 그 후 차선이 포함된 흰색 픽셀을 검출합니다.# HYPERPARAMETERmargin = 100# 흰색 픽셀 검출(이미지 전체)nonzero = binary_warped.nonzero()nonzeroy = np.array(nonzero[0])nonzerox = np.array(nonzero[1])그 후 한줄짜리 윈도우를 만듭니다. 사실 사각형이 아닌 선 하나이기 때문에 한줄짜리 윈도우라고 부릅니다. 슬라이드 윈도우에서 사용했던 윈도우 중심점 leftx_current, rightx_current 대신 left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2], right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2]를 사용합니다. 2차 방정식의 값을 중심으로 양 margin 범위 내에서 흰색 픽셀을 찾아냅니다. 이 픽셀들을 차선으로 봅니다. 그 후 찾은 픽셀들의 $x$, $y$ 값을 leftx, lefty, rightx, righty 변수에 저장합니다. 이 변수을 위에서 작성했던 다항식 계수를 구하는 함수에 대입하여 시각화를 위한 픽셀을 구합니다.# 윈도우 중심점 대신 left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] 를 사용.# 이전 프레임에서 사용했던 left, right_fit 사용, 한줄씩만 픽셀 검출left_lane_inds = ((nonzerox &gt; (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) &amp; (nonzerox &lt; (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin)))right_lane_inds = ((nonzerox &gt; (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) &amp; (nonzerox &lt; (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin))) # 검출한 차선의 x, y값 저장leftx = nonzerox[left_lane_inds]lefty = nonzeroy[left_lane_inds] rightx = nonzerox[right_lane_inds]righty = nonzeroy[right_lane_inds]# 다항식 결과물, 범위left_fitx, right_fitx, ploty = fit_poly(binary_warped.shape, leftx, lefty, rightx, righty)다음은 시각화 단계입니다. 지금까지 구한 정보를 토대로 윈도우, 그래프를 시각화합니다. 이 과정은 위의 슬라이드 윈도우 시각화 과정과 크게 다르지 않습니다.## 시각화 ### 컬러 시각화를 위한 채널 추가out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255window_img = np.zeros_like(out_img)# 좌우 차선 픽셀에 색상값 추가out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]# cv2.fillpoly() 함수에서 사용할 한줄짜리 윈도우 픽셀 좌표left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])left_line_pts = np.hstack((left_line_window1, left_line_window2))right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])right_line_pts = np.hstack((right_line_window1, right_line_window2))# 선을 따라가는 한줄짜리 윈도우 시각화cv2.fillPoly(window_img, np.int_([left_line_pts]), (0, 255, 0))cv2.fillPoly(window_img, np.int_([right_line_pts]), (0, 255, 0))result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0) # 곡선 그래프 시각화plt.plot(left_fitx, ploty, color = 'yellow')plt.plot(right_fitx, ploty, color = 'yellow') return result이전 프레임으로부터 윈도우 중심 찾기 예제지금까지 설명한 예제의 전문을 보겠습니다.import cv2import numpy as npimport matplotlib.image as mpimgimport matplotlib.pyplot as plt# Load our image - this should be a new frame since last time!img = mpimg.imread('/home/이미지 경로/warped_example.png')binary_warped = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# 이전 프레임에서의 2차 방정식 계수, 실제 코드에선 이전 프레임 것을 사용해야 함.left_fit = np.array([ 2.13935315e-04, -3.77507980e-01, 4.76902175e+02])right_fit = np.array([4.17622148e-04, -4.93848953e-01, 1.11806170e+03])def fit_poly(img_shape, leftx, lefty, rightx, righty): # 다항식 계수 구하기 # 지역 변수로서 left_fit, right_fit. 전역 변수의 수정은 없다. left_fit = np.polyfit(lefty, leftx, 2) right_fit = np.polyfit(righty, rightx, 2) # 그래프 그리기 위해 방정식 &amp; 범위 규정 ploty = np.linspace(0, img_shape[0]-1, img_shape[0]) left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2] right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] return left_fitx, right_fitx, plotydef search_around_poly(binary_warped): # HYPERPARAMETER margin = 100 # 흰색 픽셀 검출(이미지 전체) nonzero = binary_warped.nonzero() nonzeroy = np.array(nonzero[0]) nonzerox = np.array(nonzero[1]) print(nonzeroy) # 윈도우 중심점 대신 left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] 를 사용. # 이전 프레임에서 사용했던 left, right_fit 사용, 한줄씩만 픽셀 검출 left_lane_inds = ((nonzerox &gt; (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) &amp; (nonzerox &lt; (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) right_lane_inds = ((nonzerox &gt; (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) &amp; (nonzerox &lt; (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin))) # 검출한 차선의 x, y값 저장 leftx = nonzerox[left_lane_inds] lefty = nonzeroy[left_lane_inds] rightx = nonzerox[right_lane_inds] righty = nonzeroy[right_lane_inds] # 다항식 결과물, 범위 left_fitx, right_fitx, ploty = fit_poly(binary_warped.shape, leftx, lefty, rightx, righty) ## 시각화 ## # 컬러 시각화를 위한 채널 추가 out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255 window_img = np.zeros_like(out_img) # 좌우 차선 픽셀에 색상값 추가 out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0] out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255] # cv2.fillpoly() 함수에서 사용할 한줄짜리 윈도우 픽셀 좌표 left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))]) left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))]) left_line_pts = np.hstack((left_line_window1, left_line_window2)) right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))]) right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))]) right_line_pts = np.hstack((right_line_window1, right_line_window2)) # 선을 따라가는 한줄짜리 윈도우 시각화 cv2.fillPoly(window_img, np.int_([left_line_pts]), (0, 255, 0)) cv2.fillPoly(window_img, np.int_([right_line_pts]), (0, 255, 0)) result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0) # 곡선 그래프 시각화 plt.plot(left_fitx, ploty, color = 'yellow') plt.plot(right_fitx, ploty, color = 'yellow') return resultresult = search_around_poly(binary_warped)plt.imshow(result)plt.show()Input:히스토그램 구하기 예제와 같습니다.Output:3. 곡률 반경 구하기곡선 차선을 검출하여 차량 방향을 조향하기 위한 마지막 단계입니다. 이 과정은 2차 방정식으로 곡률 반경을 찾는 과정입니다. 2차 방정식으로부터 곡률 반경을 찾는 공식은 다음과 같습니다.$\\large{R_{curve}=\\frac{(1+(\\frac{dy}{dx})^2)^\\frac{3}{2}}{|\\frac{d^2y}{dx^2}|}}$$\\large{f(x)=y=Ax^2+Bx+C}$$\\large{f'(x)=y'=2Ax+B}$$\\large{f''(x)=y''=2A}$$\\large{R_{curve}=\\frac{(1+(2Ax+B)^2)^\\frac{3}{2}}{|2A|}}$이 공식을 참고하여 코드를 작성합니다.곡률 반경 구하기 예제# 슬라이드 윈도우 단계에서 구한 ploty, left_fit, right_fit를 파라미터로 사용def measure_curvature_pixels(ploty, left_fit, right_fit): # 곡률 반경을 볼 지점 선택, 이 예제에서는 이미지 제일 하단의 지점을 선택했습니다. y_eval = np.max(ploty) # 곡률 반경 공식 left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0]) right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0]) return left_curverad, right_curverad참고 사이트Udacity Self-driving car nanodegree - Advanced CV(링크 공유 불가능)"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (13) - 나만의 msg와 srv 파일(Python)",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-custom-msgsrv.html",
      "date"        : "2021-01-26 11:04:23 +0900",
      "description" : "",
      "content"     : "커스텀 메시지와 서비스 파일메시지와 서비스 파일이 무엇인지는 이전 강의들에서 설명했습니다. 이번 게시글에서는 나만의 msg와 srv 파일을 만들어 사용하는 방식을 익혀보도록 합시다. 과정은 다음과 같습니다. msg &amp; srv 전용 패키지 생성 msg &amp; srv 파일 작성 이전에 사용했던 publisher &amp; subscriber 수정 이전에 사용했던 server &amp; client 수정 빌드 &amp; 소스(적용), 실행조금 길어 보이지만, 각각 짧은 과정들이니 빠르게 해 봅시다.1. msg &amp; srv 패키지 생성msg와 srv파일을 관리하는 패키지를 만들겠습니다. ~/dev_ws/src 디렉토리에 패키지를 생성합니다.$ ros2 pkg create --build-type ament_cmake tutorial_interfaces그 다음, 패키지 폴더 안으로 이동하여 msg, srv 파일을 담을 폴더를 생성합니다.$ cd tutorial_interfaces$ mkdir msg$ mkdir srv2. msg &amp; srv 파일 작성msg 폴더로 이동하여 Num.msg 파일을 생성합니다. 파일 내용은 다음과 같습니다. 이는 64비트 정수형을 의미합니다.int64 num그 다음, srv 폴더로 이동하여 AddThreeInts.srv 파일을 생성합니다. 3개의 수를 더하는 서비스를 실행할 예정이니 request로 int64형 정수 3개와 response로 합을 의미하는 int64형 정수 하나를 데이터로 넣습니다.int64 aint64 bint64 c---int64 summsg 파일과 srv 파일을 추가했기 때문에 CMakeList.txt 파일도 수정할 필요가 있습니다. CMakeList.txt 파일에 다음 구문을 추가합니다.find_package(rosidl_default_generators REQUIRED)rosidl_generate_interfaces(${PROJECT_NAME} \"msg/Num.msg\" \"srv/AddThreeInts.srv\" )이때, 이 구문은 ament_package() 위에 추가되어야 합니다.마지막으로 package.xml 파일을 수정하여 의존성 정보를 추가해 줍니다.&lt;build_depend&gt;rosidl_default_generators&lt;/build_depend&gt;&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;msg와 srv를 위한 패키지의 작성은 여기까지입니다. 한번 빌드하고 갈지, 아니면 모든 과정이 끝난 후 colcon build 명령어로 한번에 빌드할지는 여러분의 자유입니다. 초보자 분들이라면 한번 빌드하는 것을 추천드립니다.$ cd ~/dev_ws$ colcon build --packages-select tutorial_interfaces$ . /install/setup.pyros2 interface(msg/srv) show빌드하고 소스까지 완료하셨다면, 다음 명령어를 사용하여 방금 만든 msg, srv 파일을 확인할 수 있습니다. 하지만 이 명령어는 dashing 버전과 Eloquent 이상 버전에서 사용법이 다릅니다. 먼저 메시지 파일을 확인하는 명령어는 다음과 같습니다.ROS2 Eloquent and newer:$ ros2 interface show tutorial_interfaces/msg/NumROS2 Dashing:$ ros2 msg show tutorial_interfaces/msg/Numdashing 버전에서는 ROS1의 rosmsg 명령어와 비슷합니다. 하지만 Eloquent and newer 버전에서는 조금 다릅니다. 이제 서비스 파일을 확인하는 명령어를 봅시다. 명령어는 다음과 같습니다.ROS2 Eloquent and newer:$ ros2 interface show tutorial_interfaces/srv/AddThreeIntsROS2 Dashing:$ ros2 srv show tutorial_interfaces/srv/AddThreeIntsdashing에서의 명령어는 다시 ROS1의 rossrv 명령어를 떠오르게 합니다. 하지만 Eloquent and newer 버전은 오히려 방금 봤던 메시지에서의 Eloquent and newer 버전 명령어를 떠오르게 합니다. Eloquent and newer 버전에서는 msg와 srv 파일 확인의 명령어가 통합된 것을 볼 수 있습니다.3. Publisher &amp; Subscriber 패키지 수정방금 만든 msg 파일을 사용하기 위해 지난번 게시글에서 작성했던 퍼블리셔와 서브스크라이버 패키지를 조금 수정해 줍시다.수정할 파일은 퍼블리셔, 서브스크라이버 노드들과 package.xml 파일입니다. 퍼블리셔의 수정할 부분은 다음과 같습니다.import rclpyfrom rclpy.node import Node#from std_msgs.msg import Stringfrom tutorial_interfaces.msg import Num # CHANGEclass MinimalPublisher(Node): def __init__(self): super().__init__('minimal_publisher') #self.publisher_ = self.create_publisher(String, 'topic', 10) self.publisher_ = self.create_publisher(Num, 'topic', 10) # CHANGE timer_period = 0.5 self.timer = self.create_timer(timer_period, self.timer_callback) self.i = 0 def timer_callback(self): #msg = String() msg = Num() # CHANGE #msg.data = 'Hello World: %d' % self.i msg.num = self.i # CHANGE self.publisher_.publish(msg) #self.get_logger().info('Publishing: \"%s\"' % msg.data) self.get_logger().info('Publishing: \"%d\"' % msg.num) # CHANGE self.i += 1def main(args=None): rclpy.init(args=args) minimal_publisher = MinimalPublisher() rclpy.spin(minimal_publisher) minimal_publisher.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()원래의 String 메시지 부분과 String의 내용물인 data 부분을 Num.msg에 맞게 바꿔 주었습니다. 서브스크라이버 역시 마찬가지입니다.import rclpyfrom rclpy.node import Node#from std_msgs.msg import Stringfrom tutorial_interfaces.msg import Num # CHANGEclass MinimalSubscriber(Node): def __init__(self): super().__init__('minimal_subscriber') #self.subscription = self.create_subscription( #String, #'topic', #self.listener_callback, #10) self.subscription = self.create_subscription( Num, # CHANGE 'topic', self.listener_callback, 10) self.subscription def listener_callback(self, msg): #self.get_logger().info('I heard: \"%s\"' % msg.data) self.get_logger().info('I heard: \"%d\"' % msg.num) # CHANGEdef main(args=None): rclpy.init(args=args) minimal_subscriber = MinimalSubscriber() rclpy.spin(minimal_subscriber) minimal_subscriber.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()퍼블리셔와 서브스크라이버 수정을 완료하였다면 package.xml 파일을 수정합니다. 와 사이에 다음 구문을 추가합니다.&lt;exec_depend&gt;tutorial_interfaces&lt;/exec_depend&gt;패키지를 수정했으니 빌드를 해야 하지만 역시 빌드는 자유입니다. 아까 수정한 tutorial_interfaces 패키지부터 방금 수정한 py_pubsub 패키지, 이제 수정할 py_srvcli 패키지 모두 dev_ws 작업공간 안에 있으니 결국 colcon build 명령어로 빌드할 예정입니다. 여기서 또 한번 빌드할 것이라면, 다음의 명령어를 실행합니다.$ cd ~/dev_ws$ colcon build --packages-select py_pubsub$ . /install/setup.pypy_pubsub 패키지를 빌드했을 때방금 과정에서 패키지를 빌드했다면 퍼블리셔와 서브스크라이버 노드를 한번 실행해 봅시다. 반드시 위의 tutorial_interfaces 패키지까지 빌드되어 있어야 합니다.먼저 퍼블리셔 노드를 실행해 봅시다.$ ros2 run py_pubsub talker [INFO] [minimal_publisher]: Publishing: \"0\"[INFO] [minimal_publisher]: Publishing: \"1\"[INFO] [minimal_publisher]: Publishing: \"2\"[INFO] [minimal_publisher]: Publishing: \"3\"[INFO] [minimal_publisher]: Publishing: \"4\"그 다음, 새로운 터미널을 열어 서브스크라이버 노드를 실행합니다.$ ros2 run py_pubsub listener [INFO] [minimal_subscriber]: I heard: \"4\"[INFO] [minimal_subscriber]: I heard: \"5\"[INFO] [minimal_subscriber]: I heard: \"6\"[INFO] [minimal_subscriber]: I heard: \"7\"[INFO] [minimal_subscriber]: I heard: \"8\"정상적으로 발행(publish)-구독(subscribe)하는 것을 볼 수 있습니다. 새로운 터미널을 열어 서브스크라이버 노드를 실행시키는데 조금 시간 차이가 있었기에 퍼블리셔의 초반 출력값과 서브스크라이버의 초반 출력값이 조금 다릅니다. 물론 서브스크라이버가 4,5,6,7,8 값을 구독하고 있을 때는 퍼블리셔도 4,5,6,7,8을 발행하고 있습니다.4. Service &amp; Client 패키지 수정이제 서비스를 사용하는 서비스-클라이언트 패키지를 수정해 줍시다. 지난번 게시글에서 작성했던 서비스와 클라이언트 노드를 수정합니다. 이번 역시 수정할 파일은 같습니다.먼저 서비스 노드를 수정해 줍니다.#from example_interfaces.srv import AddTwoIntsfrom tutorial_interfaces.srv import AddThreeInts # CHANGEimport rclpyfrom rclpy.node import Nodeclass MinimalService(Node): def __init__(self): super().__init__('minimal_service') #self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback) self.srv = self.create_service(AddThreeInts, 'add_three_ints', self.add_three_ints_callback) # CHANGE def add_three_ints_callback(self, request, response): #response.sum = request.a + request.b response.sum = request.a + request.b + request.c # CHANGE #self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b)) self.get_logger().info('Incoming request\\na: %d b: %d c: %d' % (request.a, request.b, request.c)) # CHANGE return responsedef main(args=None): rclpy.init(args=args) minimal_service = MinimalService() rclpy.spin(minimal_service) rclpy.shutdown()if __name__ == '__main__': main()위의 수정본과 같이 AddTwoInts.srv의 데이터를 사용한 구문을 고쳐 줍니다. 클라이언트 역시 마찬가지입니다.#from example_interfaces.srv import AddTwoIntsfrom tutorial_interfaces.srv import AddThreeInts # CHANGEimport sysimport rclpyfrom rclpy.node import Nodeclass MinimalClientAsync(Node): def __init__(self): super().__init__('minimal_client_async') #self.cli = self.create_client(AddTwoInts, 'add_two_ints') self.cli = self.create_client(AddThreeInts, 'add_three_ints') # CHANGE while not self.cli.wait_for_service(timeout_sec=1.0): self.get_logger().info('service not available, waiting again...') #self.req = AddTwoInts.Request() self.req = AddThreeInts.Request() # CHANGE def send_request(self): self.req.a = int(sys.argv[1]) self.req.b = int(sys.argv[2]) self.req.c = int(sys.argv[3]) # CHANGE(추가됨) self.future = self.cli.call_async(self.req)def main(args=None): rclpy.init(args=args) minimal_client = MinimalClientAsync() minimal_client.send_request() while rclpy.ok(): rclpy.spin_once(minimal_client) if minimal_client.future.done(): try: response = minimal_client.future.result() except Exception as e: minimal_client.get_logger().info( 'Service call failed %r' % (e,)) else: #minimal_client.get_logger().info( #'Result of add_three_ints: for %d + %d = %d' % #(minimal_client.req.a, minimal_client.req.b, response.sum)) minimal_client.get_logger().info( 'Result of add_three_ints: for %d + %d + %d = %d' % # CHANGE (minimal_client.req.a, minimal_client.req.b, minimal_client.req.c, response.sum)) # CHANGE break minimal_client.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()서비스 &amp; 클라이언트 노드 수정을 완료했다면 package.xml 파일에 의존성을 추가해 줍니다.&lt;exec_depend&gt;tutorial_interfaces&lt;/exec_depend&gt;저번 과정에서는 이제 빌드할 차례지만, 빌드하는 과정밖에 남지 않았으니 굳이 빌드하지는 않겠습니다. 다음 과정에서 한꺼번에 하겠습니다.5. 빌드 &amp; 소스(적용), 실행이제 지금까지 수정한 패키지를 빌드해 봅시다. 지금까지 하나씩 빌드한 사람도 있겠지만 사실 그냥 이 과정에서 모두 빌드해 버릴 수 있습니다.$ cd ~/dev_ws$ colcon build$ . /install/setup.py이제 tutorial_interfaces, py_pubsub, py_srvcli 패키지가 모두 빌드되었을 것입니다. 노드들을 실행해 봅시다.퍼블리셔 노드 실행결과:$ ros2 run py_pubsub talker [INFO] [minimal_publisher]: Publishing: \"0\"[INFO] [minimal_publisher]: Publishing: \"1\"[INFO] [minimal_publisher]: Publishing: \"2\"[INFO] [minimal_publisher]: Publishing: \"3\"[INFO] [minimal_publisher]: Publishing: \"4\"서브스크라이버 노드 실행결과:$ ros2 run py_pubsub listener [INFO] [minimal_subscriber]: I heard: \"0\"[INFO] [minimal_subscriber]: I heard: \"1\"[INFO] [minimal_subscriber]: I heard: \"2\"[INFO] [minimal_subscriber]: I heard: \"3\"[INFO] [minimal_subscriber]: I heard: \"4\"위의 퍼블리셔 &amp; 서브스크라이버 수정 과정에서 조금 다루었습니다.이제 서비스 &amp; 클라이언트 노드를 실행해 봅시다.서비스 노드 실행결과 (before client):$ ros2 run py_srvcli service처음 서비스 노드를 실행하면 요청(request)가 없나 멀뚱멀뚱 보고만 있습니다. 새로운 터미널을 열어 클라이언트 노드를 실행해 줍시다. 3개의 정수를 더하는 노드이니 3개의 정수형 인자를 파라미터로 줍니다.클라이언트 노드 실행결과:$ ros2 run py_srvcli client 2 3 1[INFO] [minimal_client_async]: Result of add_two_ints: for 2 + 3 + 1 = 6이때, 다시 서비스 노드의 실행결과를 보면,서비스 노드 실행결과 (after client):$ ros2 run py_srvcli service [INFO] [minimal_service]: Incoming requesta: 2 b: 3 c: 1서비스 노드가 받은 요청을 표시합니다. 클라이언트에서 2,3,1의 요청을 보내고, 서비스 노드에서 그걸 받아 계산한 후에 응답(response)으로써 계산 결과를 다시 클라이언트에 보냅니다. 필요할 때만 주고받는 비즈니스적인 관계인 셈입니다.마무리이번 게시글에서는 직접 msg 파일과 srv 파일을 만들어 보았습니다. 다음 시간에는 노드 소스 파일에서 파라미터를 사용하는 방법을 알아보겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 나만의 msg와 srv 파일 작성 편"
    } ,
  
    {
      "title"       : "[Udacity] Computer Vision (3) - Gradient and Color Spaces",
      "category"    : "",
      "tags"        : "Udacity, Computer Vision",
      "url"         : "./uda-cv-gradient.html",
      "date"        : "2021-01-24 17:45:23 +0900",
      "description" : "",
      "content"     : "이미지의 개체 식별법카메라 캘리브레이션, 왜곡 교정, 원근 변환 등으로 보정된 이미지에서 어떤 개체, 예를 들면 차선 같은 개체를 찾아내는 방법에 대해서 알아보겠습니다. 컴퓨터가 이미지에서 개체를 식별하는 방법의 키워드는 그래디언트(Gradient)입니다. 그래디언트는 경도, 기울기, 변화도 등으로 해석될 수 있습니다. 이전의 색상과 비교하여 다음 색상과의 색상값 차이(RGB)를 계산하여, 변화가 크면 변화도가 큰 것으로, 물체의 경계(edge)라고 판단합니다. 이번 게시글에서는 그래디언트 계산 방법 중 Sobel 필터에 대해서 정리합니다.Sobel Filter(Sobel Mask)소벨 필터는 위에서 언급했듯이 물체의 경계를 찾는 필터입니다. 생김새는 다음과 같습니다.$\\large{sobel_x=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\\\-2 &amp; 0 &amp; 2 \\\\-1 &amp; 0 &amp; 1\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\;\\;sobel_y=\\begin{bmatrix}-1 &amp; -2 &amp; -1 \\\\0 &amp; 0 &amp; 0 \\\\1 &amp; 2 &amp; 1\\end{bmatrix}}$$sobel_x$는 수직 성분 검출, $sobel_y$는 수평 성분 검출 필터입니다. 이는 에지 검출의 대표적인 1차 미분 연산자인데, 미분 연산자라는 말이 조금 이해하기 힘들 수 있습니다. 그럼 먼저 미분 연산자에 대해서 알아봅시다.1차 미분 연산자미분 연산자라고 불리는 이유는 소벨 필터는 변화도를 이용하기 때문입니다. 1차 미분 연산자란 다음과 같습니다.$\\large{\\begin{align*}\\frac{\\partial f}{\\partial x}&amp;=\\frac{f(x+1)-f(x)}{x+1-x}\\\\&amp;=f(x+1)-f(x)\\\\&amp;=\\begin{bmatrix}-1 &amp; 1 \\end{bmatrix}\\;\\begin{bmatrix}f(x+1) \\\\f(x)\\end{bmatrix}\\end{align*}}$즉, 미분 연산자만 따로 보면 다음과 같습니다.$\\large{\\frac{\\partial }{\\partial x}\\times f=\\begin{bmatrix}-1 &amp; 1 \\end{bmatrix}\\;\\begin{bmatrix}f(x+1) \\\\f(x)\\end{bmatrix}}$$\\large{\\frac{\\partial }{\\partial x}=\\begin{bmatrix}-1 &amp; 1 \\end{bmatrix}}$그렇다면 $x+1$부터 $x-1$의 변화량을 봅시다. 어차피 기울기의 크기를 보는거라 분모는 별로 중요하지 않기 때문에 분자만을 나타내면 다음과 같습니다.$\\large{\\begin{align*}\\frac{\\partial f}{\\partial x}&amp;=f(x+1)-f(x-1)\\\\&amp;=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\end{bmatrix}\\;\\begin{bmatrix}f(x+1) \\\\f(x) \\\\f(x-1)\\end{bmatrix}\\end{align*}}$$\\large{\\frac{\\partial }{\\partial x}=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\end{bmatrix}}$1$\\times$3 마스크는 다음과 같습니다. 소벨 필터는 보통 차원이 홀수인 n$\\times$n 정방행렬로 되어 있는데, 이는 *검출할 라인을 제외하고 미분 연산자를 곱해주기 때문입니다. 위에서 언급한 소벨 필터의 생김새 역시 3$\\times$3 행렬입니다.소벨 마스크이런 식으로 1$\\times$3 1차 미분 연산자 3개를 3$\\times$3 행렬로 만든 것을 Prewitt Masks라고 합니다. 소벨 마스크가 되기 전 단계입니다. 생김새는 다음과 같습니다.$\\large{sobel_x=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1 \\\\-1 &amp; 0 &amp; 1\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\;\\;sobel_y=\\begin{bmatrix}-1 &amp; -1 &amp; -1 \\\\0 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 1\\end{bmatrix}}$깔끔하게 3개의 1차 미분 연산자로 만들어져 있습니다. 이 프리윗 마스크를 조금 수정하여 중심화소에 조금 가중치를 둔 것이 바로 Sobel Mask입니다. 중심화소에 가중치를 크게 함으로서 대각선 방향에서의 에지도 잘 검출합니다.$\\large{수직\\;방향\\;검출:\\;sobel_x=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\\\-2 &amp; 0 &amp; 2 \\\\-1 &amp; 0 &amp; 1\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\;\\;수평\\;방향\\;검출:\\;sobel_y=\\begin{bmatrix}-1 &amp; -2 &amp; -1 \\\\0 &amp; 0 &amp; 0 \\\\1 &amp; 2 &amp; 1\\end{bmatrix}}$$\\large{대각선\\;방향\\;검출:\\;sobel_d=\\begin{bmatrix}0 &amp; -1 &amp; -2 \\\\1 &amp; 0 &amp; -1 \\\\2 &amp; 1 &amp; 0\\end{bmatrix}}$소벨 필터로 Gradient 구하기이제 소벨 필터로 그래디언트를 구해 봅시다. 그래디언트를 구하는 방법은 간단합니다. 먼저 3$\\times$3 이미지 픽셀에 소벨 마스크의 각각의 요소들을 곱합니다. 그리고 결과의 모든 요소의 합이 바로 그래디언트입니다. 식으로 나타내면 다음과 같습니다.$\\large{gradient=\\sum (region\\times Sobel\\;mask)}$이 그래디언트 값은 3$\\times$3 이미지 픽셀의 중심에서의 그래디언트 값으로 취급합니다. 이제 gradient를 구했으니, 예제를 한번 해 보도록 합시다.Sobel Filter 예제소벨 필터를 사용하는 예제는 다음과 같습니다.import cv2import numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimg = mpimg.imread('/home/이미지 경로/sobel_ex.png')gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# Sobel(이미지, 이미지 비트 수, x축, y축)sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)# Gradient를 모두 양수로 변환, 절댓값을 본다.abs_sobelx = np.absolute(sobelx)abs_sobely = np.absolute(sobely)# 8비트로 변환하는 과정.scaled_sobelx = np.uint8(255*abs_sobelx/np.max(abs_sobelx))scaled_sobely = np.uint8(255*abs_sobely/np.max(abs_sobely))# thresh 범위 내의 픽셀만 검출 (단위: 색상값(0~255))thresh_min = 20thresh_max = 100sxbinary = np.zeros_like(scaled_sobelx)sxbinary[(scaled_sobelx &gt;= thresh_min) &amp; (scaled_sobelx &lt;= thresh_max)] = 1sybinary = np.zeros_like(scaled_sobely)sybinary[(scaled_sobely &gt;= thresh_min) &amp; (scaled_sobely &lt;= thresh_max)] = 1plt.imshow(sxbinary, cmap='gray')#plt.imshow(sybinary, cmap='gray')plt.show()Input:Output:$sobel_x$의 결과$sobel_y$의 결과결과를 보면 $sobel_x$에서는 주로 수직선이, $sobel_y$에서는 주로 수평선이 검출되는 것을 볼 수 있습니다.왜인진 모르겠지만 Sobel함수의 결과물을 cv2.imshow로 실행하면 이미지가 안보이고 plt.imshow 함수로 실행하면 이미지가 보입니다. 어떤 이유 때문인지 잘 모르겠네요.Sobel Filter를 사용한 다양한 이미지 검출 방법소벨 필터를 이용하여 이미지를 검출하는 방법은 총 4가지입니다. 둘은 앞서 설명한 $x$축 검출과 $y$축 검출입니다. $x$축 검출과 $y$축 검출을 그대로 사용하기도 하고 이 둘을 이용하여 검출된 다른 이미지를 사용하기도 합니다.Magnitude of Gradient$x$축, $y$축 검출 이외의 세 번째 차선 검출 방법은 그래디언트의 크기 검출입니다. 이 두 검출값을 조합하여 더 좋은 값을 찾는 과정입니다. 그 방식은 다음과 같습니다.$\\large{abs\\_sobelx = \\sqrt{(sobel_x)^2}}$$\\large{abs\\_sobely = \\sqrt{(sobel_y)^2}}$$\\large{abs\\_sobelxy = \\sqrt{(sobel_x)^2+(sobel_y)^2}}$$sobel_x$와 $sobel_y$의 제곱의 합의 제곱근입니다. 이 방식으로 $sobel_x$와 $sobel_y$의 값이 모두 반영된 값을 찾을 수 있습니다.Magnitude of Gradient 예제import cv2import numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimg = cv2.imread('/home/이미지 경로/sobel_ex.png')gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# Sobel(이미지, 이미지 비트 수, x축, y축)sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)# Magnitude of Gradientgradmag = np.sqrt(sobelx**2 + sobely**2)# 8비트로 변환하는 과정.scale_factor = np.max(gradmag) / 255gradmag = (gradmag/scale_factor).astype(np.uint8)# thresh 범위 내의 픽셀만 검출 (단위: Gradient, 단위없음)thresh_min = 20thresh_max = 100binary = np.zeros_like(gradmag)binary[(gradmag &gt;= thresh_min) &amp; (gradmag &lt;= thresh_max)] = 1plt.imshow(binary, cmap='gray')plt.show()Input:Sobel Filter 예제의 입력값과 같습니다.Output:Direction of the Gradient마지막으로, 그래디언트의 방향(각도)를 통해 차선을 검출할 수 있습니다. 차선은 항상 일직선이라고 가정했을 때, 자동차가 촬영하는 이미지의 수평선 즉 이미지의 $x$축과 이루는 각도가 일정합니다. 이 점을 이용하여 $sobel_x$와 $sobel_y$가 이루는 각도를 계산합니다.$\\large{\\theta = arctan(\\frac{sobel_y}{sobel_x})}$Direction of the Gradient 예제import cv2import numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimg = cv2.imread('/home/이미지 경로/sobel_ex.png')gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# Sobel(이미지, 이미지 비트 수, x축, y축)sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=15)sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=15)# Gradient 방향의 절댓값 (0~pi/2)absgraddir = np.arctan2(np.absolute(sobely),np.absolute(sobelx))# thresh 범위 내의 픽셀만 검출 (단위: radian)thresh_min = 0.7thresh_max = 1.3binary = np.zeros_like(absgraddir)binary[(absgraddir &gt;= thresh_min) &amp; (absgraddir &lt;= thresh_max)] = 1plt.imshow(binary, cmap='gray')plt.show()Input:Sobel Filter 예제의 입력값과 같습니다.Output:노이즈가 많지만 차선은 검출해 내고 있습니다.Color Spaces차선을 검출하는 다른 방법은 색공간을 이용하는 것입니다. 우리는 주로 Grayscale을 통해 채널 줄이기 &amp; 흰색, 노란색 검출을 하였지만, 노란색 차선은 종종 Grayscale을 하면 사라져버리는 경우도 있습니다. 이 점을 보완하기 위해 우리는 다른 색공간을 이용합니다. 다음 그림은 이미지를 각각 RGB의 3채널로 분리한 것입니다.검출 결과를 보면 R채널과 G채널은 노란색 차선을 잘 검출합니다. 하지만 노란색에 B(Blue)성분이 없기 때문에 B채널에서는 잘 검출이 안되는 모습을 볼 수 있습니다. 하지만 R, G채널도 너무 밝은 부분에서는 노란색 선이 잘 검출되지 않습니다. 이 점을 보완하기 위해 우리는 HSV/HLS 색공간에 대해서 알아보겠습니다.HSV Color spaces이번 강의에서는 HSV 색공간을 다루지 않습니다. 개념만 간단하게 설명하고 넘어가겠습니다.HSV 색공간에서의 H, S, V는 각각 다음과 같습니다. H(Hue): 색상, 원색을 나타냅니다. 한 색을 딱 정하고, $x$, $y$축을 S, V값으로 조정하는 방식입니다. S(Saturation): 채도, 가장 진한 상태를 100%로 나타내는 진함의 정도를 말합니다. 낮을수록 원색이 옅어집니다. V(Value): 명도, 색의 밝은 정도를 나타냅니다. 이 값이 낮을수록 검은색에, 높을수록 원색에 가깝습니다.HLS Color spaces이 게시글에서 다루는 색공간은 HLS 색공간입니다. H, S: HSV 색공간의 H, S와 동일합니다. L(Lightness): 밝기, 높은 값으로 갈수록 흰색에 가까운 색입니다.이 색공간을 이용하여 채널을 분리해 봅시다.분리 결과 S 채널의 이미지의 차선이 가장 선명하게 드러납니다. 이제 우리는 S 채널 이미지를 사용할 것입니다.Color Threshold검출한 이미지를 이진적으로 처리하는 과정입니다. 위의 과정들에서도 많이 했지만, S 채널의 threshold 결과물을 한번 보도록 하겠습니다.차선이 훨씬 선명하게 검출되었습니다. threshold 값을 지정하는 구문은 다음과 같습니다. 예시는 S 채널 이미지입니다.thresh = (90, 255)binary = np.zeros_like(S)binary[(S &gt; thresh[0]) &amp; (S &lt;= thresh[1])] = 1이와 같은 방식으로 차선을 검출합니다.HLS Color spaces 예제사실 단지 해당 채널을 검출하는 예제이므로 복잡하지는 않습니다.import cv2import numpy as npimg = cv2.imread('/home/이미지 경로/sobel_ex3.png')hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)# H, L, S 채널 분리H = hls[:, :, 0]L = hls[:, :, 1]S = hls[:, :, 2]cv2.imshow('Result', S)cv2.waitKey()Input:Output:강의에서 본 결과와는 조금 다릅니다… 하지만 HLS 중에서 가장 괜찮은 결과입니다. 이제 threshold를 도입하여 binary 이미지를 만들어 보겠습니다.import cv2import numpy as npimport matplotlib.pyplot as pltimg = cv2.imread('/home/이미지 경로/sobel_ex3.png')hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)# H, L, S 채널 분리H = hls[:, :, 0]L = hls[:, :, 1]S = hls[:, :, 2]# threshold 설정, binary 이미지 추출thresh = (90, 255)binary = np.zeros_like(S)binary[(S &gt; thresh[0]) &amp; (S &lt;= thresh[1])] = 1plt.imshow(binary, cmap='gray')plt.show()Input:HLS 채널 분리 예제와 같습니다.Output:HLS 채널 분리 예제의 결과는 조금 의아했지만, 결국 binary 처리를 하고 나니 좋은 결과가 나왔습니다.Combine Color spaces and Gradient마지막으로 색공간으로 검출한 이미지와 소벨 필터로 검출한 이미지를 합치는 과정입니다.두 이미지는 다음 구문과 같은 방법으로 결합할 수 있습니다.combined_binary = np.zeros_like(sxbinary)combined_binary[(s_binary == 1) | (sxbinary == 1)] = 1질문 소벨 필터는 마스크+$\\sum$하는 과정을 모두 합쳐서고, 소벨 마스크는 3$\\times$3 행렬만을 말하는 건가요? Direction of the Gradient를 통해 차선 검출 방법에서 커브를 트는 차선도 검출할 수 있나요? $sobel_x$의 결과가 가장 좋다고 하던데, 다 똑같아보여서 잘 모르겠습니다… 왜 소벨 필터의 결과는 plt.imshow함수로만 제대로 보이나요? cv2.imshow 함수는 검은색 화면만 보이는 이유 8비트 변환은 왜 하나요? (그림은 8비트 변환을 안했을때의 결과) Sobel 함수의 결과물은 그래디언트값, 그래디언트 방향은 어떤 각도를 말하는 건가요? HLS 공간을 쓰는 이유는 무엇인가요? RGB는 잘 안보여서 그런거라 하던데 HSV는 안되나요? 굳이 HLS 색공간을 골라 쓰는 이유가 뭔가요?참고 사이트Udacity Self-driving car nanodegree - Gradient and Color Spaces(링크 공유 불가능)Programming 블로그 - 1차 미분 마스크, 2차 미분 마스크"
    } ,
  
    {
      "title"       : "[Udacity] Computer Vision (2) - Camera Calibration",
      "category"    : "",
      "tags"        : "Udacity, Computer Vision",
      "url"         : "./uda-cv-cam-cali.html",
      "date"        : "2021-01-23 15:25:23 +0900",
      "description" : "",
      "content"     : "제대로 된 이미지로 보정하기컴퓨터 비전에서 사용하는 센서 중 가장 중요한 것은 단연코 카메라입니다. 이미지 처리가 주된 내용인 만큼 카메라가 감지하는 정보는 높은 정확도를 요구합니다. 하지만 공장에서 만들어지기만 한 카메라를 바로 사용하기에는 문제가 있습니다. 내부 파라미터 조정이나 렌즈에 의한 왜곡 등이 아직 보정되지 않았기에 왜곡된 이미지가 출력될 수 있습니다. 이러한 상태의 카메라의 이미지를 보정하는 과정은 다음과 같습니다. 카메라 캘리브레이션(Camera Calibration) 왜곡 교정 (Distortion Correction) 원근 변환 (Perspective Transform)사실 원근 변환은 이미지를 보정하는 과정보다는 가공하는 과정에 가깝지만 이번 강의에서 소개해주는 김에 다루도록 하겠습니다.카메라 캘리브레이션 (Camera Calibration)카메라 캘리브레이션은 간단히 말하면 3차원의 형상을 2차원으로 온전히 옮길 수 있도록 도와주는 파라미터들을 도출하는 과정입니다.카메라 영상은 3차원 형상을 2차원으로 옮길 때 3차원 공간상의 점들을 외부 파라미터(extrinsic parameter)와 내부 파라미터(intrinsic parameter)에 곱하여 나타냅니다. 다음의 공식은 3차원 월드 좌표계 $(X, Y, Z)$의 점을 2차원 평면 좌표계 $s(x, y)$로 변환하는 공식입니다.$\\large{s \\begin{bmatrix}x\\\\ y\\\\ 1\\end{bmatrix}=\\begin{bmatrix}f_x &amp; skew\\_cf_x &amp; c_x\\\\ 0 &amp; f_y &amp; c_y\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1\\\\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2\\\\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z\\\\1\\end{bmatrix}=A[R|t]\\begin{bmatrix}X\\\\ Y\\\\ Z\\\\1\\end{bmatrix}}$이 공식을 살펴보면 우선 월드 좌표계의 점 $(X, Y, Z)$를 회전 변환하여 카메라 좌표계에서의 점으로 변환시킵니다. 그 후 카메라 내부 파라미터를 곱하여 2차원의 점으로 변환합니다. 이때 변환된 좌표계를 이미지 좌표계라고 하며, 공식에서 $s(x, y, 1)$에 해당합니다. 좌표계들을 그림으로 표현하면 다음과 같습니다.하지만 현재 우리가 공장에서 막 나온 카메라를 가지고 있다고 하면, 위의 공식에서 모르는 요소가 두 개 있습니다. 바로 외부 파라미터와 내부 파라미터입니다.1. 외부 파라미터외부 파라미터는 월드 좌표계와 카메라 좌표계 사이의 관계입니다. 쉽게 말하면 물리적인 관계입니다. 카메라가 월드 좌표계로부터 얼마만큼 병진이동했는지($[R|t]$ 중 $t$, $\\begin{bmatrix}t_1\\ t_2\\ t_3\\end{bmatrix}$), 얼마만큼 회전이동했는지($[R|t]$ 중 $R$, $\\begin{bmatrix}r_{11} &amp; r_{12} &amp; r_{13} \\ r_{21} &amp; r_{22} &amp; r_{23} \\ r_{31} &amp; r_{32} &amp; r_{33} \\end{bmatrix}$)를 나타냅니다. 사실 이 부분은 카메라 캘리브레이션의 결과값에 도출되는 부분은 아닙니다.2. 내부 파라미터내부 파라미터는 카메라 좌표계와 이미지 좌표계 사이의 관계입니다. 내부 파라미터의 구성요소는 크게 초점거리 $(f_x, f_y)$, 주점 $(c_x, c_y)$, 그리고 비대칭 계수 $skew_c$로 이루어져 있습니다. 간단히만 설명하겠습니다. 자세한 내용은 다음 블로그를 참고해 주시기 바랍니다.핀홀 카메라내부 파라미터의 구성요소를 설명하기에 앞서 내부 파라미터에 사용되는 이상적인 카메라 모델인 핀홀 카메라를 소개하겠습니다. 핀홀 카메라는 바늘구멍이라는 초점이 있는 카메라입니다. 이 게시글은 핀홀 카메라를 기준으로 설명됩니다.1. 초점 거리(focal length) $(f_x, f_y)$초점거리는 센서로부터 렌즈 중심까지의 거리를 의미합니다. 단위는 픽셀이며 이미지 센서의 셀의 크기에 종속된 값입니다. 만약 셀의 크기가 0.1mm이고 초점 거리가 500픽셀이라면, mm단위의 초점거리는 50mm입니다.2. 주점(principle point) $(c_x, c_y)$주점은 카메라 렌즈의 중심으로부터 이미지 센서에 내린 수선의 발을 말합니다. 단위는 픽셀로 역시 셀의 크기에 종속되어 있습니다. 영상 중심점과 같은 값을 보이기도 하지만 카메라 조립과정 중 오차로 렌즈와 이미지 센서의 수평이 어긋나면 다른 값이 나옵니다.3. 비대칭 계수(skew coefficient) $skew_c$비대칭 계수는 이미지 센서의 셀 array의 y축이 기울어진 정도를 나타냅니다. 표현은 $skew_c=tan(\\alpha)$로 나타냅니다. 요즘 카메라들은 skew 에러가 거의 없기 때문에 비대칭 계수는 잘 고려하지 않는다고 합니다.결과적으로 이렇게 사용그림은 카메라 좌표계의 $Z_C$축을 가로축으로 나타내어 사용법을 설명합니다. 카메라 좌표계 위의 한 점 $(X_C, Y_C, Z_C)$을 가지고 이미지 좌표계 위에 나타내는 방법을 보겠습니다.초점으로부터 광학축 $Z_C$를 따라 거리가 1인 평면을 정규 이미지 평면(Normalized Image Plane)이라고 합니다. 이것은 이미지 평면을 정규화시킨 것으로 실제로 존재하는 평면은 아닙니다. 카메라 좌표계의 점을 정규 이미지 평면으로 투영(projection)시켜 정규 이미지 평면상의 좌표로 변환합니다. 이 평면은 $Z_C=1$이란 특징을 갖고 있으므로 $(X_C, Y_C, Z_C)$를 $Z_C$로 나누어 $Z_C$ 좌표를 1로 만들어 줍니다. 따라서 점 $(X_C, Y_C, Z_C)$를 정규 이미지 평면에 나타낸 점은 $(\\frac{X_C}{Z_C},\\frac{Y_C}{Z_C},1)$입니다. 그림에서의 $(u, v)$에 해당하는 점입니다.이제 다시 이 점을 이미지 평면으로 올려 줍시다. 이미지 평면의 특징은 $Z_C=f$라는 것입니다. 따라서 정규화되었던 $Z_C$에 초점거리 $f$만 곱해 준다면 이미지 평면에 점을 올려줄 수 있습니다. 위의 공식에서 $s$란 바로 초점거리 $f$를 말하는 것입니다. 따라서 $Z_C=f$일 때 이미지 평면상의 점 좌표는 $(f_x\\frac{X_C}{Z_C}, f_y\\frac{Y_C}{Z_C}, f)$입니다. 이미지 평면상의 좌표기는 하지만 아직 그림에서의 $(x,y)$ 점은 아닙니다.마지막으로 이미지에서 픽셀좌표는 이미지의 중심이 아닌 좌상단 모서리를 원점으로 하기 때문에 렌즈 중심에서 셀에 내린 수선의 발인 주점을 좌표에 더하여 줍니다. 따라서 최종적인 좌표는 $(f_x\\frac{X_C}{Z_C}+C_x, f_y\\frac{Y_C}{Z_C}+C_y, f)$이 됩니다. 물론 이 좌표는 카메라 좌표계적 표현이고, 이미지 평면의 점은 $(f_x\\frac{X_C}{Z_C}+C_x, f_y\\frac{Y_C}{Z_C}+C_y)$입니다. 그림에서의 $(x,y)$에 해당하는 점입니다. 식을 정리하면 다음과 같습니다.$\\large{1.\\;\\;(X_C, Y_C, Z_C)\\;\\;\\xrightarrow[정규화]{}\\;\\;(\\frac{X_C}{Z_C},\\frac{Y_C}{Z_C},1)\\;\\;\\;\\;\\;\\;(u,v)=(\\frac{X_C}{Z_C},\\frac{Y_C}{Z_C})}$$\\large{2.\\;\\;(\\frac{X_C}{Z_C},\\frac{Y_C}{Z_C},1)\\;\\;\\xrightarrow[이미지\\;평면화]{}\\;\\;(f\\frac{X_C}{Z_C},f\\frac{Y_C}{Z_C},f)\\;\\;\\;\\;\\;\\;(x',y')=(f\\frac{X_C}{Z_C},f\\frac{Y_C}{Z_C})}$$\\large{3.\\;\\;(f\\frac{X_C}{Z_C},f\\frac{Y_C}{Z_C},f)\\;\\;\\xrightarrow[원점을\\;주점으로]{}\\;\\;(f\\frac{X_C}{Z_C}+C_x,f\\frac{Y_C}{Z_C}+C_y,f)}$$\\large{(x,y)=(f\\frac{X_C}{Z_C}+C_x,f\\frac{Y_C}{Z_C}+C_y)}$정규화의 의미규칙이 없는 데이터에서 어떤 조건을 부여함로서 규칙을 따르는 데이터로 만드는 것을 의미합니다.카메라 캘리브레이션 예제왜곡 교정 (Distortion Correction)이미지 보정의 다음 과정은 왜곡 교정입니다. 왜곡이란 카메라가 완벽한 이미지를 생성하지 못해 이미지가 휘어져 보이는 것인데, 차선 인식을 목표로 하는 우리에겐 치명적인 결함입니다. 따라서 이미지의 일부, 특히 가장자리 근처에 있는 요소가 늘어나거나 기울어질 수 있어 이를 수정해 주어야 합니다. 이미지 왜곡 해제는 차선 인식 뿐만 아니라 다른 이미지 처리에서도 가장 기본적인 단계입니다.왜곡의 종류에는 두 가지가 있습니다. 방사 왜곡 (Radial Distortion) 접선 왜곡 (Tangential Distortion)1. 방사 왜곡 (Radial Distortion)방사 왜곡은 볼록렌즈의 굴절률에 의한 것으로 그림과 같이 왜곡 정도가 중심에서 이미지의 가장자리로 갈수록 왜곡이 심해지는 형태입니다.방사 왜곡의 수학적 모델은 다음과 같습니다.$\\large{x_{corrected}=x(1+k_1r^2+k_2r^4+k_3r^6)}$$\\large{y_{corrected}=y(1+k_1r^2+k_2r^4+k_3r^6)}$2. 접선 왜곡 (Tangential Distortion)접선 왜곡은 카메라 제조 과정에서 렌즈와 이미지 센서의 수평이 맞지 않거나 렌즈 자체의 centering이 맞지 않아서 발생합니다.접선 왜곡의 수학적 모델은 다음과 같습니다.$\\large{x_{corrected}=x+[2p_1xy+p_2(r^2+2x^2)]}$$\\large{y_{corrected}=y+[p_1(r^2+2y^2)+2p_2xy]}$따라서 왜곡 교정을 위해서는 총 5개의 파라미터가 필요합니다. 이 파라미터를 왜곡 계수라고 합니다.$\\large{Distortion coefficient = (k_1\\;k_2\\;p_1\\;p_2\\;k_3) }$이 왜곡 계수는 카메라 캘리브레이션의 결과물로 도출됩니다.왜곡이 반영된 수학적 모델렌즈계 왜곡이 없다고 할 때, 3차원 공간상의 한 점을 정규 이미지 평면에 나타내면 다음과 같습니다.(첨자 $n: normalized, u:undistorted$)$\\large{\\begin{bmatrix}x_{n\\_u} \\\\y_{n\\_u}\\end{bmatrix}=\\begin{bmatrix}\\frac{X_C}{Z_C} \\\\\\frac{Y_C}{Z_C}\\end{bmatrix}}$하지만 실제로 카메라로 찍은 영상은 왜곡이 발생합니다. 왜곡의 수학적 모델을 반영된 모델은 다음과 같습니다.(첨자 $d:distorted$)$\\large{\\begin{bmatrix}x_{n\\_d} \\\\y_{n\\_d}\\end{bmatrix}=(1+k_1r^2+k_2r^4+k_3r^6)\\begin{bmatrix}x_{n\\_u} \\\\y_{n\\_u}\\end{bmatrix}+\\begin{bmatrix}2p_1xy+p_2(r^2+2x^2) \\\\p_1(r^2+2y^2)+2p_2xy\\end{bmatrix}}$단,$\\large{r_u^2=x_{n\\_u}^2+y_{n\\_u}^2}$위 수식에서 우변의 첫번째 항은 방사 왜곡, 두번째 항은 접선 왜곡을 나타냅니다. 이 공식에서 방사 왜곡과 접선 왜곡의 수학적 모델은 왜곡을 보정하는 것이 아닌 왜곡을 반영하는 모델이라는 것을 알 수 있습니다. $r_u$는 왜곡이 없을 때의 중심, 즉 주점까지의 거리(반지름)입니다. 결국, 왜곡된 영상의 실제 영상 좌표 $(x_{p_u}, y_{p_u})$는 카메라 내부 파라미터를 반영하여 다음과 같이 구해집니다.$\\large{\\begin{bmatrix}x_{p\\_d} \\\\y_{p\\_d} \\\\1\\end{bmatrix}=\\begin{bmatrix}f_x &amp; skew\\_cf_x &amp; c_x\\\\ 0 &amp; f_y &amp; c_y\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}x_{n\\_d} \\\\y_{n\\_d} \\\\1\\end{bmatrix}}$즉,$\\large{x_{p\\_d}=f_x(x_{n\\_d}+skew\\_cy_{n\\_d})+c_x}$$\\large{y_{p\\_d}=f_yy_{n\\_d}+c_y}$왜곡의 교정 단계위의 단계에서 왜곡된 이미지의 모델을 살펴보았습니다. 이제 왜곡된 이미지로부터 교정된 이미지를 구하는 법을 살펴보겠습니다. 전체적인 방법은 교정 영상에서의 이미지를 역순의 변환을 통해 픽셀 좌표값을 왜곡된 이미지에 대응되는 픽셀 좌표에 채우는 것입니다. 다음 그림과 같습니다.왜곡되지 않은 이미지의 모델 역시 위의 공식을 따릅니다. 왜곡되지 않은 경우 공식은 다음과 같습니다.$\\large{\\begin{bmatrix}x_{p\\_u} \\\\y_{p\\_u} \\\\1\\end{bmatrix}=\\begin{bmatrix}f_x &amp; skew\\_cf_x &amp; c_x\\\\ 0 &amp; f_y &amp; c_y\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}x_{n\\_u} \\\\y_{n\\_u} \\\\1\\end{bmatrix}}$우리가 *이미 교정된 이미지 $\\begin{bmatrix}x_{p_u} y_{p_u} 1\\end{bmatrix}$를 알고 있다고 할 때, 이 공식에 역행렬을 취하여 정규 이미지 평면의 좌표 $\\begin{bmatrix}x_{n_u} y_{n_u} 1\\end{bmatrix}$로 변환합니다.$\\large{\\begin{bmatrix}x_{n\\_u} \\\\y_{n\\_u} \\\\1\\end{bmatrix}=\\begin{bmatrix}f_x &amp; skew\\_cf_x &amp; c_x\\\\ 0 &amp; f_y &amp; c_y\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}^{-1}\\begin{bmatrix}x_{p\\_u} \\\\y_{p\\_u} \\\\1\\end{bmatrix}}$즉,$\\large{x_{n\\_d}=\\frac{x_{p_u}-c_x}{f_x}-skew\\_cy_{n\\_u}}$$\\large{y_{n\\_d}=\\frac{y_{p_u}-c_y}{f_y}}$그 다음, $r_u^2=x_{n_u}^2+y_{n_u}^2$ 공식으로 $r_u$를 구하고 왜곡 모델을 적용하여 왜곡된 좌표 $(x_{n_d}, y_{n_d})$를 구합니다.$\\large{\\begin{bmatrix}x_{n\\_d} \\\\y_{n\\_d}\\end{bmatrix}=(1+k_1r^2+k_2r^4+k_3r^6)\\begin{bmatrix}x_{n\\_u} \\\\y_{n\\_u}\\end{bmatrix}+\\begin{bmatrix}2p_1xy+p_2(r^2+2x^2) \\\\p_1(r^2+2y^2)+2p_2xy\\end{bmatrix}}$마지막으로 $(x_{n_d}, y_{n_d})$를 다시 픽셀 좌표계로 변환하면 $(x_{n_u}, y_{n_u})$의 왜곡된 영상에서의 좌표 $(x_{p_d}, y_{p_d})$를 구할 수 있습니다.$\\large{\\begin{bmatrix}x_{p\\_d} \\\\y_{p\\_d} \\\\1\\end{bmatrix}=\\begin{bmatrix}f_x &amp; skew\\_cf_x &amp; c_x\\\\ 0 &amp; f_y &amp; c_y\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}x_{n\\_d} \\\\y_{n\\_d} \\\\1\\end{bmatrix}}$왜곡 교정 예제OpenCV에서는 사용자가 이런 복잡한 과정을 거칠 필요 없이 undistort라는 왜곡 교정 함수가 존재합니다. 이 함수의 파라미터에 대해서 살펴보겠습니다.cv2.undistort(img, mtx, dist, None, mtx) img: 왜곡이 있는 이미지 mtx: 카메라 내부 파라미터 dist: 왜곡 계수위의 이론에서 사용한 요소들을 함수의 파라미터에 input합니다.저는 예제를 이용했기 때문에 카메라 캘리브레이션의 결과값이 담긴 pickle 파일을 사용하여 불러왔습니다.import pickleimport cv2import numpy as npdist_pickle = pickle.load( open( \"/home/pickle 파일 경로/wide_dist_pickle.p\", \"rb\" ) )mtx = dist_pickle[\"mtx\"]dist = dist_pickle[\"dist\"]img = cv2.imread('/home/이미지 파일 경로/test_image2.png')nx = 8 ny = 6 # 왜곡 보정 함수 실행undist = cv2.undistort(img, mtx, dist, None, mtx)cv2.imshow('undist', undist)cv2.waitKey()Input:Output:왜곡이 잘 보정된 것을 볼 수 있습니다.원근 변환 (Perspective Transform)다른 각도나 방향에서 효과적으로 물체를 볼 수 있도록 이미지를 변형하는 기법입니다. 차선 인식에서 좌회전/우회전 시 방향을 틀 곳을 알아야 합니다. 컴퓨터 비전을 공부하는 우리가 알아야 할 것은 차선이 휘는 정도, 즉 차선의 곡률입니다. 하지만 우리는 카메라가 찍은 사진만으로는 차선이 휘었는지 직선인지 구분하기 어렵습니다. 다음 그림을 보면, 왼쪽 차선은 곡률이 있지만, 오른쪽 차선은 직선처럼 보입니다. 하지만 원근 변환을 통해 위에서 내려다 본 각도로 변환시키면, 곡률이 확실하게 드러납니다. 게다가 위에서 내려다보면 내 차의 현재 위치를 지도와 바로 매치시킬 수 있기 때문에 더 유용합니다. 이렇게 위에서 내려다본 형태로 사진을 가공하는 것을 조감도(Bird’s Eye View)라고 합니다.이 조감도 모델을 만드는 원리는 네 점을 지정한 후, 그 점을 변환하여 위에서 내려다본 이미지를 생성하는 것입니다. 이러한 과정은 OpenCV의 getPerspectiveTransform 함수 내부에서 일어납니다.차선의 곡률을 계산하는 방법 중 하나는 차선에 2차 다항식을 맞추는 것입니다. 그림과 같은 방법으로 말입니다.하지만 이번 강의에서는 곡률은 구하지 않습니다. 다음 강의에서 구해보도록 합시다.원근 변환 예제OpenCV에서는 getPerspectiveTransform 함수를 통해 원근 변환 행렬을 구합니다.import pickleimport cv2import numpy as npimport matplotlib.pyplot as pltimport matplotlib.image as mpimgdist_pickle = pickle.load( open( \"/home/pickle 파일 경로/wide_dist_pickle.p\", \"rb\" ) )mtx = dist_pickle[\"mtx\"]dist = dist_pickle[\"dist\"]img = cv2.imread('/home/이미지 파일 경로/test_image2.png')nx = 8 # the number of inside corners in xny = 6 # the number of inside corners in ydef corners_unwarp(img, nx, ny, mtx, dist): undist = cv2.undistort(img, mtx, dist, None, mtx) gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY) ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None) if ret == True: cv2.drawChessboardCorners(undist, (nx, ny), corners, ret) # corner 검출 결과를 더 잘 보기 위한 offset 설정, 상하좌우 100씩 여유를 줌. offset = 100 # offset for dst points # Grab the image shape img_size = (gray.shape[1], gray.shape[0]) # src = 체스보드에서 검출된 corners의 네 끝점 src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]]) # destination point 목표 지점의 배열. dst = np.float32([[offset, offset], [img_size[0]-offset, offset], [img_size[0]-offset, img_size[1]-offset], [offset, img_size[1]-offset]]) # 원근 변환 행렬 도출 M = cv2.getPerspectiveTransform(src, dst) # 변환 행렬에 따라 이미지를 변환 warped = cv2.warpPerspective(undist, M, img_size) return warpedtop_down = corners_unwarp(img, nx, ny, mtx, dist)cv2.imshow('Undistorted and Warped Image', top_down)cv2.waitKey()Input:왜곡 교정 예제의 입력과 같습니다.Output:질문Q1. 내부 파라미터의 구성요소가 무엇인진 알겠는데, 어떻게 구하는지를 모르겠습니다.Q2. 비대칭 계수는 왜 이미지 좌표계 변환의 x에밖에 없나요?Q3. 셀의 크기는 셀의 가로인가요 세로인가요?A3. 초점거리의 $x$축, $y$축 성분에 따라 각각 가로 크기, 세로 크기로 계산합니다.Q4. 실제 렌즈에서 초점이란?Q5. 접선왜곡을 일부러 발생시키는 경우도 있다고 하는데 어디쓸려고 하는건가요?A5. 왜곡을 일부러Q6. *교정된 이미지를 만들려고 왜곡된 이미지와 픽셀 좌표를 대응시키는 것인데 이미 교정된 이미지가 있다면 그럴 필요가 있을까요?왜곡된 이미지에서 교정된 이미지니까 반대 아닌가?참고 사이트Udacity Self-driving car nanodegree - Camera Calibration(링크 공유 불가능)다크 프로그래머 블로그 - 카메라 캘리브레이션 (Camera Calibration)다크 프로그래머 블로그 - 카메라 왜곡보정 - 이론 및 실제"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (12) - 서비스와 클라이언트(Python)",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-srv-cli-py.html",
      "date"        : "2021-01-18 09:33:23 +0900",
      "description" : "",
      "content"     : "서비스와 클라이언트서비스와 클라이언트는 ROS의 pub/sub와는 다른 통신방식입니다. 이들에 대한 설명은 다음 게시글에 정리되어 있습니다. 이번 게시글은 사용방법에 대해서 알아보기로 합시다.서비스-클라이언트 작성의 단계srv-cli 작성은 약 단계로 진행됩니다. 패키지 작성 서비스, 클라이언트 노드 작성 setup.py 편집 build &amp; 실행차례대로 살펴봅시다.1. 패키지 작성패키지 작성은 지금까지와 비슷합니다. ros2 pkg create 명령어를 사용합니다.ros2 pkg create --build-type ament_python py_srvcli --dependencies rclpy example_interfaces이번 ros2 pkg create 명령어에서는 --dependencies rclpy example_interfaces라는 구문을 추가하였습니다. 이 구문은 package.xml에 의존성을 자동으로(?) 추가시켜 줍니다. 그래서 이번 강의에서는 package.xml 파일은 건드리지 않습니다.또한, 이 패키지에는 srv 파일을 작성하지 않을 것입니다. srv 파일은 msg 파일과 생김새가 비슷합니다. 이번 게시글에서는 example_interfaces라는 패키지로부터 AddTwoInts.srv라는 파일을 빌려 쓸 것입니다. 파일 내용은 다음과 같습니다.int64 aint64 b---int64 sum2-1. 서비스 노드 작성py_srvcli 패키지 안의 노드 폴더 py_srvcli(이름 패키지명과 같음)에 노드를 작성해 줍니다. 노드 파일 이름은 service_member_function.py로 합니다.from example_interfaces.srv import AddTwoIntsimport rclpyfrom rclpy.node import Nodeclass MinimalService(Node): def __init__(self): super().__init__('minimal_service') self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback) def add_two_ints_callback(self, request, response): response.sum = request.a + request.b self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b)) return responsedef main(args=None): rclpy.init(args=args) minimal_service = MinimalService() rclpy.spin(minimal_service) rclpy.shutdown()if __name__ == '__main__': main()2-1-1. 서비스 노드 구문 분석앞의 게시글에서 설명했던 노드와 작성 방식은 비슷합니다.맨 윗 부분은 의존성 모듈을 import하는 부분입니다. 위에서 말했듯 example_interfaces 패키지의 AddTwoInts 파일을 가져오고 있습니다.from example_interfaces.srv import AddTwoIntsimport rclpyfrom rclpy.node import Node다음 부분은 MinimalService 클래스의 생성자와 callback 함수입니다. 생성자에서 service 객체를 만들고 callback 함수에서 계산을 수행합니다. 소스를 보면 실제 덧셈을 하는 부분은 response.sum = request.a + request.b라고 볼 수 있습니다.class MinimalService(Node): def __init__(self): super().__init__('minimal_service') self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback) def add_two_ints_callback(self, request, response): response.sum = request.a + request.b self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b)) return response그 이후는 main함수로서 별거 없습니다.2-2. 클라이언트 노드 작성서비스 노드와 마찬가지로 ~/(작업공간이름)_ws/src/py_srvcli/py_srvcli 디렉토리에 파일을 생성해 줍니다. 노드 파일 이름은 client_member_function.py로 합니다.import sysfrom example_interfaces.srv import AddTwoIntsimport rclpyfrom rclpy.node import Nodeclass MinimalClientAsync(Node): def __init__(self): super().__init__('minimal_client_async') self.cli = self.create_client(AddTwoInts, 'add_two_ints') while not self.cli.wait_for_service(timeout_sec=1.0): self.get_logger().info('service not available, waiting again...') self.req = AddTwoInts.Request() def send_request(self): self.req.a = int(sys.argv[1]) self.req.b = int(sys.argv[2]) self.future = self.cli.call_async(self.req)def main(args=None): rclpy.init(args=args) minimal_client = MinimalClientAsync() minimal_client.send_request() while rclpy.ok(): rclpy.spin_once(minimal_client) if minimal_client.future.done(): try: response = minimal_client.future.result() except Exception as e: minimal_client.get_logger().info( 'Service call failed %r' % (e,)) else: minimal_client.get_logger().info( 'Result of add_two_ints: for %d + %d = %d' % (minimal_client.req.a, minimal_client.req.b, response.sum)) break minimal_client.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()클라이언트 노드는 request 요청과 response receive의 역할을 합니다. 구문의 역할을 알아봅시다.2-2-1. 클라이언트 노드 구문 분석초반부의 import는 서비스와 마찬가지이므로 생략하겠습니다.MinimalClientAsync 클래스는 생성자와 요청 전송 함수로 이루어져 있습니다. 생성자는 클라이언트 노드 생성, service로부터 응답이 없을 때의 출력값, AddTwoInts라는 함수 요청 기능을 갖고 있습니다.class MinimalClientAsync(Node): def __init__(self): super().__init__('minimal_client_async') self.cli = self.create_client(AddTwoInts, 'add_two_ints') while not self.cli.wait_for_service(timeout_sec=1.0): self.get_logger().info('service not available, waiting again...') self.req = AddTwoInts.Request() def send_request(self): self.req.a = int(sys.argv[1]) self.req.b = int(sys.argv[2]) self.future = self.cli.call_async(self.req)main함수는 응답을 받아 처리하는 기능을 갖고 있습니다. while 반복문을 통해서 응답이 있는지 계속 확인하고, 응답이 있을 때 결과를 출력하는 코드입니다.def main(args=None): rclpy.init(args=args) minimal_client = MinimalClientAsync() minimal_client.send_request() while rclpy.ok(): rclpy.spin_once(minimal_client) if minimal_client.future.done(): try: response = minimal_client.future.result() except Exception as e: minimal_client.get_logger().info( 'Service call failed %r' % (e,)) else: minimal_client.get_logger().info( 'Result of add_two_ints: for %d + %d = %d' % (minimal_client.req.a, minimal_client.req.b, response.sum)) break minimal_client.destroy_node() rclpy.shutdown()3. setup.py 편집노드를 실행할 때 사용할 이름과 정보를 기입해 줍니다.entry_points={ 'console_scripts': [ 'service = py_srvcli.service_member_function:main', 'client = py_srvcli.client_member_function:main', ],},4. build &amp; 실행빌드, source 후 서비스 노드를 실행합니다.$ cd ~/dev_ws$ colcon build$ source install/setup.bash$ ros2 run py_srvcli service다른 터미널 창에서 다음 클라이언트 노드를 실행합니다.$ ros2 run py_srvcli client 2 3결과값은 다음과 같습니다.서비스 노드를 실행한 터미널(클라이언트 실행 전):$ ros2 run py_srvcli service클라이언트 노드를 실행한 터미널:$ ros2 run py_srvcli client 2 3[INFO] [minimal_client_async]: Result of add_two_ints: for 2 + 3 = 5서비스 노드를 실행한 터미널(클라이언트 실행 후):$ ros2 run py_srvcli service[INFO] [minimal_service]: Incoming requesta: 2 b: 3서비스&amp;클라이언트 노드가 잘 실행되는것을 볼 수 있습니다.마무리다음 게시글에서는 msg 파일과 srv 파일을 직접 만들어 보도록 하겠습니다. 앞으로 약 4강정도 남은 듯 합니다.참고 사이트ROS Index-ROS2 튜토리얼 서비스&amp;클라이언트 작성(Python)편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (11) - 퍼블리셔와 서브스크라이버(Python)",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-pub-sub-py.html",
      "date"        : "2021-01-18 09:33:23 +0900",
      "description" : "",
      "content"     : "퍼블리셔와 서브스크라이버Publisher와 Subscriber는 각각 발행자와 구독자에 해당합니다. 위의 그림과 같이 퍼블리셔가 데이터(메시지)를 발행하면 서브스크라이버가 데이터(메시지)를 구독, 즉 읽어서 사용합니다. 초심자용 예시로는 주로 talker와 listener를 사용합니다. 이 게시글에서는 Python 코드를 사용한 퍼블리셔 노드와 서브스크라이버 노드에 대해서 알아봅시다.퍼블리셔 노드 작성1. 패키지 생성우선 노드를 생성하려면 패키지가 있어야 하기에 패키지를 생성합니다. dev_ws 작업공간에 py_pubsub 패키지를 작성해 봅시다.$ cd dev_ws/src$ ros2 pkg create --build-type ament_python py_pubsub2. 퍼블리셔 노드 작성Python 의존성으로 패키지를 생성하였으므로 파이썬 퍼블리셔 노드를 작성해 봅시다. py_pubsub 패키지 안의 py_pubsub 노드 폴더에 노드 파일을 추가합니다.$ gedit publisher_member_function.py소스 내용은 다음의 내용을 넣어 줍시다.import rclpyfrom rclpy.node import Nodefrom std_msgs.msg import Stringclass MinimalPublisher(Node): def __init__(self): super().__init__('minimal_publisher') self.publisher_ = self.create_publisher(String, 'topic', 10) timer_period = 0.5 # seconds self.timer = self.create_timer(timer_period, self.timer_callback) self.i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self.i self.publisher_.publish(msg) self.get_logger().info('Publishing: \"%s\"' % msg.data) self.i += 1def main(args=None): rclpy.init(args=args) minimal_publisher = MinimalPublisher() rclpy.spin(minimal_publisher) # Destroy the node explicitly # (optional - otherwise it will be done automatically # when the garbage collector destroys the node object) minimal_publisher.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()또는 그냥 ~/dev_ws/src/py_pubsub/py_pubsub/src 폴더에서 다음의 명령어로 다운로드 할 수 있습니다.wget https://raw.githubusercontent.com/ros2/examples/master/rclpy/topics/minimal_publisher/examples_rclpy_minimal_publisher/publisher_member_function.py코드의 내용을 해석해 보겠습니다.2.1 퍼블리셔 노드 해석다음 구문을 통해 Python으로 Node를 생성하는 모듈을 불러올 수 있습니다.import rclpyfrom rclpy.node import Node역시 마찬가지로 std_msgs를 사용할 수 있습니다.from std_msgs.msg import String위의 구문들은 이 노드의 의존성을 담당합니다. 이 의존성들은 추후 package.xml에도 추가해 주어야 합니다.다음 구문을 통해 MinimalPublisher 클래스를 생성합니다.class MinimalPublisher(Node): def __init__(self): super().__init__('minimal_publisher') self.publisher_ = self.create_publisher(String, 'topic', 10) timer_period = 0.5 # seconds self.timer = self.create_timer(timer_period, self.timer_callback) self.i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self.i self.publisher_.publish(msg) self.get_logger().info('Publishing: \"%s\"' % msg.data) self.i += 1클래스 안의 함수는 각각 생성자와 timer_callback이라는 함수입니다. 생성자는 노드를 초기화 해주는 함수이고, timer_callback은 타이머에 맞춰 msg에 저장된 데이터를 계속해서 publish해주는 함수입니다.마지막으로 main 함수입니다.def main(args=None): rclpy.init(args=args) minimal_publisher = MinimalPublisher() rclpy.spin(minimal_publisher) # Destroy the node explicitly # (optional - otherwise it will be done automatically # when the garbage collector destroys the node object) minimal_publisher.destroy_node() rclpy.shutdown()main문에서 실행될 함수입니다. rclpy.spin(minimal_publisher)을 사용하여 계속해서 퍼블리시 해줍니다. 남은 구문은 그냥 main함수 실행을 위한 구문입니다.if __name__ == '__main__': main()퍼블리셔 노드 package.xml &amp; setup.py 편집package.xml 파일에 의존성을 추가해줘야 합니다. 노드 파일에서 rclpy와 std_msgs 의존성을 추가해줍니다. &lt;export&gt; &lt;build_type&gt;ament_python&lt;/build_type&gt; &lt;exec_depend&gt;rclpy&lt;/exec_depend&gt; &lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt; &lt;/export&gt;또, setup.py 파일도 수정해 줍니다. entry_points 부분을 다음과 같이 수정해줍니다.entry_points={ 'console_scripts': [ 'talker = py_pubsub.publisher_member_function:main', ],},이로써 퍼블리셔 실행 준비는 끝났습니다. 여기서 빌드&amp;source 후 바로 퍼블리셔를 실행해 볼 수도 있지만, 서브스크라이버까지 작성한 후에 해 봅시다.서브스크라이버 노드 작성과정은 대체로 퍼블리셔 작성과정과 같습니다. ~/dev_ws/src/py_pubsub/py_pubsub/src 폴더에 subscriber_member_function.py 파일을 작성, 다음의 내용을 넣어 줍시다.import rclpyfrom rclpy.node import Nodefrom std_msgs.msg import Stringclass MinimalSubscriber(Node): def __init__(self): super().__init__('minimal_subscriber') self.subscription = self.create_subscription( String, 'topic', self.listener_callback, 10) self.subscription # prevent unused variable warning def listener_callback(self, msg): self.get_logger().info('I heard: \"%s\"' % msg.data)def main(args=None): rclpy.init(args=args) minimal_subscriber = MinimalSubscriber() rclpy.spin(minimal_subscriber) # Destroy the node explicitly # (optional - otherwise it will be done automatically # when the garbage collector destroys the node object) minimal_subscriber.destroy_node() rclpy.shutdown()if __name__ == '__main__': main()또는 그냥 ~/dev_ws/src/py_pubsub/py_pubsub/src 폴더에서 다음의 명령어로 파일을 다운로드 할 수 있습니다.wget https://raw.githubusercontent.com/ros2/examples/master/rclpy/topics/minimal_subscriber/examples_rclpy_minimal_subscriber/subscriber_member_function.py퍼블리셔 노드와 대부분 같지만, MinimalSubscriber 클래스가 조금 다릅니다. 생성자와 데이터를 subscribe 하는 listner_callback 함수로 이루어져 있습니다. 타이머에 상관없이 항상 응답을 받기 때문에 서브스크라이버는 타이머를 사용하지 않습니다.package.xml 파일은 퍼블리셔와 같은 의존성을 사용하기에 수정할 필요가 없고, setup.py 파일의 entry_points 부분을 다음과 같이 수정해 줍시다.entry_points={ 'console_scripts': [ 'talker = py_pubsub.publisher_member_function:main', 'listener = py_pubsub.subscriber_member_function:main', ],},'listener = py_pubsub.subscriber_member_function:main', 구문을 추가하였습니다. 이제 colcon build 명령어로 한번 빌드, . install/setup.bash 명령어로 한번 source하고 노드들을 실행해 봅시다.$ colcon build --packages-select py_pubsubStarting &gt;&gt;&gt; py_pubsubFinished &lt;&lt;&lt; py_pubsub [0.85s]Summary: 1 package finished [1.01s]$ . install/setup.bash퍼블리셔 노드의 실행 결과입니다.$ ros2 run py_pubsub talker[INFO] [minimal_publisher]: Publishing: \"Hello World: 0\"[INFO] [minimal_publisher]: Publishing: \"Hello World: 1\"[INFO] [minimal_publisher]: Publishing: \"Hello World: 2\"[INFO] [minimal_publisher]: Publishing: \"Hello World: 3\"[INFO] [minimal_publisher]: Publishing: \"Hello World: 4\"이제 새 터미널을 열어 서브스크라이버를 실행해 봅시다. 서브스크라이버 노드의 실행 결과입니다.$ ros2 run py_pubsub listener [INFO] [minimal_subscriber]: I heard: \"Hello World: 8\"[INFO] [minimal_subscriber]: I heard: \"Hello World: 9\"[INFO] [minimal_subscriber]: I heard: \"Hello World: 10\"[INFO] [minimal_subscriber]: I heard: \"Hello World: 11\"[INFO] [minimal_subscriber]: I heard: \"Hello World: 12\"새 터미널을 열고 ros2 run py_pubsub listener 명령어를 실행하는데 물리적으로 시간이 걸려 받아오는 숫자가 약간 다르지만 서브스크라이버 실행 후 퍼블리셔 실행 결과와 비교하면 같은 데이터를 받는 것을 확인할 수 있습니다.마무리착착 진도가 나가고 있습니다. 다음 게시글은 서비스-클라이언트를 파이썬으로 작성해보는 튜토리얼을 정리하겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 퍼블리셔&amp;서브스크라이버 작성(Python)편"
    } ,
  
    {
      "title"       : "[Udacity] Computer Vision (1) - CV Fundamental",
      "category"    : "",
      "tags"        : "Udacity, Computer Vision",
      "url"         : "./uda-cv-cv-fund.html",
      "date"        : "2021-01-18 00:01:23 +0900",
      "description" : "",
      "content"     : "Udacity self-driving nanodegree - Computer Vision이 게시글은 Udacity self-driving nanodegree 코스의 Computer Vision 파트의 강의를 정리하는 글입니다. 자율주행 자동차를 공부하기 위해서 컴퓨터 비전과 위치 추정을 번갈아가면서 공부할 예정입니다. 이 강의에서는 파이썬과 OpenCV를 사용합니다. 버전은 잘 모르겠지만 알아내는 대로 수정할 예정입니다.Computer Vision의 기초이번 강의는 차선 인식을 목표로 진행됩니다. 사람은 눈으로 보고 차선을 결정하지만 자동차는 눈 대신 카메라로 차선을 인식하여 그에 맞게 달려야 합니다. 차선 인식을 하기 위해서는 색상 분리, 관심 영역 설정, canny detection, hough transform의 과정을 거칩니다.색상이미지는 3가지 색상의 조합으로 이루어져 있습니다. 흔히들 알고 있는 RGB 색상입니다. 이 색상의 가짓수를 채널이라고 합니다. 주로 일반적인 이미지는 3채널입니다. 각각의 채널은 R: 0~255, G: 0~255, B: 0~255 값을 가지고 있습니다. 하지만 채널이 3개일 때는 $255^3$만큼의 계산을 해야 하기에 연산량이 큽니다. 이때 연산량을 줄이기 위해 이미지에 Grayscale 처리를 적용합니다.GrayscaleGrayscale 처리는 이미지의 채널을 3채널에서 1채널로 줄이는 과정입니다. 1채널로 줄어들면서 각 픽셀은 0~255값만 가지게 됩니다. 사진에서 볼 수 있듯이 컬러 이미지가 흑백 이미지로 변환되었습니다. 흑백 이미지로 변환하는 원리는 일반적으로 원래의 3채널 값을 모두 합하여 평균값을 내는 것입니다. 예를 들어 RGB값이 $(R, G, B) = (200, 100, 0)$이라면, Grayscale처리를 통해 Gray값은 $Y = \\frac{200+100+0}{3} = 100$이 됩니다.실제로는 완전히 평균값을 쓰지 않고 가중치를 사용합니다. $Y=0.299\\times R+0.587\\times G+0.114\\times B$와 같은 공식을 사용합니다. 왜냐하면 사람 눈에는 동일한 값을 가질 때 G가 가장 밝게 보이고 그 다음으로 R, B가 밝게 보이기 때문입니다.Grayscale을 하는 이유는 다음과 같습니다. 연산량 감소위에서 언급한 바와 같이 $255^3$에서 $255$로 줄어들었기 때문에 많은 도움이 됩니다. 차선의 특수한 색차선은 보통 흰색 또는 노란색으로 구성되어 있기 때문에 Grayscale로 바꿔보면 약 200 이상의 높은 값을 가집니다. 0~255라는 범위 중 높은 위치에 있기 때문에 이진화 하기 적합합니다.grayscale 예제OpenCV 예제에 사용되는 이미지는 여기에서 받을 수 있습니다.import cv2img = cv2.imread('/home/사진 경로/lane_line1.png')gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)cv2.imshow('Result', gray)cv2.waitKey()Input:Output:관심 영역 (ROI) 설정이제 우리는 ROI를 설정할 것입니다. ROI는 관심 영역이라고 하는데, 이는 우리가 필요한 영역만 처리하도록 처리 영역을 조정하는 것을 뜻합니다. 예를 들어 위의 그림에서 차선 인식을 할 때, 하늘에는 차선이 없으니 상단부 반쪽은 사용하지 않습니다. 또한, 도로는 대부분 앞으로 길게 뻗어 있으므로, 가운데로 모이는 모양의 사다리꼴을 범위로 지정합니다.ROI 설정 예제import cv2import numpy as npimg = cv2.imread('/home/사진 경로/lane_line1.png')# grayscale 처리gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# mask 영행렬, 이미지에서 무시할 부분 컬러 설정(255: 검정)mask = np.zeros_like(gray) ignore_mask_color = 255 imshape = img.shape# 좌표 설정vertices = np.array([[(0,imshape[0]),(360, 270), (440, 270), (imshape[1],imshape[0])]], dtype=np.int32)# fillpoly: mask 행렬에 vertices 좌표를 이은 채워진 다각형을 그립니다.cv2.fillPoly(mask, vertices, ignore_mask_color)# bitwise_and: and 논리연산으로 관심영역 이외의 부분을 잘라냅니다.masked_edges = cv2.bitwise_and(gray, mask)cv2.imshow('Result', masked_edges)cv2.waitKey()Input:Grayscale 예제의 Input과 동일합니다.Output:Canny Edge DetectionCanny edge detection은 차선의 edge를 찾아서 검출하는 알고리즘입니다. edge는 가장자리를 의미합니다. 이 강의에서는 차선의 가장자리를 추출할 때 이 알고리즘을 사용합니다.이미지에서 edge를 검출하는 key는 인접한 색상 변화를 인식하는 것입니다. 이때 색상의 변화를 나타낸 것이 Gradient입니다. Gradient는 쉽게 말하면 두 지역 간의 변화를 수치로 나타낸 값이라고 생각하면 됩니다. 위의 사진을 참고하면 검출하려는 차선 근처의 색은 검은색 도로입니다. 그렇다면 차선과 도로 사이의 경계면에서 색상이 확 변하는 부분이 있을 것입니다. 그렇다면 차선의 edge 근처에는 gradient가 클 것이고, 그 부분의 픽셀을 따라서 그리다보면 그것이 edge가 되는 것입니다.그렇다면 이제 edge를 검출하는 알고리즘 중 하나인 Canny edge detection을 알아봅시다. Canny edge detection은 John Canny라는 분이 1986년에 개발한 edge 검출 알고리즘입니다. 이 알고리즘은 gradient를 구한 후 그 값을 threshold라는 범위에 맞추어 edge인지 아닌지를 판단합니다. 그 판단 기준은 다음과 같습니다.Hysteresis ThresholdingCanny 함수의 input으로서 grayscale 이미지와 low_threshold, high_threshold를 줍니다. low_threshold와 high_threshold는 각각 그림에서의 minVal과 maxVal입니다. gradient가 minVal보다 낮으면 확실하게 edge가 아니라 간주되어 버려지고, 반대로 maxVal보다 높으면 확실하게 edge로 취급하여 추출합니다.하지만 문제는 그 사이에 있을 때입니다. minVal과 maxVal 사이에 있을 때는 다음과 같은 과정을 거칩니다.A는 maxVal값보다 높으므로 edge로 간주됩니다. C는 maxVal보다 낮지만 edge A에 연결되어 있으므로 edge로 취급할 수 있습니다. 하지만 B는 완전히 사이에 있고, 다른 edge와 연결점이 없으므로 edge가 아니라고 간주되어 버려집니다. 따라서 좋은 값을 얻으려면 minVal(low_threshold)와 maxVal(high_threshold)를 적절하게 주어야 합니다.예제를 통해 사용 과정을 알아봅시다.Canny Edge Detection 예제import cv2import numpy as npimg = cv2.imread('/home/사진 경로/lane_line1.png')# grayscale 처리gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)# 가우시안 blur: 이미지를 흐릿하게 하여 노이즈 제거blur_gray = cv2.GaussianBlur(gray, (3, 3), 0)# Canny Edge Detectionedges = cv2.Canny(blur_gray, 50, 150)cv2.imshow('Result', edges) cv2.waitKey()Input:Grayscale 예제의 Input과 동일합니다.Output:반드시 알아야 할 부분은 아니지만, 가우시안 blur 함수의 output은 다음과 같습니다.원래의 grayscale 사진보다 조금 흐릿합니다.Hough Transform허프 변환은 1959년에 Paul Hough라는 분이 개발한 이미지 좌표계를 다른 좌표계 방식으로 표현하는 변환을 말합니다. 변환이라는게 원래 저런 뜻이니 좀 더 자세히 설명하겠습니다.허프 변환은 $y=mx+b$라는 이미지 좌표계 상의 방정식을 기울기와 $y$절편 $(m, b)$가 가로, 세로축인 파라미터 좌표계 상에 나타내는 것입니다. 어떤 수학적인 변환이 있는 것이 아닌 단지 직선방정식의 $(m, b)$를 좌표계에 나타낸 것입니다. 따라서 한 직선방정식의 기울기와 $y$절편은 $(m_0, b_0)$ 단 한 점 밖에 있을 수 없습니다.그렇다면 이미지 좌표계에서 평행한 두 직선은 어떨까요? 이 문제를 풀어 봅시다.평행한 두 직선은 파라미터 좌표계에서 두 점으로 표현됩니다. 이 두 점은 기울기가 동일하지만 $y$절편이 다른 점이기 때문에 기울기가 $m_0$이라고 했을 때, $m = m_0$ 직선 위의 두 점이 됩니다. 따라서 정답은 3번입니다.그럼 이제 이미지 죄표계에서의 점은 어떻게 표현될까요? 다음 문제를 한번 봅시다.이번에는 $(x, y)$값이 고정이므로 오히려 파라미터 좌표계에서 직선으로 표현됩니다. 이미지 좌표계의 두 점을 각각 $(x_0, y_0)$, $(x_1, y_1)$으로 표현했을 때, 파라미터 좌표계의 방정식은 $b = -x_{0}m+y_{0}$, $b=-x_{1}m+y_{1}$이 됩니다. 이 두 방정식은 그래프로 그렸을 때 3번과 같이 그려지며, 교점이 생깁니다. 이 교점의 의미는 무엇일까요? $m$값과 $b$값이 같다면 두 점이 같은 직선 위에 있다는 의미입니다. 이 말은 결국 이미지 평면에서 여러 점이 한 직선 위에 있다면, 파라미터 평면에서 한 점에서 여러 직선이 만날 수 있다는 것을 보여줍니다.점이 3개라면 위의 그림과 같이 표현할 수 있습니다.하지만 $(m, b)$를 가로, 세로축으로 삼는 좌표계는 문제점이 있습니다. 바로 기울기 m이 무한대인 경우, $y$축과 수평이 되면서 이런 경우는 파라미터 좌표계에서 표현할 수 없습니다.따라서 이 문제를 해결하기 위해 새로운 파라미터 평면을 고안했습니다. 이번에는 가로-세로축을 $(\\rho, \\theta)$로 표현합니다. 이 평면을 Hough Space라고 부릅니다. 계속 파라미터 평면이라고 하다가 드디어 허프 평면이 나왔습니다.일단 비슷한 구조지만, 그래프는 조금 다르게 그려집니다.허프 평면의 그래프는 사인파 형태로 그려지지만, 교차점을 지나는 그래프의 갯수가 이미지 평면에서 한 직선 위의 점의 갯수와 같다는 점에서 파라미터 평면과 비슷한 원리입니다. 다시 한번 말하지만 허프 평면은 파라미터 평면에서 $m$이 무한대로 가는 문제를 해결하기 위해서 고안된 방법입니다.Hough Transfrom 예제OpenCV에서 지원하는 허프 변환의 함수는 두개입니다. cv2.HoughLines(image, rho, theta, threshold) image: 8bit grayscale 이미지를 넣어 주어야 합니다. Canny edge detection 후에 이 함수에 넣어줍니다. rho: 허프 평면에서 $\\rho$값을 얼마나 증가시키면서 조사할지를 의미합니다. 보통 1을 넣습니다. theta: rho와 마찬가지로 증가값입니다. 단위는 라디안이므로 1도씩 증가시키고 싶다면 $1 \\times \\frac{\\pi}{180}$을 해줘야 합니다. 범위는 0~180도입니다. threshold: 허프 변환 함수의 직선 판단 기준은 교차점의 갯수입니다. 교차점이 많이 쌓일수록 직선일 가능성이 높아지는 것입니다. threshold는 직선 판단 기준입니다. 교차점의 갯수가 누적되어 threshold값을 넘는다면 직선이라고 판단하는 것입니다. 즉 threshold 값이 작으면 기준이 낮아서 많은 직선이 검출되겠지만, threshold 값을 높게 주면 적지만 확실한 직선들만 검출될 것입니다. output은 검출된 직선 만큼의 $\\rho$와 $\\theta$입니다. cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)허프 변환을 확률적으로 계산하는 함수입니다. 앞의 매개변수 4개는 1번 함수와 같습니다. np.array([]): 빈 array입니다. min_line_length: 선의 최소 길이입니다. 너무 짧은 선은 검출하기 싫다면 이 값을 높입니다. 단위는 픽셀입니다. max_line_gap: 선 위의 점들 사이 최대 거리입니다. 즉 점 사이의 거리가 이 값보다 크면 지금 만들고 있는 선과는 다른 선으로 간주하겠다 라는 것입니다. output은 선분의 시작점과 끝점에 대한 좌표값입니다.이 두 함수의 차이점은 HoughLines는 직선을 출력하고, HoughLinesP는 선분을 출력합니다.import matplotlib.pyplot as pltimport matplotlib.image as mpimgimport numpy as npimport cv2image = cv2.imread('/home/사진 경로/lane_line1.png')gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)kernel_size = 3blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size),0)low_threshold = 50high_threshold = 150edges = cv2.Canny(blur_gray, low_threshold, high_threshold)mask = np.zeros_like(edges) ignore_mask_color = 255 imshape = image.shapevertices = np.array([[(0,imshape[0]),(360, 270), (440, 270), (imshape[1],imshape[0])]], dtype=np.int32)cv2.fillPoly(mask, vertices, ignore_mask_color)masked_edges = cv2.bitwise_and(edges, mask)# 허프 변환 파라미터 설정rho = 1theta = np.pi/180 threshold = 15 min_line_length = 40 max_line_gap = 20line_image = np.copy(image)*0 # 이미지와 같은 사이즈의 영행렬 생성# 허프 변환# 감지된 선분들의 양 끝점 반환, line형태의 데이터lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)# Line 그리기 - line_imagefor line in lines: for x1,y1,x2,y2 in line: cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)cv2.imshow('Result', line_image)# Create a \"color\" binary image to combine with line imagecolor_edges = np.dstack((edges, edges, edges)) # canny detection의 결과에 color 가중치 적용lines_edges = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0) cv2.imshow('Result', lines_edges)cv2.waitKey()Input:Grayscale 예제의 Input과 동일합니다.Output:마무리이번 강의에서는 직선 차선만을 검출하는 방법을 배워 보았습니다. 다음 강의에서는 카메라 캘리브레이션과 왜곡 제거에 대한 내용을 정리할 예정입니다.참고 사이트Udacity Self-driving car nanodegree - CV Fundamental(링크 공유 불가능)OpenCV Documentation - Canny Edge Detection[Udacity] SelfDrivingCar- 2-3. 차선 인식(hough transform)"
    } ,
  
    {
      "title"       : "[Udacity] Markov Localization",
      "category"    : "",
      "tags"        : "Markov Localization, Bayesian Filter",
      "url"         : "./uda-loc-markov.html",
      "date"        : "2021-01-16 22:02:23 +0900",
      "description" : "",
      "content"     : "Localization이란 무엇?Localization은 자율주행차 또는 모바일 로봇이 자신의 위치를 인지하는 것을 의미합니다. 직역하면 위치 추정입니다. 어떻게 하면 자동차가 자기 위치를 인식 할 수 있을까요? 그것은 바로 센서와 지도를 이용하는 것입니다. 아직은 SLAM이 아닌 지도를 알고 움직이는 자동차로 생각합니다. 이 작업에서 가장 중요한 것은 확률, $bel(x_t)$입니다.자율주행차의 확률 $bel(x_t)$자동차는 확률을 $bel(x_t)$라는 확률함수의 형태로 표현합니다. $x_t$는 자동차의 상태를 의미하는데, 이 강의에서는 position으로 취급합니다. 따라서 $bel(x_t)$는 자동차가 $x_t$라는 지점에 있을 확률이라고 해석할 수 있습니다. 이를 확률함수의 형태로 표현하면 그림과 같이 $bel(x_t)=p(x_t|z_{1:t}, u_{1:t}, m)$이 됩니다. 확률함수의 조건인 $z_{1:t}, u_{1:t}, m$에 대해서 알아보도록 하겠습니다.1. $z_{1:t}$: Observation$z_{1:t}$는 자동차의 관측(Observation)과 관련된 항입니다. 시간 1부터 $t$까지의 관측값을 반영한 성분입니다. 앞으로 들 예시에서는 자동차의 position으로부터 landmark까지의 거리로 생각합니다. 또한, 이 항을 확률함수로 나타낸다면 $p(z_t)$로 나타냅니다. 의미는 실제 관측값과 자동차가 생각하는 관측값의 차이를 정규분포에 따라 도출해낸 확률, 즉 자동차가 생각하는 관측값이 맞을 확률입니다.2. $u_{1:t}$: Motion$u_{1:t}$는 자동차의 움직임(Motion)과 관련된 항입니다. 자동차의 움직임은 역시 바퀴가 굴러가는 이동입니다. 앞으로 들 예시에서 역시 자동차가 이동한 거리로 생각합니다.3. $m$: Map$m$은 단순한 지도입니다. landmark의 좌표를 나타내는 항이라고 볼 수 있습니다. 예시 역시 별 차이 없이 landmark의 위치를 담고 있는 값으로 생각합니다.4. $x_t$: Position$x_t$는 자동차가 있는 위치(Position)와 관련된 항입니다. 자동차가 생각하는 위치라고 볼 수 있습니다. 실제 값과는 다를 수 있습니다.결론결국, 여기서 구하고자 하는 값은 $p(x_t)$입니다. 이는 자동차가 알고 있는 자신의 위치가 맞을 확률을 나타내는 확률함수입니다. 그럼 그림의 확률함수를 한번 봅시다.$\\large{bel(x_t)=p(x_t|z_{1:t}, u_{1:t}, m)}$그림의 확률함수는 결국 조건부 확률입니다. $1:t$ 동안의 관측값 $z$, 이동값 $u$, 지도 $m$이라는 조건일 때, 시간 $t$일 때 자동차가 위치 $x$에 있는게 맞을 확률을 의미합니다. NOTE!SLAM에서의 확률 $bel(x_t)$는 조금 다릅니다. 위치추정과 맵핑을 동시에 수행하기 때문에 지도 $m$ 역시 계속 업데이트 해 주어야 하는 항목입니다. 따라서 SLAM에서의 $bel(x_t)$는 $p(x_t, m|z_{1:t}, u_{1:t})$이 됩니다. 하지만 아직은 SLAM에서의 $bel(x_t)$는 신경쓰지 않습니다.베이즈 정리베이즈 정리는 지난번에 작성한 게시글이 있지만 간단히 다시 한번 설명하겠습니다. 베이즈 정리의 생김새는 다음과 같습니다.$\\large{P(A|B)=\\frac{P(B|A)P(A)}{P(B)}}$이 정리에서 주목해야 할 부분은 사전확률(Prior) $P(A)$와 사후확률(Posterior) $P(A|B)$입니다. 베이즈 정리의 핵심 기능은 사건이 발생하기 이전의 사건 발생 확률과 발생했을 때의 조건을 조합하여 사후확률, 즉 사건 발생 이후의 사건 발생 확률을 도출하는 것입니다. 이는 베이즈 정리를 여러번 사용하여 확률을 계속 업데이트하는 형태로 많이 사용됩니다. 사건과 발생이란 말이 너무 많으니 게슈탈트 붕괴가 오네요…. 심플하게 표현하면 사전+조건=사후로 정리할 수 있습니다. +는 더하기가 아닌 조합한다는 의미입니다.Localization + 베이즈 정리이제 Markov Localization 공식 유도의 첫번째 단계입니다. 우리는 베이즈 정리와 다른 정리들을 사용하여 확률함수 $p(x_t|z_{1:t}, u_{1:t}, m)$을 재귀 형태로 만들 것입니다.1. 베이즈 정리로 $bel(x_t)$ 나타내기우선 확률함수 $p(x_t|z_{1:t}, u_{1:t}, m)$을 베이즈 정리에 따라 나타내 봅시다. $z_{1:t}$를 $z_t$와 $z_{1:t-1}$으로 분리합니다. 이때 분리한 $z_t$는 위의 베이즈 정리에서 사건 $B$를 맡을 것입니다.$\\large{bel(x_t)=p(x_t|z_{1:t}, u_{1:t}, m)=p(x_t|z_t, z_{1:t-1}, u_{1:t}, m)}$그 다음 베이즈 정리를 사용합니다. 사건 $A$는 당연히 $x_t$입니다.$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m)=\\frac{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)p(x_t|z_{1:t-1}, u_{1:t}, m)}{p(z_t|z_{1:t-1}, u_{1:t}, m)}}$굉장히 길고 복잡한 식이 되었습니다. 조금 더 정리해 봅시다.2. Normalizer 처리베이즈 정리에서 분모는 Normalizer로서 생략 또는 상수화 시킬 수 있습니다. 여기서 Normalize는 정규화라고 하는데, 가장 일반적인 항으로 만드는 것을 의미합니다.간단한 예를 들어보겠습니다. 어떤 날 비가 올 확률이 항상 0.6인 세계가 있고, 이 세계는 비 또는 맑은 날씨밖에 없고, 하루하루가 독립시행이라고 가정합니다. 이때 어떤 날 비가 왔을때, 다음날도 비가 올 확률과 다음날엔 맑은 확률을 구해 봅시다.먼저 이틀간의 날씨가 비-비 일 확률입니다. 0.6$\\times$0.6=0.36입니다. 그 다음으로 비-맑음일 확률은 0.6$\\times$0.4=0.24입니다. 이 두 확률의 합은 0.6입니다.이때 확률의 총합을 1로 만들고 싶다면, 어제의 날씨가 비였을 확률로 이들을 나눠 줍니다.$\\large{\\frac{0.36}{0.6}+\\frac{0.24}{0.6}=1}$이런식으로 일반적인 항으로 만들어 주는 것을 정규화라고 합니다. 다시 자동차의 확률함수 식으로 돌아가 봅시다. 이 식에서의 normalizer 항은 다음과 같습니다.$\\large{\\begin{align*}p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) &amp;= \\frac{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)p(x_t|z_{1:t-1}, u_{1:t}, m)}{p(z_t|z_{1:t-1}, u_{1:t}, m)} \\\\&amp;= \\underbrace{\\frac{1}{p(z_t|z_{1:t-1}, u_{1:t}, m)}}_{\\text{Normalizer}}p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)p(x_t|z_{1:t-1}, u_{1:t}, m)\\end{align*}}$표시된 항을 정규화 상수로 취급하고, $\\eta$로 표현합니다. 이를 다시 정리하면 다음과 같습니다.$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) = \\eta \\; p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)p(x_t|z_{1:t-1}, u_{1:t}, m)}$Motion 모델과 Observation 모델$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) = \\eta \\; \\underbrace{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)}_{\\text{Observation Model}} \\; \\underbrace{p(x_t|z_{1:t-1}, u_{1:t}, m)}_{\\text{Motion Model}}}$Normalizer까지 처리하고 난 식에서, 표시된 항은 각각 Observation 모델, Motion 모델이라고 합니다. 이들은 정리 과정에서 각각 $z$(Observation)항과 $u$(Motion)항만이 남게 되기에 이런 이름이 붙었습니다. 먼저 Motion 모델부터 살펴봅시다.1. Motion 모델 정리 - Law of Total ProbabilityMotion 모델만 따로 떼어 생각해 봅시다.$\\large{p(x_t|z_{1:t-1}, u_{1:t}, m)}$이 항은 조건을 빼고 생각하면 사실상 어떤 조건에서의 $x_t$ 사건이 발생할 확률, 즉 $p(x_t)$입다. 이때, 재귀 형태로 만들기 위해서 새로운 항을 추가할 수 있는 Law of Total Probability 정리를 사용합니다. 이는 다음 블로그에 잘 정리되어 있습니다. 정리는 다음과 같습니다.$\\large{P(A)=\\int_{B}^{}P(A|B)P(A) \\; dB}$이를 위의 식에 적용하여 새로운 항 $x_{t-1}$을 추가합니다.$\\large{p(x_t|z_{1:t-1}, u_{1:t}, m)=\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, z_{1:t-1}, u_{1:t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t}, m) \\; dx_{t-1}}$간단하게 표현하면 일어난 일은 다음과 같습니다.$\\large{p(x_t)=\\int_{x_{t-1}}^{}p(x_t|x_{t-1})P(x_{t-1}) \\; dx_{t-1}}$Law of Total Probaility를 사용하여 이전 position $x_{t-1}$과의 연결고리를 만들었습니다. 이제 재귀 형태의 베이스가 갖춰졌으니, 생략 가능한 항을 찾아봅시다.2. Motion 모델 정리 - Markov AssumptionLaw of Total Probaility를 사용한 후, 도출된 식을 좀 더 줄이는데 Markov Assumption을 사용합니다. Markov Assumption은 쉽게 말하면 관계없는 항을 계속 생략하는 방법입니다. 오컴의 면도날 법칙을 정리로 만들어 놓은 듯한 이 정리를 통해 Motion 모델을 생략해 봅시다.먼저 현재의 Motion 모델을 그래프로 나타내면 다음과 같습니다.$\\large{p(x_t|z_{1:t-1}, u_{1:t}, m)=\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, z_{1:t-1}, u_{1:t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t}, m) \\; dx_{t-1}}$위의 그래프에서 $z_{1:t-1}$과 $u_{1:t-1}$은 이미 $x_{t-1}$을 만드는 데 반영되어 있습니다. 따라서 이들을 $x_t$에까지 반영할 필요가 없습니다. 따라서 $z_{1:t-1}$과 $u_{1:t-1}$을 생략합니다.$\\large{\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, z_{1:t-1}, u_{1:t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t}, m) \\; dx_{t-1}}$$\\large{\\rightarrow \\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t}, m) \\; dx_{t-1}}$이제 조금 더 생략해 봅시다. 위의 그래프의 $u_t$를 봅시다. 시간 $t$에서의 Motion인 $u_t$는 $t-1$에서의 position $x_{t-1}$과 연결되어 있지만 position $x_{t-1}$의 확률함수를 구하는데는 쓸모가 없습니다. 따라서 $x_{t-1}$을 가리키는 $u_t$의 화살표를 생략할 수 있습니다.$\\large{\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t}, m) \\; dx_{t-1}}$$\\large{\\rightarrow \\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t-1}, m) \\; dx_{t-1}}$이 과정을 거치고 나면, 지금까지의 결과의 $p(x_{t-1}| z_{1:t-1}, u_{1:t-1}, m)$ 항은 익숙한 모습을 하고 있습니다. 바로 $bel(x_{t-1})$의 모습입니다. 해당 항을 $bel(x_{t-1})$으로 교체해 줍니다.$\\large{\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m)p(x_{t-1}| z_{1:t-1}, u_{1:t-1}, m) \\; dx_{t-1}}$$\\large{\\rightarrow \\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m) \\; bel(x_{t-1}) \\; dx_{t-1}}$이제 위에서 언급한 대로 Motion 모델에는 $z$항이 없어지고 $u$항만 남은데다, 재귀 형태의 확률함수가 되었습니다. 이전 position의 확률로부터 현재 position에 대한 확률을 도출할 수 있게 되었습니다.시간 $t$에서의 Motion 모델위에서 나타낸 시간 $t$에서의 Motion 모델은 이산적인 확률의 합입니다.시간$1:t$에 따른 Motion 항은 다음과 같이 표현할 수 있습니다.$\\large{u_{1:t}=\\left \\{ u_1, u_2, \\cdots, u_t \\right \\}}$그리고 다시 이 항들은 이전 position에 따라서 $u_{t}^{(i)}$들로 표현될 수 있습니다.$\\large{u_{t}=\\left \\{ u_{t}^{(1)}, u_{t}^{(2)}, \\cdots, u_{t}^{(i)} \\right \\}}$여기서 각각의 항에 해당하는 확률은 이전 position과 현재 position 간에 오차의 *정규분포 $\\times \\; bel(x_{t-1})$과 같습니다. 식으로 나타내면 다음과 같습니다.$이전\\;position과\\;현재\\;position\\;간에\\;오차의\\;정규분포 = p(x_t|x_{t-1}^{(i)}, u_t, m) \\;\\;\\;\\; (Transition\\;Model)$$\\large{u_{t}^{(i)}에\\;해당하는\\;이산적\\;확률 = p(x_t|x_{t-1}^{(i)}, u_t, m) \\; bel(x_{t-1}^{(i)})}$이러한 $u_{t}^{(1)} \\sim u_{t}^{(i)}$에 해당하는 확률을 모두 합친 확률이 바로 위에서 정리했던 시간 $t$에서의 Motion 모델입니다.$\\large{시간\\;t에서의\\;Motion\\;모델=\\sum_{i}^{}p(x_t|x_{t-1}^{(i)}, u_t, m) \\; bel(x_{t-1}^{(i)})}$예시를 통해 더욱 알기 쉽게 알아보겠습니다.다음 표에 있는 항목들의 의미는 다음과 같습니다. pseudo position: position $x_t$ pre-pseudo position: position $x_{t-1}$ delta position: position $x_t$ - position $x_{t-1}$ P(transition): delta position의 정규분포 결과값 $bel(x_{t-1})$: $bel(x_{t-1})$ P(position): P(transition)$\\times\\;bel(x_{t-1})$각각의 행은 일어날 수 있는 사건을 의미합니다. 예를 들어 첫번째 행은 자동차가 position $x_{t-1}$일때 좌표 1에 있었다가, position $x_t$일때 좌표 7로 이동한 경우입니다. 나머지 항들 역시 해당 사건의 P(transition) 또는 P(position) 값입니다.결과적으로 모든 P(position)의 합이 시간 $t$에서의 Motion 모델이 됩니다. 이 강의의 예시는 쉽게 이해시키기 위해 1차원 이동만을 봤지만, 실제로 차는 최소 2차원 이동을 하게 됩니다. 그때가 되면 더욱 어려운 계산을 해야 할 것입니다.*정규분포: 이 강의에서는 확률밀도함수 정규분포를 사용하였습니다. 하지만 실제로는 정규분포함수를 사용할 수 없는 모델도 나옵니다. 항상 정규분포는 아니라는 점을 기억해 둡시다.3. Observation 모델 정리 - Markov AssumptionMotion 모델의 정리를 마쳤으니 남은 항은 왼쪽의 Observation 모델입니다.$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) = \\eta \\; \\underbrace{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)}_{\\text{Observation Model}} \\; \\underbrace{\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m) \\; bel(x_{t-1}) \\; dx_{t-1}}_{\\text{Motion Model}}}$이번 정리에서는 Markov Assumption만 사용합니다. 다음 그림을 봅시다.이 그림에서 $z_t$는 실제 관측값을 의미합니다. 하지만 $z_{t}^{}$는 자동차가 생각하는 관측값입니다. 자동차는 자신의 위치 $x_t$가 정확히 어디인지 모르기에 관측값을 토대로 자신의 위치를 찾아냅니다. 따라서 $x_t$는 자동차가 생각하는 자기 위치, 즉 *기대 위치라고 할 수 있습니다. 그림에서 자동차는 자신의 기대 위치 $x_t$를 20m 지점이라고 생각하고 있습니다. 이 기대 위치 $x_t$와 지도를 비교하여 landmark까지의 기대 관측값을 찾아냅니다. 이 기대 관측값 $z_{t}^{}$와 실제 관측값 $z_t$를 비교하여 관측이 맞을 확률을 도출합니다.(이부분 맞는지 모르겠음) 이를 통해서 알아낼 수 있는 점은 이 과정에서 $u_{1:t}$ 항을 전혀 사용하지 않은 것입니다. 게다가 관측은 Motion 이후에 시도하므로, $u_{1:t}$은 생략할 수 있는 항이 됩니다.이제 Observation 모델만 따로 떼어 생각해 봅시다.$\\large{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)}$우리는 위의 과정을 통해 $u_{1:t}$ 항을 생략할 수 있다는 사실을 알았습니다. 다음 그림을 참고합시다.위의 그림의 그래프를 보면 $z_{1:t-1}$ 항은 이미 $z_t$ 항을 구성하는 데 반영되어 있습니다. 그렇다면 Markov Assumption에 의해 생략할 수 있습니다. 따라서 생략가능한 항을 생략한 Observation 모델은 다음과 같습니다.$\\large{p(z_t|x_t, z_{1:t-1}, u_{1:t}, m)}$$\\large{\\rightarrow p(z_t|x_t, m)}$아주아주 짧아졌습니다. 이제 다시 이 결과를 베이즈 정리를 적용한 $bel(x_t)$의 식에 대입해 봅시다.Q. 자동차는 어떻게 자신의 기대 위치의 확률을 찾는 과정에서 기대 위치를 알고 있을까요?A. 이 과정은 $p(x_t)$를 찾는 과정이지 $x_t$를 찾는 과정이 아닙니다. 따라서 $x_t$는 이미 알고 있다고 가정하고 이 과정을 수행하는 것입니다.*기대 위치: 이 단어는 제 게시글에서만 이해를 돕기 위해 사용하는 말이고 전문적으로 사용되는 용어는 아닙니다. 다른 곳에서 사용은 자제해주시길 바랍니다…Markov Localization의 결과모든 정리를 끝낸 후의 결과는 다음과 같습니다.$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) = \\eta \\; \\underbrace{p(z_t|x_t, m)}_{\\text{Observation Model}} \\; \\underbrace{\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m) \\; bel(x_{t-1}) \\; dx_{t-1}}_{\\text{Motion Model}}}$하지만 보통 이 공식은 알고리즘으로 많이 사용하기에, Motion 모델을 $\\hat(bel)(x_{t})$로 대체하고, 앞선 과정에서 미리 계산합니다. 대체한 공식은 다음과 같습니다.$\\large{\\widehat{bel}(x_{t})=\\int_{x_{t-1}}^{}p(x_t|x_{t-1}, u_{t}, m) \\; bel(x_{t-1}) \\; dx_{t-1}}$$\\large{p(x_t|z_t, z_{1:t-1}, u_{1:t}, m) = \\eta \\; \\underbrace{p(z_t|x_t, m)}_{\\text{Observation Model}} \\; \\underbrace{\\widehat{bel}(x_{t})}_{\\text{Motion Model}}}$알고리즘은 다음과 같습니다.이 필터는 베이즈 정리를 사용하였기 때문에 Bayesian Filter Localization이라고도 합니다. 특징은 재귀 형태로 사전확률과 조건으로 사후확률을 구하고, 다시 구한 사후확률을 사전확률로 사용하는 방식으로 확률을 업데이트합니다. 또한 앞으로 공부할 칼만 필터, 파티클 필터 등 많은 필터의 base가 되는 필터입니다.참고 사이트Udacity Self-driving car nanodegree - Markov Localization(링크 공유 불가능)준이 블로그 - [확률과 통계] 13. 전체 확률의 법칙, Law of Total Probability[SLAM] Bayes filter(베이즈 필터)"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (10) - 패키지 생성",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-pkg.html",
      "date"        : "2021-01-16 12:28:23 +0900",
      "description" : "",
      "content"     : "패키지란?패키지는 ROS에서 실행될 코드를 담고 있는 공간입니다. 앞서 올린 작업공간 게시글에서 소스 파일과 비슷한 것이라 하였는데, 그것은 이 패키지가 사실상 코어 역할을 하고 있기 때문입니다. Visual Studio로 비유하자면 Hello world!를 출력하는 프로그램을 C언어로 작성했다고 합시다. 프로젝트(dev_ws)을 만들고 그 안의 소스 파일 폴더(src)안에 helloworld.cpp 소스 파일(패키지)을 만들면 결국 Hello world! 출력은 helloworld.cpp 파일이 하는 것입니다. 작업공간과 패키지 역시 같습니다. 작업공간도 패키지가 돌아갈 환경을 마련해 줄 뿐, 실제 operation은 패키지가 다 합니다. 따라서 어떤 부품을 사용하려 할때 그 부품에 맞는 패키지를 다운받아 사용하면, 우리가 직접적으로 코드를 짜는 일이 현저히 줄어듭니다. 이 Tutorial 게시글에는 간단한 예시로 패키지를 체험해 볼 뿐이지만, 직접 사용할 때는 디바이스와 연동하여 사용할 것입니다.패키지 사용방법1. 패키지 생성패키지를 생성하는 방법은 다음과 같습니다. 사용할 언어에 따라서 명령어가 조금 달라집니다.C++(cmake):ros2 pkg create --build-type ament_cmake &lt;패키지 이름&gt;Python:ros2 pkg create --build-type ament_python &lt;패키지 이름&gt;패키지는 src 폴더에 저장되어야 하므로 우선 src 폴더로 이동한 후 명령어를 실행해 줍시다. 저는 파이썬을 기준으로 예시를 들겠습니다.$ cd ~/dev_ws/src$ ros2 pkg create --build-type ament_python --node-name my_node my_packagesrc 폴더를 보면 my_package라는 폴더가 생성되었을 것입니다.2. colcon build패키지를 작업공간에서 사용하기 위해서 colcon build 명령어로 빌드해 줍시다.$ colcon build또는 colcon build의 --packages-select 명령어를 사용하여 지정한 패키지만 빌드할 수도 있습니다.$ colcon build --packages-select my_package3. colcon ws setup.bash 파일 소싱작업공간의 setup.bash 파일을 소싱해 줍시다. 이 작업은 오버레이(덧씌우기)하여 가장 최신 작업 상태의 작업공간을 적용하는 과정입니다.$ . install/setup.bash작업공간 내의 파일을 수정했다면 그때마다 실행해 줍시다.4. 노드 실행해보기ROS2의 패키지는 기본적으로 테스트 노드가 같이 생성됩니다. 이미 패키지 생성&amp;빌드를 완료했으니 테스트 노드를 실행해 봅시다.$ ros2 run my_package my_node실행결과는 다음과 같습니다.Hi from my_package.언어에 따른 패키지 폴더 살펴보기위에서 말했듯 ROS2는 C++과 파이썬 두 언어로 빌드할 수 있습니다. 위의 예시는 파이썬을 실행한 결과입니다. 각각의 경우 ros2 pkg create가 생성해 주는 패키지의 생김새를 살펴봅시다.1. C++(Cmake 빌드)dev_ws/src/my_package 폴더의 내용물은 이렇습니다.$ cd dev_ws/src/my_package$ lsCMakeLists.txt include package.xml srcC++로 짠 노드 파일은 src 폴더 안에 저장됩니다. 실제로 기본적으로 생성된 테스트 노드가 들어 있습니다.2. Python마찬가지로 위의 명령어를 실행하면 내용물은 다음과 같습니다.$ cd dev_ws/src/my_package$ lsmy_package package.xml resource setup.cfg setup.py testPython으로 짠 노드 파일은 my_package 폴더 안에 저장됩니다. C++과는 다르게 노드 파일 폴더의 이름이 패키지 이름과 같습니다.package.xml 파일package.xml 파일에는 패키지에 대한 간단한 설명과 의존성 정보가 들어 있습니다. 설명은 패키지 실행에 아무 영향을 주지 않지만 정보를 위해 작성합니다. 각 언어에 따라서 package.xml의 생김새가 다릅니다.1. C++(Cmake 빌드)C++패키지의 package.xml 내용은 다음과 같습니다.&lt;?xml version=\"1.0\"?&gt;&lt;?xml-model href=\"http://download.ros.org/schema/package_format3.xsd\" schematypens=\"http://www.w3.org/2001/XMLSchema\"?&gt;&lt;package format=\"3\"&gt; &lt;name&gt;my_package&lt;/name&gt; &lt;version&gt;0.0.0&lt;/version&gt; &lt;description&gt;TODO: Package description&lt;/description&gt; &lt;maintainer email=\"user@todo.todo\"&gt;user&lt;/maintainer&gt; &lt;license&gt;TODO: License declaration&lt;/license&gt; &lt;buildtool_depend&gt;ament_cmake&lt;/buildtool_depend&gt; &lt;test_depend&gt;ament_lint_auto&lt;/test_depend&gt; &lt;test_depend&gt;ament_lint_common&lt;/test_depend&gt; &lt;export&gt; &lt;build_type&gt;ament_cmake&lt;/build_type&gt; &lt;/export&gt;&lt;/package&gt;초반부는 패키지의 내용을 설명하는 부분입니다. 패키지의 이름이나, 작성자, 설명, 이메일, 라이센스의 정보를 담을 수 있습니다. 내맘대로 수정해 봅시다. &lt;name&gt;my_package&lt;/name&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;description&gt;Beginner client libraries tutorials practice package&lt;/description&gt; &lt;maintainer email=\"bhbhchoi@gmail.com\"&gt;user&lt;/maintainer&gt; &lt;license&gt;Apache License 2.0&lt;/license&gt;다음은 의존성을 담당하는 부분입니다. &lt;buildtool_depend&gt;ament_cmake&lt;/buildtool_depend&gt; &lt;test_depend&gt;ament_lint_auto&lt;/test_depend&gt; &lt;test_depend&gt;ament_lint_common&lt;/test_depend&gt; &lt;export&gt; &lt;build_type&gt;ament_cmake&lt;/build_type&gt; &lt;/export&gt;의존성을 추가하면 이곳에 같이 추가해 줘야 합니다.2. PythonPython 패키지의 package.xml 파일은 다음과 같습니다.&lt;?xml version=\"1.0\"?&gt;&lt;?xml-model href=\"http://download.ros.org/schema/package_format3.xsd\" schematypens=\"http://www.w3.org/2001/XMLSchema\"?&gt;&lt;package format=\"3\"&gt; &lt;name&gt;my_package&lt;/name&gt; &lt;version&gt;0.0.0&lt;/version&gt; &lt;description&gt;TODO: Package description&lt;/description&gt; &lt;maintainer email=\"user@todo.todo\"&gt;user&lt;/maintainer&gt; &lt;license&gt;TODO: License declaration&lt;/license&gt; &lt;buildtool_depend&gt;ament_python&lt;/buildtool_depend&gt; &lt;test_depend&gt;ament_copyright&lt;/test_depend&gt; &lt;test_depend&gt;ament_flake8&lt;/test_depend&gt; &lt;test_depend&gt;ament_pep257&lt;/test_depend&gt; &lt;test_depend&gt;python3-pytest&lt;/test_depend&gt; &lt;export&gt; &lt;build_type&gt;ament_python&lt;/build_type&gt; &lt;/export&gt;&lt;/package&gt;C++의 package.xml 파일과 거의 비슷하지만 의존성 부분이 조금 다릅니다. 패키지 내용과 의존성 추가 편집 방법 역시 C++ 패키지와 같습니다. 하지만 Python 패키지에서는 package.xml 파일을 수정할 때 고쳐줘야 할 파일이 하나 더 있습니다. 바로 setup.py 파일입니다. setup.py 파일의 내용은 다음과 같습니다.from setuptools import setuppackage_name = 'my_py_pkg'setup( name=package_name, version='0.0.0', packages=[package_name], data_files=[ ('share/ament_index/resource_index/packages', ['resource/' + package_name]), ('share/' + package_name, ['package.xml']), ], install_requires=['setuptools'], zip_safe=True, maintainer='TODO', maintainer_email='TODO', description='TODO: Package description', license='TODO: License declaration', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'my_node = my_py_pkg.my_node:main' ], },)이 파이썬 코드에서 16~19줄을 package.xml 파일의 내용에 맞게 고치면 됩니다. maintainer='bhbhchoi', maintainer_email='bhbhchoi@gmail.com', description='Beginner client libraries tutorials practice package', license='Apache License 2.0',이런 식으로 말이죠. 이미 maintainer와 maintainer_email은 본인에 맞게 되어 있을 것입니다.마무리다음 시간에는 퍼블리셔&amp;서브스크라이버 노드를 직접 작성해 보겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 패키지 생성편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (9) - 작업공간 생성",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-ws.html",
      "date"        : "2021-01-15 10:12:23 +0900",
      "description" : "",
      "content"     : "작업공간이란?작업공간은 ROS 패키지를 운용할 수 있는 디렉토리를 말합니다. 사용자는 이 작업공간 내부에 패키지를 생성하여 ROS를 사용할 수 있습니다. 마치 Visual Studio에서 프로젝트를 만들고 그 안의 소스 파일 폴더에 소스 파일을 생성하여 프로그래밍 언어를 돌리듯이, ROS에서는 작업공간을 만들어 그 안의 src(source의 줄임말) 폴더에 패키지 폴더를 생성하여 패키지를 돌리는 느낌입니다.ROS를 사용하기 위해서 터미널에는 source /opt/ros/&lt;distro&gt;/setup.bash명령어가 실행되어 있어야 합니다. &lt;distro&gt;에는 ROS2 버전 이름을 적습니다. 저는 dashing 버전을 사용하고 있기에 source /opt/ros/dashing/setup.bash 명령어를 사용하였습니다. bashrc 파일에 넣어두면 터미널을 켤 때마다 실행해 줍니다. 이 방법은 ROS2 설치 게시글에 작성되어 있습니다.1. 작업공간 디렉토리 만들기우선 작업공간 디렉토리를 만들어 줍시다. 일반적으로 이름은 workspace의 줄임말인 작업공간 이름_ws로 짓습니다. Tutorial 사이트를 참고해서 dev_ws로 만들어 줍시다.$ mkdir -p ~/dev_ws/srcdev_ws라는 이름의 작업공간을 생성하고 그 내부에 패키지가 있을 폴더인 src 폴더를 만들어 주었습니다.2. 샘플 패키지 저장이제 패키지를 실행해 봅시다. 참고 사이트에서 준비한 패키지를 저장합니다. 패키지는 반드시 src 폴더 안에 저장합니다. 이 명령어의 &lt;distro&gt; 역시 자신에게 맞는 버전명으로 바꿔줍시다.$ cd ~/dev_ws/src$ git clone https://github.com/ros/ros_tutorials.git -b &lt;distro&gt;-devel3. colcon buildcolcon은 작업공간을 빌드해주는 명령어입니다. ROS1에서 catkin과 비슷한 역할을 담당합니다. 한번 빌드해 봅시다.$ colcon build몇초 기다리면 빌드가 다 되었다고 뜰 것입니다.설치가 되어있지 않다고 뜰 때사실 이 블로그의 ROS2 설치 게시글에는 colcon 설치에 대한 내용이 없습니다. 다음 명령어로 설치해 줍시다.$ sudo apt update$ sudo apt install python3-colcon-common-extensions참고 사이트ROS Index-ROS2 튜토리얼 작업공간 생성편colcon Installation"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (8) - rosbag",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-bag.html",
      "date"        : "2021-01-13 11:23:23 +0900",
      "description" : "",
      "content"     : "rosbag이란?ROS에서 실행되고 있는 토픽을 녹화하는 명령어입니다. 녹화된 파일은 나중에 같은 환경을 조성할 수 있습니다. 예를 들어 꼬부기와 라이다를 이용한 맵핑을 하려 할때, 우선 꼬부기와 라이다 노드만 켜놓고 rosbag으로 퍼블리시되는 토픽 내용을 녹화합니다. 그 후 녹화된 파일을 재생하고 맵핑 노드를 켜면 녹화된 토픽으로 맵을 만들 수 있습니다.ros2 bag Prerequisitesros2 bag이 설치되지 않았다면 다음 명령어를 실행하여 설치합니다.$ sudo apt-get install ros-dashing-ros2bag ros-dashing-rosbag2-converter-default-plugins ros-dashing-rosbag2-storage-default-plugins설치한 후에 bag 파일을 저장할 폴더를 만듭니다.$ mkdir bag_files$ cd bag_filesros2 bag 실습을 위해서 터틀심을 실행합니다.$ ros2 run turtlesim turtlesim_node$ ros2 run turtlesim turtle_teleop_keyROS2에서의 bag기본적으로 ROS1에서의 rosbag 과 사용법이 비슷합니다. 지금까지도 그래왔듯 rosbag을 ros2 bag으로 고치면 됩니다.ros2 bag recordrecord 명령어를 사용하는 방법은 3가지 정도가 있습니다. 하나만 여러개 모두1. 하나만먼저 하나만 record 하는 명령어는 다음과 같습니다. 지정된 토픽 하나만 녹화합니다.ros2 bag record &lt;토픽 이름&gt;터틀심 예시는 다음과 같습니다.$ ros2 bag record /turtle1/cmd_vel[INFO] [rosbag2_storage]: Opened database 'rosbag2_2019_10_11-05_18_45'.[INFO] [rosbag2_transport]: Listening for topics...[INFO] [rosbag2_transport]: Subscribed to topic '/turtle1/cmd_vel'[INFO] [rosbag2_transport]: All requested topics are subscribed. Stopping discovery...지정된 /turtle1/cmd_vel 토픽만 녹화합니다.2. 여러개사실 여러개 녹화도 마찬가지입니다. 뒤에 녹화하고 싶은 토픽을 더 적어주면 됩니다.예시를 들어보겠습니다.$ ros2 bag record -o subset /turtle1/cmd_vel /turtle1/pose[INFO] [rosbag2_storage]: Opened database 'subset'.[INFO] [rosbag2_transport]: Listening for topics...[INFO] [rosbag2_transport]: Subscribed to topic '/turtle1/cmd_vel'[INFO] [rosbag2_transport]: Subscribed to topic '/turtle1/pose'[INFO] [rosbag2_transport]: All requested topics are subscribed. Stopping discovery...여기서 -o 명령어는 bag 파일에 커스텀 네임을 주기 위해서입니다. 기본적으로 bag파일은 년-월-일-시-분-초.bag의 이름으로 저장됩니다. 하지만 -o &lt;나만의 이름&gt; 명령어를 추가하여 필요한 이름으로 저장할 수 있습니다. 위의 명령어는 subset.bag 이라는 파일명으로 저장될 것입니다.3. 모두현재 실행되고 있는 모든 토픽을 녹화합니다. 하지만 이 명령어는 circular dependency의 발생이나 시스템에 문제가 생길 수 있기 때문에 왠만하면 토픽 지정 녹화를 사용하기를 권장합니다. 사용 방법은 다음과 같습니다.ros2 bag record -aros2 bag infobag 파일 정보를 볼 수 있습니다. 사용 방법은 다음과 같습니다.ros2 bag info &lt;bag 파일 이름&gt;예시로서 위에서 생성했던 subset.bag 파일의 info를 봅시다.$ ros2 bag info &lt;subset.bag&gt;Files: subset.db3Bag size: 228.5 KiBStorage id: sqlite3Duration: 48.47sStart: Oct 11 2019 06:09:09.12 (1570799349.12)End Oct 11 2019 06:09:57.60 (1570799397.60)Messages: 3013Topic information: Topic: /turtle1/cmd_vel | Type: geometry_msgs/msg/Twist | Count: 9 | Serialization Format: cdr Topic: /turtle1/pose | Type: turtlesim/msg/Pose | Count: 3004 | Serialization Format: cdrros2 bag play지정한 bag 파일을 재생합니다. 사용 방법은 다음과 같습니다.ros2 bag play &lt;bag 파일 이름&gt;예시로 위에서 생성했던 subset.bag 파일을 재생해 봅시다.$ ros2 bag play subset[INFO] [rosbag2_storage]: Opened database 'subset'.터틀심이 이전에 녹화했던 경로대로 움직입니다. {: width:”70%” height:”70%”}게다가 bag 파일이 재생되고 있을 때 토픽도 생성되므로 ros2 topic echo 명령어를 이용하여 토픽 내용도 볼 수 있습니다.마무리ROS2 beginner 첫번째 파트의 내용이 마무리되어 갑니다. 이제 workspace와 package, node를 직접 만들어보는 내용을 공부할 예정입니다.참고 사이트ROS Index-ROS2 튜토리얼 bag편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (7) - launch",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-launch.html",
      "date"        : "2021-01-11 17:13:23 +0900",
      "description" : "",
      "content"     : "launch란 뭘하는 것일까지금까지는 노드를 한땀한땀 새 터미널에서 실행했습니다. 하지만 이제 launch 파일을 사용하면 여러 노드를 한번에 실행할 수 있습니다. 파이썬 쉘에서 한줄씩 실행하다가 스크립트로 만들어버리는 느낌이랄까요? 한번 알아보도록 합시다.ROS2에서의 launch이번에는 ros를 ros2를 만들면 됩니다~~~말고도 할 말이 많습니다. 물론 roslaunch도 ros2 launch가 된건 맞지만 새로운 launch 파일 작성법이 생겼습니다. 바로 파이썬을 이용한 launch파일 작성입니다. 원래는 html 문법이었는데 배우기 쉽고 많이 사용하는 파이썬으로 이주를 시도하는 모습입니다. 자세한 설명은 밑에서 하고 터틀심을 실행해 줍시다.launch 파일 작성방법1. launch 파이썬 파일 생성우선 launch 폴더를 만들고 그 안에 launch 내용을 담을 파이썬 파일을 만들어 줍니다.$ mkdir launch$ cd launch$ touch turtlesim_mimic_launch.py파이썬 코드를 담을 파일이 만들어졌으니 열어서 작성해 줍시다. 여기선 지금까지와는 다르게 버전의 경계가 Eloquent네요. 지금까지는 dashing이었는데… 아무튼 일단 버전에 맞는 소스를 붙여줍니다.ROS2 Foxy and newer:from launch import LaunchDescriptionfrom launch_ros.actions import Nodedef generate_launch_description(): return LaunchDescription([ Node( package='turtlesim', namespace='turtlesim1', executable='turtlesim_node', name='sim' ), Node( package='turtlesim', namespace='turtlesim2', executable='turtlesim_node', name='sim' ), Node( package='turtlesim', executable='mimic', name='mimic', remappings=[ ('/input/pose', '/turtlesim1/turtle1/pose'), ('/output/cmd_vel', '/turtlesim2/turtle1/cmd_vel'), ] ) ])ROS2 Eloquent and older:from launch import LaunchDescriptionfrom launch_ros.actions import Nodedef generate_launch_description(): return LaunchDescription([ Node( package='turtlesim', node_namespace='turtlesim1', node_executable='turtlesim_node', node_name='sim' ), Node( package='turtlesim', node_namespace='turtlesim2', node_executable='turtlesim_node', node_name='sim' ), Node( package='turtlesim', node_executable='mimic', node_name='mimic', remappings=[ ('/input/pose', '/turtlesim1/turtle1/pose'), ('/output/cmd_vel', '/turtlesim2/turtle1/cmd_vel'), ] ) ])저는 dashing이므로 Eloquent and newer 버전의 소스를 사용했습니다.2. launch 파일 소스 분석launch 파이썬 소스에서는 launch 모듈을 사용합니다.from launch import LaunchDescriptionfrom launch_ros.actions import Nodegenerate_launch_description 함수는 launch파일 본문이라고 보면 될 듯 합니다. C언어의 main문과는 조금 다르지만 느낌적으로 비슷하기도 한 듯 합니다.def generate_launch_description(): return LaunchDescription([ ])launch 파일의 본문은 실행할 노드들입니다. 필요한 정보는 다음의 4가지입니다. (노드가 포함된) 패키지 이름 namespace 실행할 노드 이름 실행 중일때 쓸 노드 이름이 설명만 보면 이해가 잘 안될테니 조금 이따 실행 결과와 함께 ros2 node list를 실행해 보겠습니다.또다시 버전 분기점입니다. generate_launch_description 함수의 내용물 표현 방식 역시 Eloquent를 기점으로 바뀝니다. ROS2 Foxy and newer: Node( package='turtlesim', namespace='turtlesim1', executable='turtlesim_node', name='sim'),Node( package='turtlesim', namespace='turtlesim2', executable='turtlesim_node', name='sim'), ROS2 Eloquent and older: Node( package='turtlesim', node_namespace='turtlesim1', node_executable='turtlesim_node', node_name='sim'),Node( package='turtlesim', node_namespace='turtlesim2', node_executable='turtlesim_node', node_name='sim'), 2-1. launch remapping remapping은 다른 노드에서 실행되고 있는 토픽을 베껴오는 것입니다. launch 파일에서도 사용할 수 있습니다. 터틀심 패키지의 mimic 노드를 사용해 봅시다.ROS2 Foxy and newer: Node( package='turtlesim', executable='mimic', name='mimic', remappings=[ ('/input/pose', '/turtlesim1/turtle1/pose'), ('/output/cmd_vel', '/turtlesim2/turtle1/cmd_vel'), ]) ROS2 Eloquent and older: Node( package='turtlesim', node_executable='mimic', node_name='mimic', remappings=[ ('/input/pose', '/turtlesim1/turtle1/pose'), ('/output/cmd_vel', '/turtlesim2/turtle1/cmd_vel'), ]) 다른 노드들의 토픽들을 따라하게 하는 노드입니다. 3. ros2 launch이제 ros2 launch 명령어로 실행해 봅시다. 파이썬 launch 파일은 그 파일이 있는 디렉토리에서 실행해 줍시다.$ cd launch$ ros2 launch turtlesim_mimic_launch.py[INFO] [launch]: All log files can be found below /home/bhbhchoi/.ros/log/2021-01-13-01-29-43-022804-bhbhchoi-900X3L-19273[INFO] [launch]: Default logging verbosity is set to INFO[INFO] [turtlesim_node-1]: process started with pid [19286][INFO] [turtlesim_node-2]: process started with pid [19287][INFO] [mimic-3]: process started with pid [19288]같은 터틀심 창이 두개 열립니다.mimic은 단지 토픽을 복사해 주는 역할인 듯 합니다.다른 터미널을 열어 다음의 명령어를 실행해 봅시다.$ ros2 topic pub -r 1 /turtlesim1/turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: -1.8}}\"터틀심 두개가 똑같이 움직이는 것을 볼 수 있습니다.이제 launch 파일은 파이썬으로 작성해야 하나요? 원래 형식으로는 못쓰나요?아닙니다. 패키지에 들어있는 launch 파일은 여전히(?) ros2 launch &lt;패키지 이름&gt; &lt;launch 파일 이름&gt; 으로 실행합니다. 아직 ROS2 패키지를 배우지 않았으니 나중에 배우도록 합시다.마무리launch 파일의 새로운 면을 볼 수 있어 좋았습니다. ROS1과 바뀐 부분이 조금씩 보이기 시작하네요. 다음 게시글은 rosbag 기능에 대해서 정리하겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 launch편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (6) - 파라미터",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-param.html",
      "date"        : "2021-01-11 15:40:23 +0900",
      "description" : "",
      "content"     : "파라미터란??노드를 실행하기 위해서 주어져야 할 값입니다. 환경설정값이라고도 할 수 있습니다. 함수를 만들 때 괄호 안에 들어가는 매개 변수와 같은 개념입니다.ROS2에서의 rosparamROS2에서의 rosparam 명령어에 해당하는 것은 ros2 param입니다. 이젠 그냥 안적고 시작해도 어련히 알고 있겠군요. 오늘도 역시 터틀심 패키지로 예시를 봅시다. 실행합시다.$ ros2 run turtlesim turtlesim_node$ ros2 run turtlesim turtle_teleop_keyros2 param list현재 실행되고 있는 노드들의 파라미터 목록입니다. list 명령어는 이제 몇번 해봐서 알 수 있으실 겁니다.$ ros2 param list/teleop_turtle: scale_angular scale_linear use_sim_time/turtlesim: background_b background_g background_r use_sim_time여기서 use_sim_time 파라미터는 터틀심만이 아니라 모든 노드에 존재합니다.이 파라미터들의 이름을 통해 무슨 역할을 하는 파라미터인지 추측할 수 있습니다. 예를 들어 /turtlesim 노드의 세 파라미터는 배경색의 rgb를 조정하는 역할인 것으로 추측할 수 있습니다. 이들의 값을 알기 위해서는 ros2 param get 명령어를 사용합니다.ros2 param get지정된 파라미터값을 보여주는 명령어입니다. 사용 방법은 다음과 같습니다.ros2 param get &lt;노드 이름&gt; &lt;파라미터 이름&gt;예시로 터틀심 파라미터를 한번 보겠습니다. /turtlesim 노드의 background_g 파라미터를 봅시다.$ ros2 param get /turtlesim background_gInteger value is: 86배경색의 g성분이 86인 것을 확인할 수 있습니다. 이 값으로 파라미터의 자료형도 추측할 수 있습니다.ros2 param set지정된 파라미터값을 변경할 수 있는 명령어입니다. ros2 topic pub 명령어와 비슷합니다. 사용 방법은 다음과 같습니다.ros2 param set &lt;노드 이름&gt; &lt;파라미터 이름&gt; &lt;설정할 값&gt;터틀심 파라미터를 한번 변경해 보겠습니다.$ ros2 param set /turtlesim background_r 150Set parameter successfulbackground_r 값을 150으로 변경하여 보라색으로 바꿔 보았습니다.ros2 param dump (Eloquent)지정한 노드에서 실행되고 있는 파라미터들을 yaml 파일로 저장합니다. 사용 방법은 다음과 같습니다.ros2 param dump &lt;노드 이름&gt;하지만 이 명령어는 ROS2 Eloquent 이상부터 지원합니다. 이론적으로나마 정리해 보겠습니다.터틀심 노드의 파라미터들을 저장해 봅시다.$ ros2 param dump /turtlesimSaving to: ./turtlesim.yaml저장된 ./turtlesim.yaml파일의 내용은 다음과 같습니다.turtlesim: ros__parameters: background_b: 255 background_g: 86 background_r: 150 use_sim_time: false이 파일은 파라미터를 한창 조정하다가 저장해 놓을때 유용하게 사용 가능합니다.파라미터 파일 로드 (Eloquent)저장한 파라미터 파일을 불러오는 기능입니다. 명령어라기 보다는 ros2 run에 딸린 기능입니다. 사용 방법은 다음과 같습니다.ROS Eloquent 이상:ros2 run &lt;패키지 이름&gt; &lt;실행할 노드 이름&gt; --ros-args --params-file &lt;파일 이름&gt;ROS2 Dashing:ros2 run &lt;패키지 이름&gt; &lt;실행할 노드 이름&gt; __params:=&lt;파일 이름&gt;터틀심 노드로 예시를 들어 보겠습니다.ROS Eloquent 이상:$ ros2 run turtlesim turtlesim_node --ros-args --params-file ./turtlesim.yamlROS2 Dashing:ros2 run turtlesim turtlesim_node __params:=./turtlesim.yaml실제로 터미널에서 실행은 못해보았지만 위의 명령어를 실행하면 조정한 파라미터 값이 적용될 것입니다.마무리beginner step에서 해볼 내용이 거의 마무리되어 갑니다. 다음 게시글에서는 ROS2에서의 launch에 대해서 정리하겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 파라미터편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (5) - 서비스",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-service.html",
      "date"        : "2021-01-10 22:40:00 +0900",
      "description" : "",
      "content"     : "서비스란??ROS에서 사용하는 토픽과는 다른 통신 방법입니다. 토픽을 사용하는 퍼블리셔-서브스크라이버와는 다르게 요청(request)과 응답(response)으로 이루어집니다. 서비스는 pub-sub 대신 서버-클라이언트 노드가 있습니다. 클라이언트 노드에서 요청을 보내면, 서버 노드에서 요청에 맞는 응답(데이터)를 클라이언트로 전송합니다. 이는 1대 1전송이기 때문에 광범위하게 뿌리는 퍼블리셔와는 다르게 지정된 노드에만 데이터를 주고받습니다. 하나씩 하나씩 서비스는 주고받는 데이터 형식을 srv 파일에 저장해두었습니다. 마치 토픽에서의 msg 파일처럼요.ROS2에서의 rosserviceROS2에서 rosservice 명령어에 해당하는 것은 ros2 service입니다. 이 정도쯤은 예상하셨죠?오늘도 예시를 도와줄 터틀심 선생님을 실행합시다.$ ros2 run turtlesim turtlesim_node$ ros2 run turtlesim turtle_teleop_keyros2 service list현재 실행중인 서비스 목록들을 볼 수 있습니다. ros2 topic list와 똑같네요.$ ros2 service list/clear/kill/reset/spawn/teleop_turtle/describe_parameters/teleop_turtle/get_parameter_types/teleop_turtle/get_parameters/teleop_turtle/list_parameters/teleop_turtle/set_parameters/teleop_turtle/set_parameters_atomically/turtle1/set_pen/turtle1/teleport_absolute/turtle1/teleport_relative/turtlesim/describe_parameters/turtlesim/get_parameter_types/turtlesim/get_parameters/turtlesim/list_parameters/turtlesim/set_parameters/turtlesim/set_parameters_atomicallyros2 service type (Eloquent)지정한 서비스의 srv type을 보여주는 명령어입니다. 사용 방법은 다음과 같습니다.ros2 service type &lt;서비스 이름&gt;하지만 이 명령어는 ROS2 Eloquent 버전부터 사용할 수 있다고 합니다. 저는 dashing 버전이라서 아쉽게도 실행할 수 없었습니다.하지만 참고한 사이트에 따르면 다음과 같은 결과가 나온다고 합니다.$ ros2 service type /clearstd_srvs/srv/EmptyEmpty는 요청을 할 때 서비스 호출이 데이터를 전송하지 않고 응답을 받을 때 데이터를 수신하지 않음을 의미합니다.불행히도 몇몇 명령어는 Eloquent 버전부터 지원한다고 합니다. 그런 명령어는 소제목 옆에 (Eloquent)라고 적어 두겠습니다.ros2 sevice list -t서비스 리스트와 함께 type을 보여주는 명령어입니다. 다행히 이 명령어는 dashing에서도 문제없이 작동합니다.$ ros2 service list -t/clear [std_srvs/srv/Empty]/kill [turtlesim/srv/Kill]/reset [std_srvs/srv/Empty]/spawn [turtlesim/srv/Spawn]/teleop_turtle/describe_parameters [rcl_interfaces/srv/DescribeParameters]/teleop_turtle/get_parameter_types [rcl_interfaces/srv/GetParameterTypes]/teleop_turtle/get_parameters [rcl_interfaces/srv/GetParameters]/teleop_turtle/list_parameters [rcl_interfaces/srv/ListParameters]/teleop_turtle/set_parameters [rcl_interfaces/srv/SetParameters]/teleop_turtle/set_parameters_atomically [rcl_interfaces/srv/SetParametersAtomically]/turtle1/set_pen [turtlesim/srv/SetPen]/turtle1/teleport_absolute [turtlesim/srv/TeleportAbsolute]/turtle1/teleport_relative [turtlesim/srv/TeleportRelative]/turtlesim/describe_parameters [rcl_interfaces/srv/DescribeParameters]/turtlesim/get_parameter_types [rcl_interfaces/srv/GetParameterTypes]/turtlesim/get_parameters [rcl_interfaces/srv/GetParameters]/turtlesim/list_parameters [rcl_interfaces/srv/ListParameters]/turtlesim/set_parameters [rcl_interfaces/srv/SetParameters]/turtlesim/set_parameters_atomically [rcl_interfaces/srv/SetParametersAtomically]ros2 service find (Eloquent)타입 이름으로 서비스를 찾을 수 있는 명령어입니다. 사용 방법은 다음과 같습니다.ros2 service find &lt;타입 이름&gt;예를 들어 turtlesim/srv/Spawn의 type을 가진 서비스를 찾고자 한다면 다음과 같이 실행하면 됩니다.$ ros2 service find turtlesim/srv/Spawn/spawn아쉽게도 dashing 버전에서는 사용할 수 없습니다.ros2 interface show토픽 편에서도 설명했었던 명령어입니다. Eloquent 이상의 버전에서 사용법은 똑같습니다. dashing에서의 사용법은 토픽과 조금 다릅니다. 사용 방법은 다음과 같습니다.ROS2 dashing:ros2 srv show &lt;타입 이름&gt;/spawn 서비스의 타입입니다. 출력 내용은 다음과 같습니다.$ ros2 srv show turtlesim/srv/Spawnfloat32 xfloat32 yfloat32 thetastring name # Optional. A unique name will be created and returned if this is empty---string name이 출력 결과는 srv 파일의 내용과 일치합니다. 요청으로 위의 x, y, theta(, name)를 받으면, 응답으로 name을 발신하는 것입니다.ros2 service call서비스에 직접 명령을 주는 명령어입니다. ros2 topic pub와 비슷한 역할을 합니다. 사용 방법은 다음과 같습니다.ros2 service call &lt;service_name&gt; &lt;service_type&gt; &lt;arguments&gt;터틀심의 /spawn 명령어를 한번 사용해 봅시다. 이름처럼 거북이를 하나 더 만드는 서비스입니다.$ ros2 service call /spawn turtlesim/srv/Spawn \"{x: 2, y: 2, theme: ''}\"waiting for service to become available...requester: making request: turtlesim.srv.Spawn_Request(x=2.0, y=2.0, theta=0.2, name='')response:turtlesim.srv.Spawn_Response(name='turtle2') 한마리 더 소환! 마무리블로그 작성하기 전에는 무지 많은 줄 알았는데 적고 나니 약 4개, 그 중 쓸 수 있는건 2개밖에 없었습니다. 아직 ROS1 공부가 부족한듯 합니다. 다음 게시글은 파라미터에 대해서 공부해 보도록 하겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 서비스편"
    } ,
  
    {
      "title"       : "Raspberry Pi Camera Calibration",
      "category"    : "",
      "tags"        : "Raspberry Pi, Camera Calibration",
      "url"         : "./raspi-camera-cali.html",
      "date"        : "2021-01-10 18:35:00 +0900",
      "description" : "",
      "content"     : "raspi camera로 camera calibration하기이 게시글은 라즈베리파이 카메라로 캘리브레이션을 하는 내용입니다.순서는 다음과 같습니다. raspicam_node 설치 camera calibration Troubleshooting1. raspicam_node 설치ROS로 camera calibration을 하려면 우선 /camera/image 토픽과 /camera 토픽이 필요합니다. 따라서 해당 카메라의 토픽을 실행시킬 수 있는 패키지를 설치해야 합니다.설치방법은 여기에 나와 있지만 정리해보도록 합시다.git clone 명령어를 통해 catkin_ws/src 폴더에 패키지를 다운받습니다.$ cd catkin_ws/src$ git clone https://github.com/UbiquityRobotics/raspicam_node.gitROS에 인식되지 않는 의존성 몇 가지를 위해 다음 명령어를 실행해 줍니다.$ sudo gedit /etc/ros/rosdep/sources.list.d/30-ubiquity.list열린 30-ubiquity.list 파일에 다음의 구문을 추가하고 저장합니다.yaml https://raw.githubusercontent.com/UbiquityRobotics/rosdep/master/raspberry-pi.yamlrosdep update를 실행한 후, 다음의 명령어를 실행합니다.$ cd ~/catkin_ws$ rosdep install --from-paths src --ignore-src --rosdistro=kinetic -y$ catkin_make설치가 끝났습니다.다음의 명령어를 통해 카메라가 잘 작동하는지 봅시다. 라즈베리파이에서 실행합니다.$ roslaunch raspicam_node camerav2_1280x960.launchrqt_image_view를 실행하여 토픽을 통하여 전송되고 있는 이미지를 볼 수 있습니다.$ rqt_image_view2. camera calibration카메라 캘리브레이션을 위해 카메라 노드들을 roslaunch 명령어로 실행합니다. 하지만 그냥 실행하면 image 토픽이 없으므로, enable_raw:=true를 추가해서 image 토픽을 생성합니다.roslaunch raspicam_node camerav2_1280x960_10fps.launch enable_raw:=true생성된 토픽은 다음과 같습니다.$ rostopic list/diagnostics/raspicam_node/camera_info/raspicam_node/image/raspicam_node/image/compressed/raspicam_node/parameter_descriptions/raspicam_node/parameter_updates/rosout/rosout_agg캘리브레이션을 할 때 필요한 노드는 /raspicam_node/image입니다. 그리고 다음의 명령어를 실행하여 캘리브레이션 프로그램을 띄웁니다.rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.074 image:=/raspicam_node/image camera:=/raspicam_node공식 가이드에는 위의 방법이 적혀 있었지만, 저는 이 명령어를 실행하자 오류가 떴습니다.$ rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.050mage:=/raspicam_node/image camera:=/raspicam_nodeWaiting for service /raspicam_node/set_camera_info ...OK(display:27228): GLib-GObject-CRITICAL **: 21:23:37.945: g_object_unref: assertion 'G_IS_OBJECT (object)' failedException in thread Thread-5:Traceback (most recent call last): File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner self.run() File \"/opt/ros/melodic/lib/python2.7/dist-packages/camera_calibration/camera_calibrator.py\", line 108, in run self.function(m) File \"/opt/ros/melodic/lib/python2.7/dist-packages/camera_calibration/camera_calibrator.py\", line 189, in handle_monocular drawable = self.c.handle_msg(msg) File \"/opt/ros/melodic/lib/python2.7/dist-packages/camera_calibration/calibrator.py\", line 811, in handle_msg gray = self.mkgray(msg) File \"/opt/ros/melodic/lib/python2.7/dist-packages/camera_calibration/calibrator.py\", line 295, in mkgray return self.br.imgmsg_to_cv2(msg, \"mono8\") File \"/opt/ros/melodic/lib/python2.7/dist-packages/cv_bridge/core.py\", line 171, in imgmsg_to_cv2 dtype=dtype, buffer=img_msg.data)TypeError: buffer is too small for requested array(display:27228): GLib-GObject-CRITICAL **: 21:23:47.126: g_object_unref: assertion 'G_IS_OBJECT (object)' failed이 오류는 image 토픽이 전송하는 이미지의 크기가 buffer보다 클 때 발생하는 오류입니다.3. Troubleshooting구글링 결과, 해결법을 찾아냈습니다. image_transport 패키지의 republish 노드를 이용하여 이미지의 크기를 조절하는 것입니다.다음의 명령어를 실행하여 image 크기를 조절합니다.rosrun image_transport republish compressed in:=/raspicam_node/image raw out:=/raspicam_node/image_repub/raspicam_node/image 토픽의 이미지 크기를 조절하여 /raspicam_node/image_repub 토픽으로 출력합니다. 이 토픽을 카메라 캘리브레이션 할 때 사용합니다.rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.050mage:=/raspicam_node/image_repub camera:=/raspicam_node아래는 calibration 사진입니다. 여러 각도의 사진을 분석하여 내부 파라미터값을 찾습니다. 성공! 당장은 모니터에 띄운 사진으로 인식 확인만 했지만, 조만간 A4용지에 복사하여 제대로 calibration 예정입니다.참고 사이트UbiquityRobotics/raspicam_node GithubROS wiki - camera_calibrationHow to calibrate raspicam v2.1"
    } ,
  
    {
      "title"       : "Kobuki 길들이기 (2) - Raspberry Pi로 Kobuki 제어",
      "category"    : "",
      "tags"        : "Kobuki",
      "url"         : "./kobuki2-raspi.html",
      "date"        : "2021-01-10 17:03:54 +0900",
      "description" : "",
      "content"     : "Kobuki를 Turtlebot3처럼Kobuki를 갖고놀다 보니 터틀봇3처럼 원격접속으로 컨트롤하면 편하겠다는 생각이 들었습니다. 매번 컨트롤 할때마다 노트북을 올려놓고 하다 보니 허리를 숙여야 해서 허리에도 뭔가 부담이….그래서 한번 라즈베리파이에 ROS kinetic과 꼬부기 패키지를 설치해보았습니다. 주말 동안 고생해서 올려 보았지만 뭔가 헛수고한 시간이 태반인듯 합니다..ㅠㅠ 구글링 해봐도 잘 정리된 글이 없어서 한번 정리해 봅니다. 물론 너무 쉬워서 다들 안 남기는거 같긴 하지만… 분명 저처럼 이리저리 헤멜 초보자가 있을 것이라 생각합니다.전체적인 순서사실 순서랄 것까지 없이 단순합니다. 라즈베리파이에 Ubuntu Mate 16.04 설치 - ssh 설정 - ROS kinetic 설치 - 꼬부기 패키지 설치 - 원격 접속 설정 - 구동 순서로 소개합니다.1. 라즈베리파이 Ubuntu Mate 16.04 설치보통 라즈베리파이 하면 라즈비안을 많이 설치합니다. 저도 처음엔 라즈비안을 설치했지만… ROS 설치에 어려움을 겪어 포기했습니다. 첫번째 시도 후 ROS kinetic이 이미 설치되어 있는 이미지 파일을 발견해 그걸 설치해 봤지만 꼬부기 설치 명령어가 다시 말썽을 부려 결국 두번째 시도 역시 물거품이 되었습니다. 그리고 다른 방법을 찾던 중 Ubuntu Mate를 사용하는 방법을 찾아 이 방법을 시도했고, 지금까지의 방법 중에서 가장 성공적인 결과를 거두었습니다.Ubuntu Mate 16.04는 원래 Ubuntu Mate 홈페이지에 있었던 듯 하지만 이제는 없습니다. 20.04밖에 다운할 수 없더군요. 그래서 구글링해서 이미지 다운로드 링크를 찾았습니다. 여기를 클릭하여 다운로드 해주세요.그리고 준비한 SD카드에 다운받은 이미지 파일을 설치해 줍니다. 저는 라즈베리파이 공식 홈페이지에서 다운받은 Raspberry Pi Imager를 사용했습니다. 라즈베리파이 레토르트 파우치. 무척 편리하다. 예전에는 OS 이미지 파일만 제공해주고 SD카드 굽는 프로그램은 따로 구하는 식이었는데 최근에 보니 클릭 몇번이면 프로그램이 알아서 다 해주도록 편리하게 바뀌었더군요. 마치 즉석식품 같네요. 아무튼 설치가 끝나면 초기 설정을 합니다. Ubuntu Mate는 놀랍게도 이미 설정되있는게 많습니다. 라즈비안처럼 나눔글꼴을 다운받을 필요도, 와이파이 국가를 바꿔줄 필요도 없습니다.2. ssh 설정ssh 설정은 간단한 부분이니 빠르게 넘어가겠습니다. 먼저 다음의 명령어로 ssh를 설치해 줍니다.sudo apt -get install ssh그 후 언제든지 ssh 원격접속을 사용할 수 있도록 다음의 명령어를 실행해 줍니다.sudo systemctl enable ssh.service이제 원격접속을 할 준비가 되었습니다.헷갈리는 부분..?사실 저는 여기까지 하고 원격접속을 그냥 사용할 수 있었는지가 잘 기억이 안납니다. 긴가민가 하니 다음 부분은 실행하실 분만 하셔도 될겁니다? 한다고 손해는 아닐 겁니다…아마도…sudo raspi-config 명령어를 실행하여 3번 Interfacing Options - SSH - yes - ok 순서로 선택합니다. 3번 엔터 손 가는대로~ 이제 원격접속은 문제없음. remote pc에서 한번 접속해 봅시다.3. ROS kinetic 설치우분투에서 ROS 설치하는 방법과 똑같습니다. 라즈비안에서 ROS 하나 설치하려고 별 쇼를 다했던 걸 생각하면 눈물날 정도로 허무합니다. 그러고도 결국 못했지만요.아무튼 시작해봅시다. 우선 다음의 명령어를 실행합니다.$ sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list'$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116우선 ROS의 설치에 앞서 sudo apt-get update를 해줍니다. 웬만한 software나 package 설치시에 항상 해주는 습관을 들이는 것이 좋습니다.이제 ROS kinetic 설치 명령어를 실행해 줍시다.$ sudo apt-get install ros-kinetic-desktop-full이제 ROS 사용 전에 의존성 초기화와 update를 해 줍시다.$ sudo rosdep init$ rosdep updatesudo rosdep init 명령어만 실행해도 update 명령어를 실행해 달라고 합니다. 편하네요ㅎㅎ그 다음은 환경 설정입니다. bashrc 파일에 다음 명령어를 추가해줍시다.source /opt/ros/kinetic/setup.bash아니면 그냥 터미널에 이걸 실행하셔두 되구요.$ echo \"source /opt/ros/kinetic/setup.bash\" &gt;&gt; ~/.bashrc마지막으로 rosinstall을 설치합니다.$ sudo apt-get install python-rosinstall설치가 잘 되었는지를 보려면 다음의 명령어를 실행해 보세요. 결과가 같다면 설치가 완료된 것입니다.4. Kobuki 패키지 설치 &amp; 원격 접속 설정매우 간단합니다. 다음의 명령어를 실행하면 설치 완료입니다.$ sudo apt-get install ros-kinetic-kobuki ros-kinetic-kobuki-core이제 라즈베리파이와 remote pc와 원격 접속을 할때 필요한 master-host IP를 설정해 봅시다.라즈베리파이와 remote pc 둘 다 같은 와이파이에 연결합니다. 그 다음, 서로의 IP 주소를 확인합니다.IP 주소는 ifconfig 명령어를 통해서 확인할 수 있습니다.bashrc 파일에 환경변수를 추가합니다. 이제 터미널이 켜질 때마다 자동으로 master/hostname IP가 설정됩니다.마지막. 구동해보기라즈베리파이를 연결하고 구동해보았습니다. 잘 되긴 합니다만 터틀봇처럼 keyop를 remote pc에서 켜니 다음과 같은 에러가 뜹니다.[ERROR] [1610324571.849429410]: CmdVelMux : configuration file not found [/home/bhbhchoi/kobuki_ws/src/kobuki/kobuki_keyop/param/keyop_mux.yaml]무슨 에러인지 잘 모르겠네요.. 그래서 새로운 터미널도 원격접속해서 keyop를 실행해 보니 잘 됩니다.마무리완전히 해결된건 아니지만 일단은 성공입니다. 마지막 남은 문제는 사소하지만 외부전원 공급 문제가 되겠군요. 꼬부기에서 전원을 공급할 수 있는것 같으니 이쪽을 알아보아야 겠습니다.참고 사이트Where are the 16.04 images?[ROS] ROS Kinetic install on Ubuntu 16.04.1 LTSROS wiki - Kobuki InstallationROBOTIS e-Manual Turtlebot3 Quick Start Guide"
    } ,
  
    {
      "title"       : "라즈베리파이 초기 설정하기",
      "category"    : "",
      "tags"        : "Raspberry Pi",
      "url"         : "./raspi-init-setup.html",
      "date"        : "2021-01-09 11:34:10 +0900",
      "description" : "",
      "content"     : "라즈베리파이 초기 설정라즈베리파이에 라즈비안을 설치했을 때 몇 가지 해줘야 할 초기 설정에 대해서 정리합니다. 개인적으로 깔 때마다 찾아서 해주는 설정이기에 한번에 보고자 이곳에 정리합니다.와이파이 설정wpa_supplicant.conf 파일을 엽니다. 아직 gedit이 설치가 안되어있으니 nano 편집기를 사용합니다.$ sudo nano /etc/wpa_supplicant/wpa_supplicant.conf와이파이 사용국가를 US로 바꿉니다.# country=KRcountry=US그 후 라즈베리파이를 재부팅하면 와이파이가 잡힙니다.한글 폰트 설치와이파이를 잡았다면 한글 폰트를 설치해 줍시다.$ sudo apt-get update$ sudo apt-get install fonts-nanum재부팅하면 깨져 보이던 한글이 다시 보일것입니다.gedit 설치window 메모장과 비슷한 형태인 편집기인 gedit을 설치합니다. 다음 명령어를 터미널에서 실행합니다.sudo apt-get install gedit원격접속시에는 못쓰는 치명적인 단점이 있지만 본 컴퓨터에서 쓰기는 괜찮습니다.raspi-config에서 해야 할 것1. 저장소 확장처음 라즈비안을 설치하면 저장소가 원래 SD카드 용량보다 훨씬 작아져 있습니다. 이 저장소를 확장해 주도록 합시다. 터미널 창에 다음 명령어를 입력합니다.sudo raspi-config6번 Advanced Options에서 Expand Filesystem을 선택해줍니다. 다음 재부팅부터 확장된 용량을 사용할 수 있습니다.2. SSH 설정3번 Interface Options-SSH-yes 선택, 이제 다른 컴퓨터에서 SSH 원격 접속을 하실 수 있습니다.원격 접속 방법remote 컴퓨터와 라즈베리파이를 같은 와이파이에 연결합니다. 그 후 remote 컴퓨터의 터미널 창에서 다음의 명령어를 실행합니다.ssh &lt;접속할 컴퓨터 이름&gt;@&lt;접속할 컴퓨터의 IP주소&gt;예시: ssh pi@192.168.0.10라즈베리파이의 IP주소는 ifconfig 명령어를 통해 확인할 수 있습니다. 보통 wlan0의 IP주소를 쓰면 됩니다.3. Password 설정1번 System Options-Password-확인-새 암호 입력, 이전의 비밀번호를 잊어버렸더라도 다시 설정할 수 있습니다."
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (4) - 토픽",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-topic.html",
      "date"        : "2021-01-07 18:48:00 +0900",
      "description" : "",
      "content"     : "토픽은 대화주제다비유하자면 노드가 사람이고 토픽은 대화주제, 메시지는 주고받는 대화내용, 메시지를 이루는 형식이 문법입니다. 우리는 rostopic 명령어로 대화내용을 엿볼 수 있습니다. 유용한 몇가지 테크닉을 알아봅시다. 대강 이런 느낌. 완전히 맞지는 않다. ROS2에서의 rostopicROS2에서 rostopic 명령어에 해당하는 것은 ros2 topic입니다. 이제 슬슬 패턴이 보입니다. 대부분은 ROS1에서 쓰는 명령어에서 ros를 ros2 로 바꾸면 됩니다.ros2 topic list현재 실행되고 있는 topic을 보여주는 명령어입니다. 다음의 명령어로 실행할 수 있습니다.ros2 topic list터틀심 노드를 실행했을 때 보여주는 토픽은 다음과 같습니다.$ ros2 topic list/parameter_events/rosout/turtle1/cmd_vel/turtle1/color_sensor/turtle1/pose위의 주제들로 노드끼리 통신을 하고 있습니다. 예를 들어 /turtle1/cmd_vel은 터틀심의 속도(command_velocity)를 전달하는 토픽입니다.ros2 topic echo현재 실행되고 있는 topic을 지정하여 대화 내용을 엿볼 수 있는 명령어입니다. 사용 방법은 다음과 같습니다.ros2 topic echo &lt;토픽 이름&gt;예를 들어 현재 터틀심의 /cmd_vel 토픽을 보고 싶다면, 다음과 같이 실행하면 됩니다.ros2 topic echo /turtle1/cmd_vel아래는 실행 결과입니다.$ ros2 topic echo /turtle1/cmd_vellinear: x: 2.0 y: 0.0 z: 0.0angular: x: 0.0 y: 0.0 z: 0.0---turtle_teleop_key 노드에서 속도 메시지를 전달해주면 터틀심이 그 내용을 수행합니다. 물론 turtle_teleop_key 노드가 실행되지 않았을 때는 아무것도 뜨지 않습니다.ros2 topic info지정한 토픽의 정보를 보여줍니다. 정보는 메시지 형식과 연결된 퍼블리셔, 서브스크라이버 개수입니다. 퍼블리셔와 서브스크라이버는 쉽게 말하면 각각 말하는 이와 듣는 이를 의미합니다. 퍼블리셔가 토픽을 발행(publish)하면 서브스크라이버가 구독(subscribe)합니다. 각각 메시지를 주고 받는 이들입니다. 이들은 C++이나 Python같은 언어로 구현되어 있습니다. ros2 topic info의 사용법은 다음과 같습니다.ros2 topic info &lt;토픽 이름&gt;터틀심으로 예시를 들어봅시다. turtle_teleop_key 노드가 실행되지 않았을 때의 /turtle1/cmd_vel의 info 명령어 결과를 봅시다.$ ros2 topic info /turtle1/cmd_velTopic: /turtle1/cmd_velPublisher count: 0Subscriber count: 1퍼블리셔가 없습니다. 아무도 움직이라고 말해주지 않는군요. 하지만 터틀심의 발에 동작개시를 명령할 준비는 되어 있습니다. 이제 turtle_teleop_key 노드를 실행했을 때의 /turtle1/cmd_vel의 info 명령어 결과를 봅시다.$ ros2 topic info /turtle1/cmd_velTopic: /turtle1/cmd_velPublisher count: 1Subscriber count: 1움직이라고 말해주는 퍼블리셔가 있습니다. 터틀심은 이 노드가 말하는대로 움직입니다. 하지만 아쉽게도 누가 말해주는지는 알 수 없습니다. 말해주는 사람을 알고 싶다면 rqt_graph 명령어를 사용하면 됩니다. rqt_graph는 밑에서 간단히 소개하도록 하겠습니다.ros2 interface showROS1의 rosmsg명령어와 동일한 역할을 수행합니다. ROS2 dashing 버전과 ROS2 eloquent 이상의 버전의 명령어가 다릅니다.ROS2 dashing:ros2 msg show &lt;메시지 이름&gt;예시:ros2 msg show geometry_msgs/msg/TwistROS2 eloquent and newer:ros2 interface show &lt;메시지 이름&gt;예시:ros2 interface show geometry_msgs/msg/Twist출력결과는 다음과 같습니다.$ ros2 interface show geometry_msgs/msg/TwistVector3 linearVector3 angular메시지의 형태를 볼 수 있습니다. 문법같은 느낌입니다.ros2 topic pub지정한 토픽의 메시지에 직접 매개변수를 전달할 수 있는 명령어입니다.ros2 topic pub &lt;topic_name&gt; &lt;msg_type&gt; '&lt;args&gt;'예시: ros2 topic pub --once /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.8}}\"–once 옵션은 한번만 수행하겠다는 뜻입니다. 이 명령어는 /turtle1/cmd_vel 토픽의 geometry_msgs/msg/Twist 메시지에 linear: {x: 2.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.8} 라는 내용을 한번 전송하겠다는 의미입니다. 이 명령어를 실행하면 터틀봇은 원 둘레의 1/3만큼 도는데, –once를 지우면 계속해서 돌게 됩니다. 실행 모습 rqt_graph이 기능은 ROS1과 ROS2의 실행방법은 같습니다. rqt가 설치되어 있다면 다음의 명령어로 실행할 수 있습니다.rqt_graph터틀심의 rqt_graph는 다음과 같습니다. teleop 노드를 실행한 상태입니다./turtle1/cmd_vel 토픽에 연결된 퍼블리셔/서브스크라이버는 각각 /turtlesim과 /teleop_turtle 이었나 보군요. 터틀심의 발에 붙은 노드는 /turtlesim 이었나 봅니다.rqt의 기능은 그래프만 있는 것이 아닙니다. 추후 다양한 기능을 다른 게시글에서 소개하도록 하겠습니다.참고 사이트ROS Index-ROS2 튜토리얼 토픽편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (3) - 노드",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-node.html",
      "date"        : "2021-01-07 16:48:00 +0900",
      "description" : "",
      "content"     : "지금까지 했던 것에 대해지금까지는 설명 없이 예제만 실행해 봤습니다. 이제는 ROS에서 쓰이는 개념에 대해서 조금씩 설명해 보겠습니다.노드란?ROS2를 시작하시는 분들은 거의 ROS1을 만져 보셨을 것이기에, 노드의 설명은 간단하게 하겠습니다. 노드는 데이터를 처리하는 세포 같은 것입니다. 데이터를 보내거나 받는 기능이 있고, 혹은 둘 다 있을수도 있습니다. 이들끼리는 토픽이라는 걸 주고 받는데 이는 메시지의 형태로, 메시지는 자료형의 형태로 수신/발신 됩니다.ROS2에서의 rosrunROS2에서는 ros2 run의 형식으로 노드를 실행합니다.ros2 run &lt;패키지 이름&gt; &lt;실행할 노드 이름&gt;예를 들어 터틀심 노드를 실행할때, 다음과 같이 입력합니다.ros2 run turtlesim turtlesim_nodeROS2에서의 rosnodeROS2에서는 ros2 node의 형식으로 rosnode 명령어를 수행합니다.ros2 node list다음의 명령어는 ROS1에서의 rosnode list와 같습니다. 현재 실행되고 있는 노드들의 리스트를 보여줍니다.ros2 node list터틀심 노드를 실행하고 있다면 다음과 같이 나올겁니다./turtlesimros2 node info다음의 명령어는 ROS1에서의 rosnode info와 같습니다. 어떤 노드 하나의 정보만을 콕 집어 보고싶을때 사용합니다.ros2 node info &lt;노드 이름&gt;보통 퍼블리셔, 서브스크라이버, 서비스의 정보가 담겨있습니다.요약ROS2부터 시작하시는 분은 없으시니 다 알고 계시는 내용일 것입니다. 아직까지는 ROS1과 크게 차이가 없는 듯 합니다.참고 사이트ROS Index-ROS2 튜토리얼 노드편"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (2) - 터틀심 예제",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-turtlesim.html",
      "date"        : "2021-01-07 14:48:00 +0900",
      "description" : "",
      "content"     : "터틀심 설치터틀심 설치 명령어를 입력합니다.$ sudo apt update$ sudo apt install ros-&lt;distro&gt;-turtlesimdistro에는 자신의 ROS2 버전을 넣어 줍시다. 저는 dashing을 넣었습니다.사실 거짓말입니다. ROS1을 설치해서인지 이미 터틀심 패키지가 설치되어 있었습니다.melodic으로 설치했는데도 별 문제 없이 실행이 되나봅니다.터틀심 예제 실습roscore 실행 필요없이 바로 터틀심 노드를 실행합니다.ros2 run turtlesim turtlesim_node파란 화면에 귀여운 거북이가 나옵니다.teleop_key 노드를 실행합시다.ros2 run turtlesim turtle_teleop_key화살표 키보드로 거북이를 움직여 봅시다. 잘 움직이는군요.실행 중인 노드를 확인하고 싶다면 ros2 node 명령어를 실행합니다.ros2 run node list정보를 알고 싶다면 ros2 node info 명령어를 사용합니다.ros2 run node info /turtlesim원래 쓰던 ROS1과 비슷한 부분이 많습니다. 파이썬도 그렇고 텐서플로 2.0도 그렇고 점점 간단하게 되는 추세인가 보네요. 그래도 분명 ROS1만의 장점이, ROS2만의 단점도 존재할 것입니다. 천천히 알아봅시다. 저도 아직 잘 모릅니다.참고 사이트ROS Index-ROS2 튜토리얼 터틀심편"
    } ,
  
    {
      "title"       : "Kobuki 길들이기 (1) - 설치 및 setup",
      "category"    : "",
      "tags"        : "Kobuki",
      "url"         : "./kobuki1-install.html",
      "date"        : "2021-01-06 20:31:47 +0900",
      "description" : "",
      "content"     : "Introducing KOBUKIKobuki 로봇은 유진 로봇에서 제작한 모바일 로봇 플랫폼입니다. ROS 실습용으로 사용하기 좋은 로봇입니다. 이 게시글은 KOBUKI 설치 방법을 다루고 있습니다.Kobuki package 설치제가 사용하는 ROS 버전은 melodic이기 때문에 유진로봇 github에서 패키지를 다운받았습니다. kinetic 버전을 사용하시는 분은 아래 링크를 참고해 주십시오.ROS wiki Kobuki Tutorial-Installation우선 Kobuki를 위한 workspace를 만듭니다.$ mkdir -p ~/kobuki_ws/src$ cd ~/kobuki_ws/$ catkin_makesrc 폴더에 Kobuki 패키지를 설치합니다.$ cd src$ git clone https://github.com/yujinrobot/kobuki.gitkobuki_ws로 가서 rosdep으로 의존성을 설치해 줍니다.$ cd ~/kobuki_ws/$ rosdep install --from-paths src --ignore-src -r -y마지막으로 catkin_make해 줍니다.$ catkin_make설치가 완료되었습니다. bashrc에 kobuki_ws의 source 명령어를 추가해 줍시다.$ echo \". ~/kobuki_ws/devel/setup.bash\" &gt;&gt; ~/.bashrcKobuki 작동 확인 (teleop)우선 roscore를 실행한 후, 다른 쉘을 열어 다음의 명령어를 실행합니다.$ roslaunch kobuki_node minimal.launch다음은 teleop를 위해 다른 쉘에서 다음의 명령어를 실행합니다.$ roslaunch kobuki_keyop safe_keyop.launch이제 키보드로 터틀봇을 조종할 수 있습니다.참고 사이트Kobuki ROS - Den medium blogRos wiki Kobuki Tutorial - Beginner Level"
    } ,
  
    {
      "title"       : "ROS2 첫걸음 (1) - ROS2 설치, 예제 실행",
      "category"    : "",
      "tags"        : "ROS2",
      "url"         : "./ros2-install.html",
      "date"        : "2021-01-06 19:00:00 +0900",
      "description" : "",
      "content"     : "ROS2 설치 명령어아래 명령어들을 터미널에 실행합니다.$ sudo apt update &amp;&amp; sudo apt install curl gnupg2 lsb-release$ curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -$ sudo sh -c 'echo \"deb [arch=$(dpkg --print-architecture)] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" &gt; /etc/apt/sources.list.d/ros2-latest.list'ROS2 dashing 설치, 우분투 버전마다 설치해야 하는 버전이 다릅니다. (ex. 18.04 -&gt; dashing, 20.04 -&gt; foxy)$ sudo apt update$ sudo apt install ros-dashing-desktopbashrc 파일에 다음 명령어 추가합니다.source /opt/ros/dashing/setup.bash아니면 그냥 echo 명령어를 사용하여 bashrc 맨밑줄에 추가합시다.$ echo \"source /opt/ros/dashing/setup.bash\" &gt;&gt; ~/.bashrcROS1 버전이 이미 설치되어 있다면 조금 번거로울 수 있습니다.자신에게 필요한 ROS를 사용할 때마다 bashrc를 열어 수정해 줘야 합니다. (해결법 필요)공존하기가 쉽지 않네요.간단한 예제 실행Publisher와 Subscriber 실행ros2 run demo_nodes_cpp talkerros2 run demo_nodes_py listener저는 잘 되었습니다.벌써부터 차이점이 느껴집니다. roscore같은 master 서버를 사용하지 않습니다.참고 사이트ROS2 dashing 설치"
    } ,
  
    {
      "title"       : "Bayesian Filter (1) Bayes 정리",
      "category"    : "",
      "tags"        : "Probabilistic Robotics, SLAM, Bayesian Filter",
      "url"         : "./bayesian-filter1.html",
      "date"        : "2020-12-22 10:07:23 +0900",
      "description" : "",
      "content"     : "Bayes 정리란? $\\large{P(H|E)=\\frac{P(E|H)P(H)}{P(E)}}$ $P(H): $사전확률 (가설, Hypothesis)$P(E|H): $ 가설 $H$를 만족할 때 $E$가 발생할 확률$P(E): $ 사건 E가 발생할 확률$P(H|E): $사후확률 (증거, Evidence)베이즈 정리란 쉽게 말해 확률을 어떻게 갱신할 수 있을지를 사전확률(Prior Probability)과 사후확률(Posterior Probability)의 관계로 나타낸 것입니다.사전확률은 말 그대로 사건이 발생하기 전의 사건이 발생할 확률을 의미합니다. 사후확률 역시 별반 다르지 않습니다. 물론 사건이 발생한 후의 사건이 발생할 확률입니다.그녀가 나를 좋아할 확률베이즈 확률을 이해하기 위해 재미있는 예시를 들고 왔습니다. 한 수학자가 여사친에게 발렌타인데이에 초콜릿을 받았습니다. 그런데 이 수학자는 완전한 이과뇌라서 친구가 자신을 좋아할 확률을 수학적으로 구하기로 했습니다. 여기서 그가 생각해낸 방법은 베이즈 정리입니다.수학자는 우선 그녀가 자신을 좋아할 확률을 좋아할 확률 50%, 좋아하지 않을 확률 50%로 잡았습니다. 아무런 근거가 없을 때 각각의 확률을 동일 비율로 잡는 것을 이유 불충분의 원리라고 합니다. 여기서 수학자가 잡은 이미 지금 상태에서 자신을 좋아할 확률이 바로 사전확률입니다. 위의 베이즈 정리에서 가설 $H$와 증거 $E$는 다음과 같습니다. $\\large{가설 \\; H = ``그녀가\\;날\\;좋아함\"이라는\\;사건}$ $\\large{증거 \\; E = \"초콜릿을\\;받았다\"라는\\;사건}$ 하지만 초콜릿을 받았다는 사실과 반반의 확률만으로는 사후확률을 구할 근거가 부족했습니다.그래서 수학자는 설문조사를 통해 일반적인 확률을 구해냈습니다. 수학자가 실시한 설문조사의 내용은 “좋아하는 사람에게 초콜릿을 줄 확률은?”, 그리고 “호감이 없지만 초콜릿을 줄 확률” 이었습니다. 두 설문조사의 결과는 각각 40%, 30%가 나왔습니다. 알기 쉽게 그림으로 설명해 봅시다. 가로 10명, 세로 10명으로 총 100명의 사람이 있다고 합시다. 가로축을 좋아할 확률로 치면, 그림은 다음과 같습니다.이제 설문조사 결과에 따라 좋아하는 사람에게 초콜릿을 받았을 40%, 호감이 없지만 초콜릿은 받았을 30%를 표시해 봅시다.그림에 따르면 좋아하는 사람 중 20명이, 호감이 없는 사람 중 15명이 초콜릿을 받았습니다. 그렇다면 이제 초콜릿을 받은 상황만을 생각해 봅시다. 초콜릿을 받은 사람은 100명 중 35명, 즉 발렌타인데이에 이유 없이 초콜릿을 받을 확률은 35%입니다.이 말대로면 3년에 한번은 받는다는 말인데 저는 왜 못 받았을까요? 음.. 아무튼 초콜릿을 받을 확률을 0.35라고 해 봅시다. 그 중에서 좋아하는 사람에게 초콜릿을 받았을 사람은 100명 중 20명, 즉 확률로 따지면 0.2입니다. 이러한 확률은 사각형 방법을 쓰면 구하기 쉽습니다. 사각형의 가로축을 그녀가 날 좋아할 확률, 세로축을 초콜릿을 받을 확률로 설정합니다. 그러면 다음과 같은 사각형의 형태가 됩니다.사각형의 넓이는 세로축 좋아하는 사람에게 초콜릿을 줄 확률 $P(초콜릿|좋아함)$, 가로축 좋아할 확률 $P(좋아함)$을 곱한 0.2가 됩니다. 이 확률은 베이즈 정리에서 분자가 됩니다. 따라서 수학자가 계산한 초콜릿을 준 친구가 자신을 좋아할 확률은 $\\frac{0.2}{0.35}=0.57$이 됩니다. 이를 수식으로 정리하면 다음과 같습니다. $\\large{P(좋아함|초콜릿)=\\frac{P(초콜릿|좋아함)P(좋아함)}{P(초콜릿)}}$ 하지만 여기에는 조금 오류가 있는데, 처음 그녀가 나를 좋아할/좋아하지 않을 확률이 0.5라는 것입니다. 사전확률이 옳다는 전제가 있어야 베이즈 정리가 빛을 볼 수 있습니다. 초콜릿을 받은 사건 말고도 같이 식사를 했다던가, 영화를 봤다던가 하는 사건들이 있으면, 사전확률이 점점 수정될 것입니다. 이런 원리를 이용하여 베이즈 정리는 인공지능 학습에 사용되기도 합니다. 참고로 제가 못받은 이유는 사전확률이 0퍼센트이기 때문입니다. 이것이 사전확률이 옳다는 전제의 중요성입니다…….슬프네요다음 내용은?Probabilistic Robotics에서 공부한 내용을 정리해서 올릴 예정입니다. 신뢰도 $bel(x_t)$를 이용한 필터 모델입니다. 아직 완전히 이해하지는 못했지만 조만간 올릴 수 있으면 좋겠네요.참고 자료 베이즈 정리를 쉽고 재미있게 풀어낸 영상입니다. 이 게시글의 예시는 모두 이 영상을 정리한 것입니다."
    } ,
  
    {
      "title"       : "쿼터니안(Quaternions) 회전",
      "category"    : "",
      "tags"        : "Quaternions Rotation",
      "url"         : "./quaternions.html",
      "date"        : "2020-12-18 19:00:00 +0900",
      "description" : "",
      "content"     : "쿼터니안이란?오일러 공식에 의해 복소수를 삼각함수로 표현할 수 있다는 점에 착안하여 삼각함수 없이도 회전변환 할 수 있게 해주는 방법. (수정) $\\large{e^{i \\theta}=cos \\theta +i \\; sin\\theta \\; \\; \\; (Eulear's formula)}$ 오일러 공식을 허수 평면에서 나타내면 다음과 같다. $\\large{e^{i \\varphi} \\cdot (1, 0)=e^{i \\varphi} \\cdot (1 +i \\cdot 0)=(cos \\varphi +i \\; sin\\varphi) \\cdot (1 +i \\cdot 0)=cos \\varphi +i \\; sin\\varphi}$ 즉, 오일러 공식을 통해 좌표 $(x, y)$를 각도 $\\theta$만큼 회전시킬 수 있는 것이다.실제로 $e^{i \\theta} \\cdot (x+iy)$를 하면 2차원 유클리디언 공간에서 각도 $\\theta$만큼 회전한 좌표가 나온다. $\\large{e^{i \\theta} \\cdot (x+iy)=(cos \\theta +i \\; sin\\theta)(x+iy)=(x \\; cos \\theta-y \\; sin \\theta)+i(x \\; sin \\theta+y \\; cos \\theta)}$ 이때의 실수부와 허수부를 좌표 $(x^{‘}, y^{‘})$라고 할 때, $x^{‘}$와 $y^{‘}$는 다음과 같다. $\\large{\\begin{bmatrix}x^{'}\\\\ y^{'}\\end{bmatrix}=\\begin{bmatrix}x \\; cos \\theta-y \\; sin \\theta\\\\ x \\; sin \\theta+y \\; cos \\theta\\end{bmatrix}=\\begin{bmatrix}cos \\theta &amp; -sin \\theta\\\\ sin \\theta &amp; cos \\theta\\end{bmatrix} \\begin{bmatrix}x\\\\ y\\end{bmatrix}}$ 이것은 내가 알던 회전변환 행렬과 같다. 결론적으로 2차원 유클리디언 평면 위의 한 점을 허수평면에서 나타낸 뒤, $e^{i \\theta}$를 곱하여 회전한 좌표를 구하면 회전한 좌표를 구할 수 있다.더욱 간단하게 말하면 허수 평면 좌표$(x, y)$ 오일러 공식을 곱하고 허수 벡터 성분 $i$를 떼고 실수부, 허수부를 취한다. 그리고 남은 식에서 $x, y$벡터 성분을 떼면 회전 행렬만 남는다.1843년 수학자 William R.Hamilton은 3차원 회전을 표현할 수 있는 복소수의 형태를 발견했다. 그것을 쿼터니안이라고 불렀다. 쿼터니안의 형태는 $q_0+q_{1}i+q_{2}j+q_{3}k$이다.회전을 위한 쿼터니안3차원 유클리디언 공간의 한 점을 pure quaternion의 형태로 나타낸다. 실수부가 없는 쿼터니안을 pure quaternion이라고 하는데, 표현하자면 $q=0+xi+yj+zk$이다. 회전은 $|q_{R}|=1$인 쿼터니안 $q_{R}$로 표현된다. 한 좌표계 A에서 다른 좌표계 B로 회전하는 것은 conjugation operation을 적용한 것이다. $\\large{q_{B}=q_{R}q_{A}q_{R}^{*}}$ 그 결과 $q_{B}$도 pure quaternion으로 표현된다.긴 다항식을 풀면 이런 형태로 정리된다. 각각 허수 벡터 $i, j, k$의 허수부로 깔끔하게 정리되고, 다시 허수부 안에서 3차원 유클리디언 공간의 벡터 $x, y, z$의 축으로 정리된다.회전행렬 M을 구해보자.이때, 벡터 $i, j, k$와 $x, y, z$를 떼고 남은 회전행렬 $M$은 다음과 같이 표현할 수 있다.따라서 $q_{R}q_{A}q_{R}^{*}$을 $M$으로 나타내면 다음과 같다. $\\large{q_{B}=q_{R}q_{A}q_{R}^{*}=M \\cdot \\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix} \\begin{bmatrix}i\\\\ j\\\\ k\\end{bmatrix}}$ Matrix $M$은 $|q_{R}|=q_{0}^2+q_{1}^2+q_{2}^2+q_{3}^2=1 \\; \\leftarrow$ 이 특성때문에 간략하게 표현할 수 있다. $\\large{-q_{2}^2-q_{3}^2=q_{0}^2+q_{1}^2-1}$ $\\large{-q_{1}^2-q_{3}^2=q_{0}^2+q_{2}^2-1}$ $\\large{-q_{1}^2-q_{2}^2=q_{0}^2+q_{3}^2-1}$ Matrix $M$을 간략하게 표현하면오일러 각의 회전 행렬을 쿼터니안으로 나타내는 방법회전행렬 $M$에 $trace$ 함수를 취하면 행렬의 대각합을 구할 수 있다. $\\large{\\begin{align*} Trace(M) &amp;= M_{11}+M_{22}+M{33} \\\\ &amp;= 2(3q_{0}^2+q_{1}^2+q_{2}^2+q_{3}^2-1.5) \\\\&amp;= 2(3q_{0}^2+(1-q_{0}^2)-1.5) \\\\&amp;= 2(2q_{0}^2-0.5) \\\\&amp;= 4q_{0}^2-1 \\end{align*}}$ 이 식으로 $q_{0}$을 구할 수 있다. $\\large{|q_{0}|=\\sqrt{\\frac{Trace(M)+1}{4}}}$ $q_{0}$을 알고 있으므로 $M_{11}$로부터 $q_{1}$을 구할 수 있다. $\\large{M_{11}=2(q_{0}^2+q_{1}^2-0.5)=2(\\frac{Trace(M)+1}{4}+q_{1}^2-0.5)}$ $\\large{|q_1|=\\sqrt{\\frac{M_{11}}{2}+\\frac{1-Trace(M)}{4}}}$ 같은 방식으로 $q_2$와 $q_3$도 구할 수 있다. $\\large{|q_2|=\\sqrt{\\frac{M_{22}}{2}+\\frac{1-Trace(M)}{4}}}$ $\\large{|q_3|=\\sqrt{\\frac{M_{33}}{2}+\\frac{1-Trace(M)}{4}}}$ 이로서 $q_0, q_1, q_2, q_3$에 대한 식을 구했다.이제 오일러 각 회전 행렬이 주어졌을때를 한번 생각해보자. $z$축을 중심으로 $\\varphi$만큼 회전한 3차원 회전 행렬은 다음과 같다. $\\large{M_{\\varphi}=\\begin{bmatrix}cos \\; \\varphi &amp; -sin \\; \\varphi &amp; 0 \\\\sin \\; \\varphi &amp; cos \\; \\varphi &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}}$ 이 회전행렬에서 $Trace(M)=2cos \\; \\varphi+1$이다. 이를 위에서 구한 식에 대입한다. $\\large{|q_0|= \\sqrt{\\frac{2cos \\; \\varphi+1+1}{4}}=\\sqrt{\\frac{1+cos \\; \\varphi}{2}}=cos \\; \\frac{varphi}{2}}$ $\\large{|q_1|=|q_2|= \\sqrt{\\frac{cos \\; \\varphi}{2}+\\frac{1-(2cos \\; \\varphi+1}{4}}=0}$ $\\large{|q_3|= \\sqrt{\\frac{1}{2}+\\frac{1-(2cos \\; \\varphi+1}{4}}=\\sqrt{\\frac{1-cos \\; \\varphi}{2}}=sin \\; \\frac{\\varphi}{2}}$그러므로, $z$축을 중심으로 $\\varphi$만큼 회전한 쿼터니안은 다음과 같다. $\\large{q_{\\varphi}=cos \\; \\frac{\\varphi}{2}+k \\; sin \\; \\frac{\\varphi}{2}}$같은 방식으로 $x$축, $y$축을 중심으로 회전한 쿼터니안 역시 구할 수 있다. $\\large{q_{\\phi}=cos \\; \\frac{\\phi}{2}+i \\; sin \\; \\frac{\\phi}{2}}$ $\\large{q_{\\theta}=cos \\; \\frac{\\\\theta}{2}+j \\; sin \\; \\frac{\\theta}{2}}$궁금한 점쿼터니안을 사용할때 회전이동밖에 안되나요? 평행이동을 같이 표현할 수는 없나요?그냥 쿼터니안 회전행렬 $M$을 곱해주기 전에 평행이동성분을 추가해주면 됨.터틀봇의 자세는 위치(3차원 좌표계)+방향(쿼터니안 x,y,z,w)으로 표현어제 보니까 x,y는 고정, 회전할때 z, w값만 바뀐다. 로봇의 z축이 천장 방향, x축이 전진 방향이었으니 z축 회전이겟지? $\\large{q_{\\varphi}=cos \\; \\frac{\\varphi}{2}+k \\; sin \\; \\frac{\\varphi}{2}}$뜬금없이 z가 실수부이진 않을테니… $z=sin \\; \\frac{\\varphi}{2}$, $w=cos \\; \\frac{\\varphi}{2}$ 라는 결론이 나온다.근데 왜 쓰는지 아직도 모르겠네?? 왜지??참고 자료Quaternion Tutorial.pdf"
    } ,
  
    {
      "title"       : "Particle Filter (1) 원리 설명",
      "category"    : "",
      "tags"        : "Probabilistic Robotics, SLAM, Particle Filter",
      "url"         : "./particle-filter1.html",
      "date"        : "2020-12-17 13:28:23 +0900",
      "description" : "",
      "content"     : "Particle Filter란?노이즈가 있는 환경에서 측정된 데이터를 필터를 사용해 실제 위치를 추정하는 도구이다. 선형 시스템과 가우시안 잡음(Gaussian Noise)가 적용되는 Kalman Filter와는 달리 비선형, 가우시안 분포가 적용되지 않는 환경에서 사용된다. turtlebot에서는 위치 추정에 사용된다. 아직은 크게 와닿지 않는 개념이다. 좀 더 알아보자.Particle Filter의 원리Particle Filter는 이름처럼 Particle을 사용하는 필터 기법이다. 쉽게 설명하기 위해 Youtube 채널 Team Jupeter의 Particle Filter 강의에서 든 예시를 설명하겠다. 위의 링크의 재생목록은 내가 공부하기 편하게 모아놓은 것이다. 게시글은 순전히 내 공부를 위한 것이니 사실 저기에 나온 내용을 정리해 놓은 것이기에 영상을 봐도 문제는 없다.Stage 1 좌표 1 2 3 4 5 6 7 8 9 10 벽     문   문     문     복도   로봇                 위의 표를 복도를 이동하는 로봇을 나타낸 것이라고 가정하자. 첫번째 행이 좌표, 두번째 행이 문의 위치, 세번째 행이 로봇이 이동하는 복도이다.처음에 로봇은 자신의 위치는 모르지만 자신이 있는 곳의 지도를 갖고있다. 따라서 지금 장님 로봇은 자신이 복도상의 어떤 지점에 존재할 확률을 모든 지점에서 균등하게 보고 있다.자, 그럼 로봇은 파티클을 복도에 무작위로 뿌린다. 모든 파티클은 자기 자신과 같은 행동을 할 수 있는 더미라고 볼 수 있다. 이게 무슨 말이냐면 처음에 모든 곳에 자신이 있을 가능성을 가정하고, 더욱 더 자신이 있을 것 같은 곳에 가중치를 더해서 그곳에 자신을 가중치만큼 복제시키고, 그것을 반복하여 자신의 진짜 위치를 찾는 것이다. 지금은 이해하기 어렵더라도 과정을 보다 보면 이해될 것이다.Uniform Particle Distribution     1 2 3 4 5 6 7 8 9 10   Measurement Probability 확률 10 10 10 10 10 10 10 10 10 10 (%) The Importance Weight 가중치 o o o o o o o o o o 신뢰공간 위의 표에서 가중치를 사실상 파티클이라고 보면 된다. Stage 1은 이렇게 균등하게 파티클이 뿌려진 set 1이 생성되었다. 이 set를 Posterior Probability라고 한다. set 1 1 2 3 4 5 6 7 8 9 10   o o o o o o o o o o Stage 2 좌표 1 2 3 4 5 6 7 8 9 10 벽     문   문     문     복도   ((( 로봇               로봇이 움직이기 시작한다. 문을 발견하면 자신이 위치한 곳으로 추정되는 곳을 짚어 볼 수 있다. 하지만 예상위치에 있을 확률이 올라간 만큼, 다른 지점은 확률이 떨어진다. 표로 나타내면 다음과 같다.     1 2 3 4 5 6 7 8 9 10   Measurement Probability 확률 7 7 17 7 17 7 7 17 7 7 (%) The Importance Weight 가중치 o o O o O o o O o o 신뢰공간 확률이 올라간 지점의 가중치 o의 크기가 조금 커진 것이 보이는가? 여기서 Importance Weight의 생성과정은 Stage 1에서의 가중치 $\\times$ stage 2에서의 Measurement Probability이다. $X_{i}=X_{i-1} \\cdot W(X_{i}) \\; \\; \\; \\; (X_{i}: Stage \\; i의 \\; set, \\; W(X_{i}): Stage \\; i의 \\; The \\; Importance \\; Weight)$ 어째서 바로 윗줄과 또 말이 다른것 같다? 그건 바로 원래 Importance Weight와 set는 사실 한몸이다. 여기서는 이해를 쉽게 하기 위해 다른 방식으로 떼어서 생각해 보았지만, 사실 Importance Weight와 set가 의미하는 바는 같다. 결국 Stage 2의 set는 다음과 같이 나타낼 수 있다. set 2 1 2 3 4 5 6 7 8 9 10   o   oo o oo o   oo o   이렇게 가중치를 Density의 형태로 나타내는 과정을 Resampling이라고 한다. 여기서 주의할 점이 있는데, 이전의 파티클의 개수와 Resampling한 파티클의 개수가 같아야 한다. 즉, 파티클은 가중치가 작다고 사라지는 게 아니라, 가중치가 높은 쪽으로 계속해서 모이는 방식으로 진행된다. 사실 위의 표에서 문이 있는 3, 5, 8번 지점을 제외한 나머지의 가중치, 즉 동그라미의 크기가 조금 작아져야 한다. 작성자의 한계로 더 작게 하지는 못했으니 10에서 7이 된 것만큼 작아졌다고 생각하고 봐주길 바란다…실제로 데이터가 아주 많을 때예시인 이 강의에서는 데이터가 10개 밖에 없지만 1000개 정도 된다고 가정하자. 무작위로 뽑지만 각 파티클의 Importance Weight가 클수록 높은 확률로 샘플에 잡히게 된다고 프로그래밍 한다. 이때의 set를 나타내면 다음과 같은 형태가 나올 것이다. set 1 2 3 4 5 6 7 8 9 10   o oo ooo oo ooo oo oo ooo oo o 연속적인 형태의 그래프가 나올 것이기 때문에 3, 5, 8번 지점을 중심으로 서서히 가중치가 올라가고 내려갈 것이다.Stage 3 좌표 1 2 3 4 5 6 7 8 9 10 벽     문   문     문     복도     ((( 로봇             또 로봇이 움직였다. 하지만 이번에는 로봇이 “움직이기만” 했다고 생각해 보자. “움직인다”는 measurement와는 다른 관점에서 봐야 한다. 단순히 이동만 하는 것이므로 아직 센서를 통해 measurement를 하지 않은 상태인 것이다. 하지만 odometry를 통해 이동에 대한 정보를 알고 있으므로, 확률 정보는 업데이트될 수 있다. 그렇다면 내친김에 그냥 한칸 더 가서 업데이트해보자. 좌표 1 2 3 4 5 6 7 8 9 10 벽     문   문     문     복도       ((( 로봇           다시 문을 발견했다. 자 아직 measurement는 하지말고, 확률 정보만 업데이트해보자. 현재 로봇이 어떤 위치에 있을 확률과 그에 대한 가중치는 다음과 같다. 확률 1   1 2 3 4 5 6 7 8 9 10   Measurement Probability 확률 7 7 7 7 17 7 17 7 7 7 (%) The Importance Weight 가중치 o o o o O o O o o o 신뢰공간 이제 확률 정보를 업데이트했으니 다시 measurement를 해보자. measurement 결과 문을 발견한 사실을 가지고 추정한 로봇 자신이 어떤 위치에 있을 확률은 다음과 같다. 확률 2   1 2 3 4 5 6 7 8 9 10   Measurement Probability 확률 7 7 17 7 17 7 7 17 7 7 (%) 자 이제 원래 로봇이 갖고 있던 확률 정보와 문을 발견한 사실을 토대로 추정한 위치에 있을 확률을 곱하면, 즉 확률 1 $\\times$ 확률 2를 하면 새로운(이 stage의) Importance Weight가 나올 것이다. 확률이 높은 곳끼리 곱해진 곳은 더욱 커질 것이다. 따라서 새로운 Importance Weight는 다음과 같다.     1 2 3 4 5 6 7 8 9 10   The Importance Weight 가중치 o o O o @ o O O o o 신뢰공간 5번 지점의 가중치가 매우 큰데 어떻게 표현해야 할지 모르겠다…. 아무튼 저 지점이 현저히 높은 가중치를 가진다. 또한 원래 작았던 1, 2, 4, 6, 9, 10 지점의 가중치는 쪼그라들어 없어지려 한다. 이 가중치를 토대로 Resampling을 하면 다음과 같은 결과가 나온다. set 3 1 2 3 4 5 6 7 8 9 10       oo   oooo   oo oo     다시 말하지만 파티클 수는 유지된다. 가중치가 쪼그라든 지점의 파티클을 끌어다가 가중치가 높은 지점으로 뿌려주었기 때문이다. Resampling 결과를 보면 아직 몇군데 짐작 가는 부분이 남았지만 거의 현재 로봇의 위치와 결과가 근접했다. 파티클 필터는 이런식으로 작은 가중치를 채로 걸러내듯이 굵은 가중치(?)만 남기는 방식으로 로봇의 위치를 추정할 수 있다.한눈에 보는 Particle Filter의 원리그림을 봐도 파티클 개수는 변하지 않았다. 파티클이 로봇의 위치를 거의 추정했을 때 계산을 효율적으로 하기 위해 파티클의 개수를 줄이는 방식인 AMCL 방식도 있지만 다음 기회에 알아보기로 하자. 나도 여기까지밖에 모른다.참고 사이트Youtube 채널 Team Jupeter의 Particle Filter 강의 강추Tuttlebot의 Particle Filter 적용 방식 한번에 이해된다. 강추쉽게 설명한 파티클 필터(particle filter) 동작 원리와 예제 (사진 출처: Ryan Blog)"
    } ,
  
    {
      "title"       : "Kalman Filter 외전 - 선형 칼만 필터의 기초",
      "category"    : "",
      "tags"        : "Probabilistic Robotics, SLAM, Kalman Filter",
      "url"         : "./kf1.html",
      "date"        : "2020-12-15 00:00:00 +0900",
      "description" : "",
      "content"     : "Kalman filter란??잡음이 포함된 선형 시스템에서 대상체의 상태(State)를 추적하는 재귀 필터. 쉽게 말하면 피드백을 통해 대상의 파라미터를 계속해서 수정하는 역할을 하는 것이다.선형 칼만 필터칼만 필터는 예측 단계와 추정(업데이트) 두개의 시퀀스를 가진다. 이전의 데이터를 가지고 대강 다음에 들어올 입력값을 예상. (예측 단계) 입력값이 들어온다. (입력 단계) 입력값과 예측값을 비교해서 최적의 출력값을 추정(업데이트 단계)이때 나온 출력값은 다시 1번에서의 ‘이전의 데이터’가 된다.1. 이전의 데이터를 가지고 다음에 들어올 입력값을 예상. (예측 단계)칼만 필터의 예시Constant Velocity Model아주 간단하게 상태 모델을 $x = [r, v]$ (거리: r, 속도: v) 라고 한다. 우리는 연속적인 세계에 살지만, 디지털 세계 사람을 위해 샘플링 속도를 dt($\\triangle t$)로 정의. $\\large{x_{k-1}=[r_{k-1}, v_{k-1}] \\; \\; \\; \\; \\; \\; \\; \\Phi _{k}=\\begin{bmatrix}1 &amp; \\triangle t\\\\ 0 &amp; 1\\end{bmatrix}}$ $\\large{x_{k}=\\Phi_{k} x_{k-1}=\\begin{bmatrix}1 &amp; \\triangle t\\\\ 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}r_{k-1}\\\\ v_{k-1}\\end{bmatrix}=\\begin{bmatrix}r_{k-1}+v_{k-1}\\triangle t\\\\ v_{k-1}\\end{bmatrix}}$ 이 모델을 관찰하면 $x_{k-1}$과 $x_{k}$의 $v$ 차이가 없다. 이런 이유로 이 모델을 Constant Velocity Model(이하 CVM)이라고 부르는 것이다.Constant Acceleration일땐? $\\large{x_{k}=\\Phi_{k} x_{k-1}=\\begin{bmatrix} 1&amp; \\triangle t&amp; \\frac{1}{2}\\triangle t^{2}\\\\ 0&amp; 1&amp; \\triangle t\\\\ 0&amp; 0&amp; 1\\end{bmatrix}\\begin{bmatrix}r\\\\ v\\\\ a\\end{bmatrix}}$ 물리에서 배웠던 등가속도 공식이 생각나지 않는가? 그거 맞다.칼만필터의 역할이란??예측값 $x_{k-1}$과 계측(관찰)값(나중에 나옴)이 차이(노이즈)를 가질때, 이것을 어떻게 보정할까??그것이 칼만필터의 역할이다.1.5 예측값의 분산의 계산 (공분산 계산단계)공분산 행렬 $P_{k}$에 대해서 알아보자.Process Noise Q $\\large{x_{k}=\\Phi_{k-1}x_{k-1}+w_{k-1}, w_{k} \\sim N(0, Q_{k})}$ 어떤 정밀한 기계도 측정/예측에는 반드시 오차가 발생한다. 그래서 우리는 True Value를 알 수 없고, 불확실한 측정에 의해 True Value를 추정할 수 밖에 없다.즉 어떠한 노이즈가 발생한다는 뜻이다. 노이즈를 이 식에서 $w_{k}$로 나타낸다.여기서 잠깐, 아까 CVM에선 $x_{k}=\\Phi_{k}x_{k-1}$ 였는데 왜 위의 식에서는 $\\Phi_{k-1}$를 사용하는가? 이건 위에 공식이 등속도 운동이라 $\\Phi$의 값이 항상 일정하기 때문이다.State (co)variance $P$ (공분산 $P$) $\\large{x_{k} \\leftarrow P, x_k \\sim N(x_k, P)}$ $P$는 칼만필터의 최종 추정치의 분산을 나타낸다. 즉, 우리는 $P$가 최소가 되도록 우리들의 추정치와 예측치를 사용해서 최적의 추정값을 찾아야 한다. 분산 $P$가 최소가 될때, 즉 중앙값에 수렴할 때 우리가 원하는 추정값(실제값과 일치하는)에 점점 더 가까워질 것이다. 공분산 $P$는 관측 전과 관측 후로 나뉘는데, 앞으로 관측 전의 $P$값을 $P(-)$, 관측 후의 $P$값을 $P(+)$라고 하기로 한다. 이는 $x_k$에도 마찬가지다.공분산 행렬 $P_k$의 계산오차의 정의: $e=$예측값-관측값예측 오차 $\\large{\\hat{x}_k(-)-x_k=\\Phi_{k-1}(\\hat{x}_{k-1}(+)-x_{k-1})+w_{k-1}}$ $\\large{x_k-E(x_k)=\\Phi_{k-1}(x_{k-1}-E(x_{k-1}))+w_{k-1}}$ $\\hat{x}$는 $x$의 예측값을 나타낸다. $E$ 함수는 Expected, 즉 기댓값,예측값을 말하는 것이다.$Var[X]=E[ee^T]$: 분산은 오차의 제곱의 기댓값과 같다. $\\large{P_k=E[(\\hat{x}_k(-)-x_k)(\\hat{x}_k(-)-x_k)^T]}$ $\\large{=E[\\Phi_{k-1}(\\hat{x}_{k-1}(+)-x_{k-1})(\\hat{x}_{k-1}(+)-x_{k-1})^T {\\Phi_{k-1}}^T+w_{k-1}{w_{k-1}}^T (C=AB, C^T=B^TA^T)}$ (이부분 수정필요) 여기부터 오차도 포함한다. $\\large{P_{k-1}=\\Phi_{k-1} P_{k-1} {\\Phi_{k-1}}^T+Q_{k-1} (E[w_{k-1}{w_{k-1}}^T]=Q_{k-1})}$ 모델 $\\Phi_{k-1}$와 ${\\Phi_{k-1}}^T$ 사이에 공분산행렬 $P_{k-1}$을 넣으면, 다음 상태의 공분산행렬 $P_k$가 나온다.2. 입력이 들어왔다. (관측)관측 모델 $\\large{z_k=H_{k}x_{k}+v_{k} \\; \\; \\; \\; \\; \\; \\; \\; \\; H=\\begin{bmatrix} 1&amp; 0\\\\ 0&amp; 1\\end{bmatrix} \\; \\; \\; \\; \\; \\; \\; \\; \\; v_{k} \\sim N(0, R_{k})}$ 엥? 관측에 모델이 필요한가요? 그냥 본거 그대로 적으면 안되나요?관측 노이즈값에 대한 표현이 필요하기 때문에 노이즈값을 포함한 모델을 고안한 것이다. 여기서 $v_k$가 노이즈를 의미한다.좌표변환(자이로 센서, 가속도 센서)이 일어난다던지, 값이 뻥튀기 되어서 나온다던지, 아무튼 우리가 생각한대로 측정이 안된다.위의 첫번째 공식에 $H$를 대입하면,$z_k=\\begin{bmatrix} 1&amp; 0\\ 0&amp; 1\\end{bmatrix}x_{k}+v_{k}$이다. $H$가 단위행렬인 이유는 거의 자기 자신을 의미하는 것이다. …왜 곱해주는거지? 보통은 다른 행렬이 들어가나 보다. 여기서는 쉽게 표현하기 위해 단위행렬로 표현한 듯 하다.알파 베타 함수와 칼만 게인 $\\large{F_{t+1}=\\alpha A_{t}+(1-\\alpha)F_{t} \\; \\; \\; \\; \\; \\; (\\alpha+\\beta=1)}$ 알파 베타 함수의 일종인 지수이동 평균 필터. $A$는 실측치, $F$는 필터링 후의 값. $\\alpha$값만 가지고 들어오는 데이터의 가중치를 계산, 최종필터치를 계산하는 방식.마치 저울질 하는거 같다?$\\alpha \\beta$ 함수와 칼만 게인 구하는 법이 유사하다. $\\large{\\hat{x}_{k}(+)=\\hat{x}_k(-)+K_{k}[z_{k}-H\\hat{x}_{k}(-)]}$ 여기서 $H$를 1로 바꾸면?? (단위행렬이니까 그래도 된다.) $\\large{\\hat{x}_{k}(+) = K_{k}z_{k} + (1-K_{k}) \\hat{x}_{k}(-)}$ 칼만 게인이 커지면 계측치 $z_k$를 신뢰 칼만 게인이 작아지면 예측치 $\\hat{x}_{k}(-)$를 신뢰3. 입력값과 예측값을 비교해서 최적의 출력값을 추정한다. (update)관측 시퀀스 복습 $\\large{z_{k}=H_{k}x_{k}+v_{k} \\; \\; (v_k는 \\; 노이즈)}$ 이제 우리들은 앞서 설명한 칼만추정식에 관측 모델의 식을 대입해보려 한다. $\\large{\\hat{x}_{k}(+)=\\hat{x}_{k}(-)+K_{k}[z_{k}-H_k \\hat{x}_{k}(-)]}$ $K_k$는 칼만 게인, 결국 정체는 가중치다!! 즉, 퍼센트, 비율이다. 0~1 사이의 값이란 뜻이다…. “필터”는 값을 보정해준다는 뜻이었구나..여기에 관측모델 $z_k$를 대입하면, $\\large{\\hat{x}_{k}(+)=\\hat{x}_{k}(-)+K_{k}[H_{k}x_{k}-H_{k}\\hat{x}_{k}(-)+v_k]}$ 이 된다.추정치와 오차의 공분산 행렬오차 $e=\\hat{x}_{k}(+)-x_k \\; \\; \\; \\; \\; \\; \\; $ (관측후-실제값)분산의 식에 대입하면 $\\large{P_{k}(+)=E(ee^T)=E[(\\hat{x}_{k}(+)-x_{k})(\\hat{x}_{k}(+)-x_{k})^T]}$ 전개하면, $\\large{P_{k}(+)=E[{(I-K_{k} H)(x_{k}=\\hat{x}_{k}(-))-K_{k}v_{k}}{(I-K_{k} H)(x_{k}=\\hat{x}_{k}(-))-K_{k}v_{k}}^T]}$ $\\large{P_k(+)=P_k(-)-K_{k}H_{k}P_{k}(-)-P_{k}(-)H_{K}^T K_{k}^T +K_{k}(H_{k}P_{k}H_{k}^T + R_{k})K_{k}^T}$ 휴우 적느라 힘들었다. 이런 복잡한 식을 적은 이유가 무엇일까…. 그것은 이 식에 최소 공분산 행렬을 찾는 방법이 있기 때문이다.$P_{k}$를 $K_{k}$에 대한 식으로 나타낸 것… 최소..공분산 행렬.. 감이 오는가? 함수의 최솟값을 찾는 방법은 바로 미분이다.미분을 통해 미분값이 0이 되는 $x$점이 바로 함수의 최솟값이 되는 지점이다. 이 식에서 $x$축은 바로 $K_{k}$이다.거의 다 왔다 근데 우리의 목표가 뭐였지?추정치와 진치의 오차공분산행렬인 $P_{k}(+)$의 크기 최소화이다. 따라서 여기서 선형대수학으로부터의 꿀팁. trace 함수를 사용하는 것이다.trace 함수는 대각성분의 합을 나타내는 함수이다. 대각성분의 스칼라합을 취한 후, 그 1차 미분이 0이 되는 부분을 취하면 최소지점을 찾을 수 있다. $\\large{\\frac{d}{dK_k} trace(P_{k}(+))=-2(H_{k}P_{k}(-))^T+2K_{k}(H_{k}P_{k}(-)H_{k}^T+R_k)=0}$ 이때의 $K_k$값이 $P_k(+)$가 최소가 되는 값이다. $\\large{K_{k}=P_{k}(-)H_{k}^T \\cdot [H_{k}P_{k}(-)H_{k}^T+R_{k}]^{-1}}$ $\\large{\\frac{P_{k}(-)H_{k}^T}{H_{k}P_{k}(-)H_{k}^T+R_{k}}}$ 여기서 $R_{k}$는 measurement error의 공분산이다. 아직 이게 뭔지 잘 모르겠으니 나중에 다시 보도록 하자..아무튼 더 간단하게, $H_k=\\begin{bmatrix} 1&amp; 0\\ 0&amp; 1\\end{bmatrix}$로 두면, $\\large{K_{k}=\\frac{P_k(-)}{P_k(-)+R_{k}}}$ 식이 아주 간단해졌다!최종 정리지금까지 공부했던 공식을 한 장에 정리해 보겠다.이건 그냥 공식의 총집편 같은거고… 정말 중요한건 이거다.칼만 필터 알고리즘을 사진 한 장만에 정리했다. 물론 내가 한건 아니다…음 정말 알아듣기 쉽게 잘 요약되있다. 지금까지 공부한게 한번에 이해되는 느낌이다.아직 예정은 없지만 칼만 필터에 대해 더 배운다면 더 올리도록 하겠다.참고 사이트Youtube: 손가락 TV 채널의 선형 칼만 필터의 기초 재생목록사실 이 강의의 요약정리본이었다. 순전히 내가 공부하기 위해 작성한 게시글이므로 이 채널로 보는게 빠를지도?"
    } ,
  
    {
      "title"       : "Turtlebot SLAM 실습 - SLAM과 내비게이션",
      "category"    : "",
      "tags"        : "SLAM, Turtlebot3",
      "url"         : "./turtlebot-slam-1.html",
      "date"        : "2020-12-14 00:00:00 +0900",
      "description" : "",
      "content"     : "SLAM과 내비게이션사실상 모바일 로봇(turtlebot 같은)의 최종적인 목적은 내비게이션이다. 여기서 말하는 내비게이션이란 정해진 목적지까지 이동하는 것이다. 스스로 목적지까지 이동하기 위해서는 무엇이 필요할까? 내비게이션 알고리즘에 따라 다르겠지만 보통 다음 기능들이 필수적으로 갖춰져 있어야 한다. 지도 로봇 자세 계측/추정 기능 벽, 물체 등 장애물 계측 기능 최적 경로를 계산하고 주행하는 기능1. 지도로봇 스스로 지도를 만드는 기법이 바로 SLAM이다. SLAM은 Simultaneous localization and mapping의 줄임말로, 동시적 위치 추정 및 지도 작성이라고 할 수 있겠다. 쉽게 말해 지금 내가 어디 있는지, 그리고 어떤 장소에 있는지를 계속해서 인지하면서 지도를 만드는 것이다.2. 로봇 자세 계측/추정 기능로봇이 자신의 자세(pose)를 계측하고 추정할 수 있어야 한다. GPS를 통한 자기 위치 추정은 실내에서 사용할 수 없다는 점과 정밀한 측정이 불가하단 점에서 모바일 로봇에 어울리지 않는다. 현재 실내 서비스 로봇이 가장 많이 사용하는 것은 추측 항법(dead reckoning)으로 상대적 위치 추정 방식이다. 이는 로봇 자신의 이동량을 바퀴의 회전축의 회전량을 가지고 측정하게 된다. 하지만 바퀴의 회전량이란게 오차가 꽤 발생하므로 IMU 센서로 관성 정보로 위치 보상을 통해 그 오차를 줄여 준다.로봇의 자세(pose)란?자세(pose)는 위치(positon: $x, y, z$)와 방향(orientation: $x, y, z, w$)으로 정의된다. 위치는 공간좌표 $x, y, z$이고, 방향은 $x, y, z, w$로 사원수(quaternion)의 형태이다.추측 항법의 간략한 설명다음과 같은 모바일 로봇이 있고, 두 바퀴 간의 거리는 $D$, 바퀴의 반지름을 $r$이라고 하자. 그림의 $q_k$ 지점의 좌표를 $(x_{k}, y_{k}, \\theta_{k})$이다. 이 로봇이 매우 짧은 시간 $T_e$ 동안 움직였을 때 좌우 모터 회전량(현재 엔코더 값 $E_{l}c, E_{r}c$와 이동 전의 엔코더 값 $E_{l}p, E_{r}p$)을 가지고 좌우 바퀴 회전 속도를 $(v_l, v_r)$를 구할 수 있다. $v_l=\\frac{E_{l}c, E_{l}p}{T_e}\\cdot \\frac{\\pi}{180}(radian/sec)$ $v_r=\\frac{E_{r}c, E_{r}p}{T_e}\\cdot \\frac{\\pi}{180}(radian/sec)$ 다음 식으로 좌우 바퀴의 이동 속도 $(V_l, V_r)$를 구하고 $V_l=v_l \\cdot r (meter/sec)$ $V_r=v_r \\cdot r (meter/sec)$ 다음 식으로 로봇의 병진속도$(linear velocity: v_k)$와 회전속도$(angular velocity: w_k)$를 구한다. $v_k=\\frac{V_r-V_l}{2} (meter/sec)$ $w_k=\\frac{V_r-V_l}{D} (meter/sec)$ 마지막으로 이 값들을 이용하여 로봇의 이동 후의 위치 $(x_{k+1}, y_{k+1})$ 및 방향 $\\theta_{k+1}$을 구할 수 있다. $\\triangle s=v_{k}T_{e} \\;\\;\\; \\triangle \\theta=w_{k}T_{e}$ $ x_{k+1}=x_{k}+\\triangle s\\: cos(\\theta_{k}+\\frac{\\triangle \\theta}{2})$ $ y_{k+1}=y_{k}+\\triangle s\\: sin(\\theta_{k}+\\frac{\\triangle \\theta}{2})$ $\\theta_{k+1}=\\theta_{k}+\\triangle \\theta$ 3. 벽, 물체 등 장애물 계측 기능당연히 센서를 사용한다. 종류에는 거리 센서(Lidar, 초음파 센서 등), 비전 센서(카메라 - stereo, mono, depth 등)을 사용한다.4. 최적 경로 계산 및 주행 기능목적지까지의 최적 경로를 계산하고 주행하는 내비게이션 기능이다. 이는 경로 탐색 및 계획이라 하는데, A*알고리즘, 포텐셜 필드, 파티클 필터, RRT(Rapidly-exploring Random Tree) 등 다양하다.참고 문헌/사이트ROS 로봇 프로그래밍: SLAM과 내비게이션추측항법(Dead-reckoning): odometric localization(이미지 출처)"
    } ,
  
    {
      "title"       : "Tensorflow 공부 (1) - Tensorflow 설치",
      "category"    : "",
      "tags"        : "모두를 위한 딥러닝, Deep Learning, Tensorflow",
      "url"         : "./tensorflow1.html",
      "date"        : "2020-12-01 18:00:00 +0900",
      "description" : "",
      "content"     : "Tensorflow 설치일반적으로 Terminal에서 pip install tensorflow 명령어를 통해 설치할 수 있다.특정한 버전이 필요하다면 pip install tensorflow -v '&lt;필요한 버전&gt;' 명령어를 사용하면 된다.anaconda 가상환경 설치 anaconda를 설치 다음 블로그를 참고하여 acaconda를 설치한다.[Ubuntu 18.04] Anaconda 설치 및 가상환경 구축 anaconda 가상 환경 설치 후 PATH 환경변수 설정 bashrc에 export PATH=~/anaconda3/bin:~/anaconda3/condabin:$PATH 추가 Terminal에서 conda activate base 실행, anaconda 가상환경을 실행 conda activate base에서 base는 가상환경 이름이기에 임의로 설정이 가능하다. conda activate base 실행에서 다음의 오류가 발생할 수 있다.CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.위의 오류 발생 시 source ~/anaconda3/etc/profile.d/conda.sh 를 실행하여 conda.sh를 다시 적용해주면 된다. 가상환경 내에서 Python 인터프리터 실행, 텐서플로를 conda install tensorflow 명령어로 설치Tensorflow 설치 확인, 버전 확인 anaconda 가상 환경에서 python3을 실행 import tensorflow as tf 실행 오류가 뜨지 않는다면 tensorflow가 성공적으로 설치된 것이다. tf.__version__ 실행 tensorflow의 버전을 확인할 수 있다.버전에 따라 실행할 수 있는 코드가 달라지거나, 실행 가능했던 코드가 실행할 수 없는 코드가 될 수도 있다.이 점 유의하도록 하자. 참고 사이트솜씨좋은장씨: CommandNotFoundError 해결욕심쟁이와이엇: [Ubuntu 18.04] Anaconda 설치 및 가상환경 구축"
    } ,
  
    {
      "title"       : "모두를 위한 딥러닝 정리(3) - Cross Entropy",
      "category"    : "",
      "tags"        : "모두를 위한 딥러닝, Deep Learning",
      "url"         : "./deep-learning3.html",
      "date"        : "2020-11-27 18:00:00 +0900",
      "description" : "",
      "content"     : "CROSS-ENTROPY 비용 함수Cross entropy는 Softmax 함수를 이용한 다중 분류의 비용, 즉 오차에 대한 평가를 나타내기 위한 용도입니다. 기본적인 공식은 다음과 같습니다. $\\large{D(S, L)=-\\sum_{i} \\; L_{i} \\; log(S_{i})}$ $S_{i}:$ Softmax 함수의 출력값, $S(\\overline{y})=\\overline{Y}$$L_{i}:$ Label값. 실제값이다. $L=Y, \\; A,B,C$ 범주 중 $A$가 정답일때, $\\begin{bmatrix} 1\\0\\0 \\end{bmatrix}$이제 이 식이 어째서 비용함수인지 알아봅시다. 위의 식을 다시 나타내면 다음과 같습니다. $\\large{-\\sum_{i} \\; L_{i} \\; log(S_{i})=\\sum_{i} \\; (L_{i}) \\cdot \\; (-log(S_{i}))}$ $\\sum$ 안으로, $log$ 앞으로 마이너스 부호가 이동했습니다. 여기서 우리는 $-log$함수를 알아볼 필요가 있습니다. 먼저 그래프부터 봅시다.자 설명 들어갑니다잉. 지난번에 나온 Softmax 함수의 결과물을 기억하시나요? 네, $\\begin{bmatrix} 0.7\\0.2\\0.1 \\end{bmatrix} $입니다. 이 값을 위의 $-log$함수에 대입합니다. 확률이 작을수록, 즉 0에 가까울수록 비용이 천정부지로 치솟습니다. 반대로 값이 클수록, 즉 1에 가까울수록 비용은 0에 수렴합니다. Cross-entropy의 원리는 $-log$ 함수의 0~1 범위 사이의 성질을 이용하는 것입니다. 이제 이 함수가 어떻게 비용을 올리고 내리는지에 대한건 감이 잡혔을 겁니다."
    } ,
  
    {
      "title"       : "모두를 위한 딥러닝 정리(2) - Softmax Regression",
      "category"    : "",
      "tags"        : "모두를 위한 딥러닝, Deep Learning",
      "url"         : "./deep-learning2.html",
      "date"        : "2020-11-27 18:00:00 +0900",
      "description" : "",
      "content"     : "Logistic 회귀 - 이진 분류Logistic 회귀는 데이터를 0과 1로 나누기 위해 사용되는 모델이다. 다른 이름으로 이진 분류(Binary Classification)이 있다. Softmax는 데이터를 2개 이상의 그룹으로 나누기 위해서 이진 분류를 확장한 모델이다. Logistic 회귀를 나타낸 그림은 다음과 같다. $\\large{H_{L}(X)=WX}$ $\\large{z=H_{L}(X), \\; \\; \\; Logistic Regression: g(z)}$ 일반적으로 Logistic 회귀를 사용할 때 Sigmoid 함수를 사용한다. 결과가 항상 0 또는 1로 수렴되기 때문이다.Multinomial Classification그렇다면 다중 분류는 어떻게 할 수 있을까? 이진 분류는 Sigmoid 함수에 넣으면 뚝딱 나오지만 다중 분류는 아직 감이 잡히지 않는다.다음 표를 토대로 그린 분류 그래프이다. x1 x2 y 10 5 A 9 5 A 3 2 B 2 4 B 11 1 C 이 그래프는 각각의 이진 분류로 따로 분리해서 생각할 수 있다. 마치 프로그래밍에서 if문을 여러개 쓰는 것처럼 분류할 수 있는 것이다.왼쪽부터 순서대로 A 분류, B 분류, C 분류이다. 각각의 가설은 다음과 같다. $\\large{\\begin{bmatrix}w_{A1} &amp; w_{A2} &amp; w_{A3}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\x_2 \\\\x_3\\end{bmatrix}=\\begin{bmatrix}w_{A1}x_1+w_{A2}x_2+w_{A3}x_3\\end{bmatrix}}$ $\\large{\\begin{bmatrix}w_{B1} &amp; w_{B2} &amp; w_{B3}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\x_2 \\\\x_3\\end{bmatrix}=\\begin{bmatrix}w_{B1}x_1+w_{B2}x_2+w_{B3}x_3\\end{bmatrix}}$ $\\large{\\begin{bmatrix}w_{C1} &amp; w_{C2} &amp; w_{C3}\\end{bmatrix} \\begin{bmatrix}x_1 \\\\x_2 \\\\x_3\\end{bmatrix}=\\begin{bmatrix}w_{C1}x_1+w_{C2}x_2+w_{C3}x_3\\end{bmatrix}}$ 이들을 하나의 식으로 나타내면 $\\large{\\begin{bmatrix}w_{A1} &amp; w_{A2} &amp; w_{A3} \\\\ w_{B1} &amp; w_{B2} &amp; w_{B3} \\\\ w_{C1} &amp; w_{C2} &amp; w_{C3} \\end{bmatrix} \\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\end{bmatrix}=\\begin{bmatrix}\\overline{y}_A\\\\ \\overline{y}_B\\\\ \\overline{y}_C\\end{bmatrix}}$ 이렇게 예측값 $\\overline{y}_A, \\overline{y}_B, \\overline{y}_C$을 구해 보았다. 이제 이 값들을 Softmax 함수에 집어넣을 것이다.Softmax 회귀란 어떤걸까? Softmax는 통계학에서 사용하는 표현인 Hardmax의 반댓말이다. Hardmax는 가장 큰 값을 제외한 나머지 수를 0으로 취급하는 것이다. 쉽게 말해 가장 큰 값, 가장 일치할 값이 높은 녀석만 남기고 싹 지우는 느낌이다. 하지만 그의 반대인 Softmax는… 반대라기 보다는 판단이 완화된 함수라고 보면 되겠다. 가장 큰 값, 즉 딥러닝에서 가장 일치할 확률이 높은 데이터를 가장 높은 값이 나오도록 출력하는 함수가 Softmax이다. 예를 들어 개인지 아닌지를 판단하는 모델이 있다. 주어진 데이터가 [개, 고양이, 호랑이] 일때 Hardmax 함수를 사용하면 결과값은 [1, 0, 0] 이 될 것이다. 하지만 Softmax 함수를 사용하면 [0.8, 0.15, 0.05] 같은 형태로 출력값이 나온다. Softmax 함수의 특징은 다음과 같다. 출력값이 0~1 사이 값 출력값 전체의 합이 1임.이 점을 알고 위의 예시를 계속 진행해 보도록 하자. Softmax에 $\\overline{y}$ 예측 벡터를 넣으면?$\\begin{bmatrix}\\overline{y}_A\\ \\overline{y}_B\\ \\overline{y}_C\\end{bmatrix}=\\begin{bmatrix}2.0\\ 1.0\\ 0.1\\end{bmatrix}$일 때, 다음 그림과 같은 결과가 나온다.그림에는 Softmax 공식이 적혀 있지만, Tensorflow에서는 이미 Softmax식이 구현되어 있으므로 그냥 쓰면 된다.고 한다. 나도 아직 텐서플로는 안써봐서 잘 모르겠다…이어서 마지막으로 $\\begin{bmatrix}0.7 0.2 0.1\\end{bmatrix}$에 one hot encoding이란 걸 할거다. one hote encoding이란 확률이 가장 높은 것을 1로, 그 외에는 0으로 만드는 알고리즘이다. Hardmax와 무슨 차이인가 싶기도 하지만 Hardmax는 one hot encoding에서 1이 올 자리에 자기 자신이 온다는것 정도만 다르다. 아무튼 이렇게 one hot encoding을 거치고 나면 $\\begin{bmatrix} 1\\ 0\\ 0\\ \\end{bmatrix}$이 남는다. 이렇게 Softmax에서의 판단이 끝나면 “아! 이 자료는 A 범주에 속하겠구나!” 라는 결론이 나오는 것이다.다음 시간에는 Softmax 분류기의 비용함수에 대해서 공부한 내용을 정리해야겠다. 인강 들을때는 A4용지 한페이지 분량정도였는데 풀어서 적어 보니 꽤 양이 된다…궁금한 점Q. 이러면 그냥 하드맥스 하고 그거를 원핫인코딩하면 되는거 아닌가? Softmax만의 특별한 점이 있나? 확률로서 나타낼 수 있는점? 그게 왜 특별한거지?참고 자료모두의 딥러닝-ML lec 6-2: Softmax classifier 의 cost함수"
    } ,
  
    {
      "title"       : "모두를 위한 딥러닝 정리(1) - 가설, 선형 회귀, Logistic 회귀, 비용 함수",
      "category"    : "",
      "tags"        : "모두를 위한 딥러닝, Deep Learning",
      "url"         : "./deep-learning1.html",
      "date"        : "2020-11-26 18:00:00 +0900",
      "description" : "",
      "content"     : "PPT 참고2학년 동아리 활동 시절에 정리한 자료이다.세미나는 학습한 내용보다는 issue에 중심을 맞추어 작성해야 한다고 한다. 다음 세미나는 이 점 명심해서 준비하도록 하자.머신러닝의 기초.pptx"
    } ,
  
    {
      "title"       : "YJ Link 회사 견학 후기",
      "category"    : "",
      "tags"        : "회사 견학 후기",
      "url"         : "./company-visit-1.html",
      "date"        : "2020-11-19 18:00:00 +0900",
      "description" : "",
      "content"     : "YJ Link는 어떤 기업인가?YJ Link는 SMT 공정 과정에 사용되는 기계를 만드는 회사입니다.SMT(Surface Mounting Technology)란 표면실장기술을 뜻하는 것으로 전자기기 조립을 자동으로 실행하는 장치를 총칭합니다.인쇄회로기판(PCB) 위에 반도체나 다이오드, 칩 등을 다수의 장비로 실장하고 이를 경화시키는 기능을 수행하는 기술로 표면 실장형 부품을 PWB 표면에 장착하고 납땜하는 기술을 의미합니다.[출처] SMT 공정과정 * SMT장점&amp;단점|작성자 쎄크회사 견학 내용반도체 공정 중에 사용되는 모바일 로봇을 견학하고 왔다. 독일 로봇 회사 OMRON에서 제작한 로봇에 yj link의 모듈을 추가한 모바일 로봇이었는데 youtube ros 강의에서 실습했던 터틀봇 시뮬레이션과 거의 비슷한 시뮬레이터를 구현하고 있었다. 사실상 성능 엄청 좋은 터틀봇이다…라고도 할 수 있긴 하지만 더 많은 연구의 결정체가 들어가 있을 것이다.이 기술이 실무에서도 사용된다는 것을 알고 적잖이 놀랐다. 하지만 그런 만큼 turtlebot만 하더라도 쉽지 않을 것임을 의미하는 것일지도 모른다.. YJ Link에서 봤었던 모바일 로봇 기술이사님과의 이야기에서 들었던 산학협력의 어긋남이 기억에 남는다. 기업은 실무 중심의 인재를 키워서 자기 회사로 보내주기를 원한다. 하지만 학교에서는 논문, 실적거리를 찾기 때문에 새로운 것을 배우기를 원한다. 이 문제는 어떤 방식으로 해결할 수 있을까?2020.12.12 수정내용)확실히 터틀봇의 라이다로서는 맵핑하는데 조금 무리가 있다. 안되는건 아니지만 5000만원짜리 좋은 OMRON의 라이다와 센서들 만은 못했다.Question1. 로봇/기계 설계 제어에 필요한 실력은 어느 정도인가요? 학부졸업생도 채용하나요?실제로 고졸도 많이 오는 환경이라 대졸이라면 그리 낮은 스펙은 아니라고 한다. 하지만 그만큼 기대값을 가지고 채용했기 때문에 고졸 직원이 힘들어하는 계산적인 설계를 요구한다. 역학과 제어, 설계의 중요성은 더욱 커지는 듯 하다. 석사 이상의 스펙도 있지만 그들은 아무래도 연구업무 쪽에서 종사한다. 실제로 카이스트 졸업생도 오는 것을 봐서 역시 취업의 문이란 쉽지 않은듯하다.2. 경북대 터틀봇 제가 아는 그거인가요?버거, 와플 등의 터틀봇이 맞다. 경북대는 왜 리눅스 부팅만 한거지?? 아깝게시리..3. slam 기술이 적용되면 단가가 많이 오르는가? 장비문제? 연구문제?LM계열의 라이다 센서가 성능이 좋은 대신 비싸긴 하지만(1000만원) 반드시 장비단가 문제라고는 볼 수 없다. 연구개발에도 돈이 들기 때문에 무엇 하나가 파이에서 엄청난 비율을 잡아먹는다거나 하지는 않는다."
    } ,
  
    {
      "title"       : "ROS SLAM &amp; NAVIGATION 강의 정리",
      "category"    : "",
      "tags"        : "SLAM, Navigation",
      "url"         : "./SLAM1.html",
      "date"        : "2020-11-18 18:00:00 +0900",
      "description" : "",
      "content"     : "SLAM이란?SLAM이란 위치 추정과 맵핑을 동시에 하는 기법.2차원, 3차원 지도를 그리는 방법이다.방법: Gmapping, Katographer 등OpenSLAM에 공개된 SLAM의 한 종류, ros에서 패키지로 제공.Occupancy Grid Mapping: 2차원 점유 격자 지도직접 만든 로봇관 2층 빈방의 occupancy grid map.흰색: 자유 영역(이동 가능)흑색: 점유 영역(이동 불가)회색: 미지 영역(확인 안됨)Gmapping은 개발이 거의 되고 있지 않는데다가 2d이다.구글 cartographer는 개발도 잘되고 있고, 3d이다.카토그래퍼쪽이 비전이 있다고 한다.Q. 사실상 SLAM과 컴퓨터 비전은 다른 분야인거네요??맞다. 본질적으로는 다르다. 하지만 교집합이 매우 넓고 점점 겹쳐지고 있는 상태라서 결국 둘 다 하게 될것이다."
    } ,
  
    {
      "title"       : "Camera_calibration 실습",
      "category"    : "",
      "tags"        : "RealSense, Camera Calibration",
      "url"         : "./realsense3.html",
      "date"        : "2020-11-04 18:00:00 +0900",
      "description" : "",
      "content"     : "realsense d435i의 카메라는 Monocular 인가? Stereo 인가?이 부분을 잘못 알아서 한참 헤맸다. 틀림없이 Stereo인줄 알았는데 오른쪽에 렌즈 하나만 달린 Monocular 였던 것이다. 아…. 현타 엄청나게 왔다Monocular 카메라의 캘리브레이션 방법은 다음 사이트에 나와 있다.ROS wiki: How to Calibrate a Monocular CameraIntel이 제공하는 Calibration SDK 사용intel에서 제공하는 calibration tool을 사용한 캘리브레이션 시도해 보았다. 하지만 내부 파라미터를 빼오기 힘들어 지양하는 방법이란 말을 듣고 바로 포기했다.사실 issue로서 failed to start calibration 이 떴다. 왜인지는 모르겠다.Monocular Camera Calibration-issue: Waiting for service /camera/set_camera_info …Service not foundrosrun 명령어 끝에 –no-service-check를 붙임. 서비스 없이 가능할까?-issue: (display:11948): GLib-GObject-CRITICAL **: 17:07:10.740: g_object_unref: assertion ‘G_IS_OBJECT (object)’ failed무슨 이슈인지 짐작도 안간다.. gstreamer와 관계있다고 추정? 이 에러는 다양한 이유가 있기 때문에 직접적 해결은 못했다….하지만 결국 해결헀다!! 토픽과 rosrun의 경로가 안맞았기 때문이다./camera/camera_info/camera/image_raw위의 두 Topic이 실행되고 있었어야 하는데, 실행은 되고 있지만 경로가 맞지 않아 Calibration node가 실행을 못하고 있었던 것이다.실제 실행되고 있었던 Topic은 /camera/color/camera_info, /camera/color/image_raw 였기에, 결국 rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/camera/image_raw camera:=/camera을rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/camera/color/image_raw camera:=/camera 으로 고치고 나서야 실행되었다.공부부족이었다! rosrun문법에 대해서 더 공부하자. pub, sub/srv, client/param 역시 한번 복습 필요Calibration 결과터미널 창에 출력된 결과를 보면 camera matrix에 내부 파라미터의 값이 나온 것을 볼 수 있다. 그 외에 distortion(왜곡)이나 rectification(정류? 뭔지 모르겠다), projection(사영) 성분 역시 검출할 수 있다."
    } ,
  
    {
      "title"       : "Camera_calibration(1) 내부 파라미터의 구성요소",
      "category"    : "",
      "tags"        : "RealSense, Camera Calibration",
      "url"         : "./realsense2.html",
      "date"        : "2020-11-03 18:00:00 +0900",
      "description" : "",
      "content"     : "Camera_calibrationCamera calibration을 하는 이유는 2d 평면을 3d 모델로 해석하기 위해서 필요한 파라미터를 구하는 것이다.A[R/t] 행렬에서 A행렬이다.A행렬은 내부 파라미터(intrinsic calibration)라고 하는데 이의 구성은1. 초점거리 fx, fy실제 단위: mm, 카메라 모델 단위: pixelpixel은 이미지 센서의 셀 크기에 대해 상대적인 값으로 표현예를 들어 셀 크기: 0.1mm, 초점거리가 500mm면 렌즈 중심에서 이미지 센서까지의 거리는 이미지 센서 셀 크기의 500배, 50mm라는 뜻이다. 실제 단위를 셀 크기와 초점거리로 표현해 놓은 것이다. 왜? 값을 이미지 센서에 종속시킴으로서 영상에서의 기하학적 해석을 용이하게 하기 위해서.2. 주점 cx, cy렌즈에서 이미지 센서에 내린 수선의 발이론상으론 거의 영상 중심점과 같지만 렌즈의 길이가 잘못되었으면 다를 수도 있다.3. 비대칭계수 skew_c = tan α이미지 센서의 y축이 기울어진 정도. 왜 기울어진거지?Camera calibration의 이론-좌표계, 파라미터 행렬, 그의 식normalized image plane에 대한 설명원점을 카메라 죄표계의 Zc축이 관통하고, 원점으로부터의 거리가 1인 평면.다크 프로그래머의 글에서 이해 안되는 부분인 x = fxX/Z+cx, y = fyY/Z+cy의 유도과정.월드 좌표계 항인 (X, Y, Z)는 [R/t]가 이미 곱해진것임. 이를 계산하면(scale은 생략)이리하여 그 공식이 나오고 만것이다!!사실 이건 그리 중요한 것이 아니다. 중요한건 정규좌표계와 픽셀좌표계, 그리고 픽셀의 개념이다…!오늘의 의문점-rviz_ pointcloud2의 정체는 3d lidar였다.-velodyne lidar : 최근 자율주행 자동차에 많이 사용되는 라이다.-카메라 캘리브레이션의 이론과 실습 중참고한 사이트다크 프로그래머 :: 카메라 캘리브레이션 (Camera Calibration)다크 프로그래머 :: 카메라 좌표계"
    } ,
  
    {
      "title"       : "이전의 RealSense D435i 진행 요약",
      "category"    : "",
      "tags"        : "Realsense",
      "url"         : "./realsense1.html",
      "date"        : "2020-10-31 18:00:00 +0900",
      "description" : "",
      "content"     : "지금까지의 진행상황 요약RealSense D435i를 교수님께 받고 SDK 툴을 설치했다. ROS wiki의 RealSense 항목을 참조했다.ROS wiki RealSense 항목그 후 Rviz를 설치하여 Depth Camera의 PointCloud와 rgb camera를 볼 수 있었다.(rviz 사진 첨부 필요)realsense d435i와 rviz 연동해봄.필요한것은 *roslaunch realsense2_camera rs_rgbd.launch, rviz이를 위해서 rgbd가 설치되어 있어야 한다.안되어 있으면 Resource not found: rgbd_launch 오류 발생.sudo apt install ros-&lt;ros_version&gt;-rgbd-launch 커맨드를 실행해 설치.이후 rviz에서 map-&gt;camara_link로 변경, Add-&gt;Pointcloud2를 선택하여 realsense 작동을 확인할 수 있다."
    } 
  
]
